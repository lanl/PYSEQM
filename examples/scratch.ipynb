{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mai\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mchatgpt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mWrite a poem about C++.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2475\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2474\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2475\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2477\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2478\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2479\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/jupyter_ai_magics/magics.py:603\u001b[0m, in \u001b[0;36mAiMagics.ai\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    600\u001b[0m ip \u001b[39m=\u001b[39m get_ipython()\n\u001b[1;32m    601\u001b[0m prompt \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat_map(FormatDict(ip\u001b[39m.\u001b[39muser_ns))\n\u001b[0;32m--> 603\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_ai_cell(args, prompt)\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/jupyter_ai_magics/magics.py:545\u001b[0m, in \u001b[0;36mAiMagics.run_ai_cell\u001b[0;34m(self, args, prompt)\u001b[0m\n\u001b[1;32m    542\u001b[0m provider \u001b[39m=\u001b[39m Provider(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprovider_params)\n\u001b[1;32m    544\u001b[0m \u001b[39m# generate output from model via provider\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m result \u001b[39m=\u001b[39m provider\u001b[39m.\u001b[39;49mgenerate([prompt])\n\u001b[1;32m    546\u001b[0m output \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n\u001b[1;32m    548\u001b[0m \u001b[39m# if openai-chat, append exchange to transcript\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/langchain/llms/base.py:227\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    222\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    225\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    226\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    228\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/langchain/llms/base.py:178\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    177\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    179\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/langchain/llms/base.py:165\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    163\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    166\u001b[0m                 prompts,\n\u001b[1;32m    167\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    168\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    170\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    171\u001b[0m             )\n\u001b[1;32m    172\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    173\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/langchain/llms/openai.py:822\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(\n\u001b[1;32m    819\u001b[0m         generations\u001b[39m=\u001b[39m[[Generation(text\u001b[39m=\u001b[39mresponse)]],\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m     full_response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    823\u001b[0m     llm_output \u001b[39m=\u001b[39m {\n\u001b[1;32m    824\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtoken_usage\u001b[39m\u001b[39m\"\u001b[39m: full_response[\u001b[39m\"\u001b[39m\u001b[39musage\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    825\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name,\n\u001b[1;32m    826\u001b[0m     }\n\u001b[1;32m    827\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMResult(\n\u001b[1;32m    828\u001b[0m         generations\u001b[39m=\u001b[39m[\n\u001b[1;32m    829\u001b[0m             [Generation(text\u001b[39m=\u001b[39mfull_response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m])]\n\u001b[1;32m    830\u001b[0m         ],\n\u001b[1;32m    831\u001b[0m         llm_output\u001b[39m=\u001b[39mllm_output,\n\u001b[1;32m    832\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/langchain/llms/openai.py:106\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/langchain/llms/openai.py:104\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "Write a poem about C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-tg1-large`, `bedrock:anthropic.claude-v1`, `bedrock:anthropic.claude-instant-v1`, `bedrock:ai21.j2-jumbo-instruct`, `bedrock:ai21.j2-grande-instruct` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0` |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-0301` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-0301` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-tg1-large\n",
       "* bedrock:anthropic.claude-v1\n",
       "* bedrock:anthropic.claude-instant-v1\n",
       "* bedrock:ai21.j2-jumbo-instruct\n",
       "* bedrock:ai21.j2-grande-instruct\n",
       "\n",
       "anthropic\n",
       "Requires environment variable ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "\n",
       "cohere\n",
       "Requires environment variable COHERE_API_KEY (not set)\n",
       "* cohere:medium\n",
       "* cohere:xlarge\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable OPENAI_API_KEY (not set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable OPENAI_API_KEY (not set)\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "\n",
       "openai-chat-new\n",
       "Requires environment variable OPENAI_API_KEY (not set)\n",
       "* openai-chat-new:gpt-4\n",
       "* openai-chat-new:gpt-4-0314\n",
       "* openai-chat-new:gpt-4-32k\n",
       "* openai-chat-new:gpt-4-32k-0314\n",
       "* openai-chat-new:gpt-3.5-turbo\n",
       "* openai-chat-new:gpt-3.5-turbo-0301\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Create a random non-symmetric 3x3 matrix\u001b[39;00m\n\u001b[1;32m      4\u001b[0m matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a random non-symmetric 3x3 matrix\n",
    "matrix = torch.randn(3, 3)\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_evals, r_vec  = torch.linalg.eig(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_evals = r_evals.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_vec = r_vec.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2490,  0.9967, -0.8862],\n",
       "        [ 0.4433,  0.0478, -0.4604],\n",
       "        [ 0.8611, -0.0663,  0.0523]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.8666,  0.7185,  0.2705])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7268, -0.9412, -0.5543],\n",
       "        [-0.0827,  0.2500, -1.5807],\n",
       "        [-0.2431,  0.1129, -2.8544]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_vec @ torch.diag(r_evals) @ torch.inverse(r_vec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     LEFT EIGENVECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reorder eigenvalues and eigenvectors for left side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_evals, l_vec = torch.linalg.eig(matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_evals = l_evals.real\n",
    "l_vec = l_vec.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4536,  0.0722,  0.0671],\n",
       "        [-0.8389, -0.8951, -0.0159],\n",
       "        [ 0.3008,  0.4400,  0.9976]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7185,  0.2705, -2.8666])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, idx = torch.sort(l_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.8666,  0.2705,  0.7185])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_evals[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mT_rec = l_vec[:, idx] @ torch.diag(l_evals[idx]) @ torch.inverse(l_vec[:, idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7268, -0.0827, -0.2431],\n",
       "        [-0.9412,  0.2500,  0.1129],\n",
       "        [-0.5543, -1.5807, -2.8544]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mT_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7268, -0.9412, -0.5543],\n",
       "        [-0.0827,  0.2500, -1.5807],\n",
       "        [-0.2431,  0.1129, -2.8544]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mT_rec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4393,  0.0403, -0.0690],\n",
       "        [-1.1232, -0.0030,  0.3823],\n",
       "        [ 0.7024,  0.9954, -1.0709]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_vec[:, idx] @ r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, vl, vr = scipy.linalg.eig(matrix, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8665645 +0.j,  0.71850705+0.j,  0.2704992 +0.j], dtype=complex64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06713676,  0.45360634,  0.07221775],\n",
       "       [-0.01585622, -0.8389088 , -0.89509785],\n",
       "       [ 0.9976178 ,  0.30078772,  0.43998227]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4536,  0.0722,  0.0671],\n",
       "        [-0.8389, -0.8951, -0.0159],\n",
       "        [ 0.3008,  0.4400,  0.9976]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24895024,  0.99665254, -0.8861679 ],\n",
       "       [ 0.44334868,  0.04781595, -0.46040115],\n",
       "       [ 0.8610841 , -0.06631196,  0.05231897]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2490,  0.9967, -0.8862],\n",
       "        [ 0.4433,  0.0478, -0.4604],\n",
       "        [ 0.8611, -0.0663,  0.0523]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5812,  0.3241, -0.1168],\n",
       "        [ 1.7754, -0.6307,  0.3123],\n",
       "        [-3.2363,  0.1830, -0.1128]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_vec @ matrix @ r_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5812,  0.3241, -0.1168],\n",
       "        [ 1.7754, -0.6307,  0.3123],\n",
       "        [-3.2363,  0.1830, -0.1128]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_vec @ r_vec @ torch.diag(r_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7268, -0.9412, -0.5543],\n",
       "        [-0.0827,  0.2500, -1.5807],\n",
       "        [-0.2431,  0.1129, -2.8544]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7268, -1.8825, -1.6628],\n",
       "        [-0.0827,  0.5001, -4.7422],\n",
       "        [-0.2431,  0.2258, -8.5632]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix * torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m         orb_loc2\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m         orb_loc1\u001b[39m.\u001b[39mappend(orb_loc2[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m         orb_loc2\u001b[39m.\u001b[39mappend(orb_loc2[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m4\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39morb_loc1:\u001b[39m\u001b[39m\"\u001b[39m, orb_loc1)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "atomic_numbers = [8, 1, 1]  # Example atomic numbers for Hydrogen and Oxygen\n",
    "\n",
    "orb_loc1 = []\n",
    "orb_loc2 = []\n",
    "\n",
    "for atomic_number in atomic_numbers:\n",
    "    if atomic_number == 1:\n",
    "        orb_loc1.append(0)\n",
    "        orb_loc2.append(0)\n",
    "    else:\n",
    "        orb_loc1.append(orb_loc2[-1] + 1)\n",
    "        orb_loc2.append(orb_loc2[-1] + 4 - 1)\n",
    "\n",
    "print(\"orb_loc1:\", orb_loc1)\n",
    "print(\"orb_loc2:\", orb_loc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal1 = torch.cumsum(torch.arange(0, 6), dim=0) \n",
    "pascal1 = pascal1 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  0,  2,  5,  9, 14])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pascal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging' from '/scratch/soft/conda/envs/se_exc/lib/python3.10/logging/__init__.py'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload module logging\n",
    "reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging, sys\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    force = True)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logFormatter = logging.Formatter(fmt='%(funcName)s :: %(lineno)d :: %(levelname)s :: %(message)s')\n",
    "# consoleHandler = logging.StreamHandler()\n",
    "# consoleHandler.setLevel(logging.DEBUG)\n",
    "# consoleHandler.setFormatter(logFormatter)\n",
    "# logger.addHandler(consoleHandler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is color!\n"
     ]
    }
   ],
   "source": [
    "from colorama import init as colorama_init\n",
    "from colorama import Fore\n",
    "from colorama import Style\n",
    "\n",
    "print(f\"This is {Fore.GREEN}color{Style.RESET_ALL}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mhello\u001b[0m \u001b[32mworld\u001b[0m\n",
      "\u001b[31mhello red world\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "print(colored('hello', 'red'), colored('world', 'green'))\n",
    "print(colored(\"hello red world\", 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1m\u001b[47m\u001b[31mhello red world\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored(\"hello red world\", 'red', 'on_white', attrs=['bold', 'underline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(funcName)s :: %(lineno)d :: %(levelname)s :: %(message)s')\n",
    "\n",
    "QM_LEVEL_NUM = evel=logging.DEBUG - 5\n",
    "logging.addLevelName(QM_LEVEL_NUM, \"FATAL\")\n",
    "def qm(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM_LEVEL_NUM):\n",
    "        self._log(QM_LEVEL_NUM, message, args, **kwargs) \n",
    "        \n",
    "logging.Logger.qm = qm     \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.qm(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tensor(tensor):\n",
    "    return '\\n' + str(colored(tensor, 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<module> :: 1 :: INFO :: Hello world \n",
      "\u001b[31mtensor([[ 0.7164, -0.1639, -0.7737],\n",
      "        [-0.0483, -1.3250,  0.4045],\n",
      "        [-1.6294, -0.8557, -0.9866]])\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Hello world %s\\n\", format_tensor(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.randn(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception occurred\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_552085/2100013069.py\", line 4, in <module>\n",
      "    c = a / b\n",
      "ZeroDivisionError: division by zero\n",
      "INFO:root:Hello world\n",
      "DEBUG:root:Hello world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = 5\n",
    "b = 0\n",
    "try:\n",
    "  c = a / b\n",
    "except Exception as e:\n",
    "\n",
    "  logging.exception(\"Exception occurred\")\n",
    "  \n",
    "logging.info(\"Hello world\")\n",
    "logging.debug(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Hello world\n"
     ]
    }
   ],
   "source": [
    "logging.debug(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:name raised an error\n"
     ]
    }
   ],
   "source": [
    "logging.error('%s raised an error', 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are learning Python logging!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# Format the log message\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(name)s:%(message)s')\n",
    "# Emit a warning message\n",
    "logging.warning('You are learning Python logging!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: John, Age: 30\n"
     ]
    }
   ],
   "source": [
    "name = \"John\"\n",
    "age = 30\n",
    "\n",
    "log_message = f\"Name: {name!s}, Age: {age!s}\"\n",
    "print(log_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.tensor([    0.18390,      0.47971,     0.11075,     -0.44567,      -0.16180,      -0.70265,     -0.07267]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[     0.00019,      0.39385,      0.68065,      0.08393,      0.12593,      0.02553],\n",
    "        [    -0.00305,     -0.85121,      0.31723,      0.07221,     -0.03733,      0.04126],\n",
    "        [     0.99588,     -0.00374,      0.00348,     -0.04622,      0.00885,      0.07131],\n",
    "        [     0.00055,     -0.03038,     -0.09445,     -0.01060,     -0.01580,     -0.01141],\n",
    "        [    -0.00582,     -0.06560,     -0.07990,     -0.02999,     -0.01007,      0.07922],\n",
    "        [    -0.01756,      0.06470,      0.00373,     -0.06577,      0.02665,      0.25105],\n",
    "        [    -0.00898,     -0.12151,      0.12681,     -0.00861,      0.01494,      0.13020],\n",
    "        [     0.04704,     -0.01807,     -0.15131,      0.61237,      0.50253,     -0.11545],\n",
    "        [     0.05169,     -0.12784,      0.17780,      0.21873,     -0.04797,     -0.73203],\n",
    "        [     0.02170,      0.20901,     -0.17349,      0.04403,     -0.02703,     -0.31161],\n",
    "        [     0.00634,     -0.10987,     -0.55207,     -0.05356,     -0.08935,     -0.11011],\n",
    "        [     0.01441,      0.11632,      0.00882,      0.04844,     -0.00403,     -0.20311],\n",
    "        [    -0.00756,     -0.05988,      0.06480,     -0.70860,      0.41245,     -0.32108],\n",
    "        [     0.04057,      0.07037,      0.10241,     -0.02929,     -0.70903,     -0.24095],\n",
    "        [     0.01614,     -0.04040,     -0.00739,     -0.17080,      0.21197,     -0.21081],\n",
    "        [     0.01479,      0.00141,     -0.02137,      0.13435,      0.00750,     -0.03007]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor(\n",
    "       [[     0.00019,      0.39385,      0.68065,      0.08393,      0.12593,      0.02553],\n",
    "        [    -0.00305,     -0.85121,      0.31723,      0.07221,     -0.03733,      0.04126],\n",
    "        [     0.99588,     -0.00374,      0.00348,     -0.04622,      0.00885,      0.07131],\n",
    "        [     0.00055,     -0.03038,     -0.09445,     -0.01060,     -0.01580,     -0.01141],\n",
    "        [    -0.00582,     -0.06560,     -0.07990,     -0.02999,     -0.01007,      0.07922],\n",
    "        [    -0.01756,      0.06470,      0.00373,     -0.06577,      0.02665,      0.25105],\n",
    "        [    -0.00898,     -0.12151,      0.12681,     -0.00861,      0.01494,      0.13020],\n",
    "        [     0.04704,     -0.01807,     -0.15131,      0.61237,      0.50253,     -0.11545],\n",
    "        [     0.05169,     -0.12784,      0.17780,      0.21873,     -0.04797,     -0.73203],\n",
    "        [     0.02170,      0.20901,     -0.17349,      0.04403,     -0.02703,     -0.31161],\n",
    "        [     0.00634,     -0.10987,     -0.55207,     -0.05356,     -0.08935,     -0.11011],\n",
    "        [     0.01441,      0.11632,      0.00882,      0.04844,     -0.00403,     -0.20311],\n",
    "        [    -0.00756,     -0.05988,      0.06480,     -0.70860,      0.41245,     -0.32108],\n",
    "        [     0.04057,      0.07037,      0.10241,     -0.02929,     -0.70903,     -0.24095],\n",
    "        [     0.01614,     -0.04040,     -0.00739,     -0.17080,      0.21197,     -0.21081],\n",
    "        [     0.01479,      0.00141,     -0.02137,      0.13435,      0.00750,     -0.03007]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0030, -0.8512,  0.3172,  0.0722, -0.0373,  0.0413])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8012)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(A[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize_torch(U, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Orthogonalizes the matrix U (d x n) using Gram-Schmidt Orthogonalization.\n",
    "    If the columns of U are linearly dependent with rank(U) = r, the last n-r columns \n",
    "    will be 0.\n",
    "    \n",
    "    Args:\n",
    "        U (numpy.array): A d x n matrix with columns that need to be orthogonalized.\n",
    "        eps (float): Threshold value below which numbers are regarded as 0 (default=1e-15).\n",
    "    \n",
    "    Returns:\n",
    "        (numpy.array): A d x n orthogonal matrix. If the input matrix U's cols were\n",
    "            not linearly independent, then the last n-r cols are zeros.\n",
    "    \n",
    "    Examples:\n",
    "    ```python\n",
    "    >>> import numpy as np\n",
    "    >>> import gram_schmidt as gs\n",
    "    >>> gs.orthogonalize(np.array([[10., 3.], [7., 8.]]))\n",
    "    array([[ 0.81923192, -0.57346234],\n",
    "       [ 0.57346234,  0.81923192]])\n",
    "    >>> gs.orthogonalize(np.array([[10., 3., 4., 8.], [7., 8., 6., 1.]]))\n",
    "    array([[ 0.81923192 -0.57346234  0.          0.        ]\n",
    "       [ 0.57346234  0.81923192  0.          0.        ]])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(U[0])\n",
    "    # numpy can readily reference rows using indices, but referencing full rows is a little\n",
    "    # dirty. So, work with transpose(U)\n",
    "    V = U.T\n",
    "    for i in range(n):\n",
    "        prev_basis = V[0:i]     # orthonormal basis before V[i]\n",
    "        coeff_vec = prev_basis @ V[i].T  # each entry is np.dot(V[j], V[i]) for all j < i\n",
    "        # subtract projections of V[i] onto already determined basis V[0:i]\n",
    "        V[i] -= (coeff_vec @ prev_basis).T\n",
    "        if torch.norm(V[i]) < eps:\n",
    "            V[i][V[i] < eps] = 0.   # set the small entries to 0\n",
    "        else:\n",
    "            V[i] /= torch.norm(V[i])\n",
    "    return V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_587060/3498202963.py:34: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  coeff_vec = prev_basis @ V[i].T  # each entry is np.dot(V[j], V[i]) for all j < i\n"
     ]
    }
   ],
   "source": [
    "A_orto = orthogonalize_torch(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_orto == A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = torch.tensor([[     0.00487,      9.97446,     16.85499,      1.36908,      1.68907,      0.35934],\n",
    "        [    -0.07908,    -21.92047,      7.89247,      1.02650,     -0.20035,      0.36962],\n",
    "        [    25.81744,     -0.08932,      0.06386,     -0.41008,      0.07489,      0.61599],\n",
    "        [     0.01411,     -0.90634,     -2.49518,     -0.18099,     -0.24790,     -0.17374],\n",
    "        [    -0.15499,     -0.65752,     -0.82035,     -0.20654,      0.09200,      0.72623],\n",
    "        [    -0.46512,      0.59608,      0.01199,     -0.62086,      0.14642,      2.40241],\n",
    "        [    -0.23257,     -1.17181,      1.04003,     -0.33873,      0.12309,      1.10003],\n",
    "        [     1.21955,     -0.43131,     -2.77491,      5.43359,      4.25084,     -0.99723],\n",
    "        [     1.33620,     -1.41798,      1.89508,      1.63204,     -0.41887,     -6.21818],\n",
    "        [     0.56070,      1.69008,     -1.56129,      0.78984,     -0.33191,     -2.73018],\n",
    "        [     0.16439,     -1.00996,     -4.75088,      0.23844,      0.09762,     -0.72571],\n",
    "        [     0.37680,      1.12287,      0.05600,      0.51423,     -0.21354,     -1.88172],\n",
    "        [    -0.19604,     -1.42909,      1.18835,     -6.28742,      3.48889,     -2.77344],\n",
    "        [     1.05172,      1.67942,      1.87805,     -0.25985,     -5.99765,     -2.08124],\n",
    "        [     0.41839,     -0.96417,     -0.13556,     -1.51554,      1.79300,     -1.82097],\n",
    "        [     0.38351,      0.03356,     -0.39200,      1.19205,      0.06348,     -0.25972]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = torch.tensor([[     0.00019,      0.39385,      0.68065,      0.08393,      0.12593,      0.02553],\n",
    "        [    -0.00305,     -0.85121,      0.31723,      0.07221,     -0.03733,      0.04126],\n",
    "        [     0.99588,     -0.00374,      0.00348,     -0.04622,      0.00885,      0.07131],\n",
    "        [     0.00055,     -0.03038,     -0.09445,     -0.01060,     -0.01580,     -0.01141],\n",
    "        [    -0.00582,     -0.06560,     -0.07990,     -0.02999,     -0.01007,      0.07922],\n",
    "        [    -0.01756,      0.06470,      0.00373,     -0.06577,      0.02665,      0.25105],\n",
    "        [    -0.00898,     -0.12151,      0.12681,     -0.00861,      0.01494,      0.13020],\n",
    "        [     0.04704,     -0.01807,     -0.15131,      0.61237,      0.50253,     -0.11545],\n",
    "        [     0.05169,     -0.12784,      0.17780,      0.21873,     -0.04797,     -0.73203],\n",
    "        [     0.02170,      0.20901,     -0.17349,      0.04403,     -0.02703,     -0.31161],\n",
    "        [     0.00634,     -0.10987,     -0.55207,     -0.05356,     -0.08935,     -0.11011],\n",
    "        [     0.01441,      0.11632,      0.00882,      0.04844,     -0.00403,     -0.20311],\n",
    "        [    -0.00756,     -0.05988,      0.06480,     -0.70860,      0.41245,     -0.32108],\n",
    "        [     0.04057,      0.07037,      0.10241,     -0.02929,     -0.70903,     -0.24095],\n",
    "        [     0.01614,     -0.04040,     -0.00739,     -0.17080,      0.21197,     -0.21081],\n",
    "        [     0.01479,      0.00141,     -0.02137,      0.13435,      0.00750,     -0.03007]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = orthogonalize_torch(V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = V @ R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval, _ = torch.linalg.eig(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_mo = torch.tensor([[-36.58831, -17.32189, -14.70534, -12.33198,   4.02645,   5.08758,   0.00000,   0.00000,   0.00000,   0.00000,   0.00000,   0.00000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-36.5883, -36.5883, -14.7053, -12.3320])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_mo[0][occ_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_idx = torch.tensor([0, 0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to square number\n",
    "def square(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def fit_and_plot_best_fit_line(x, y): \n",
    "    # Fit a best-fit line using linear regression\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "\n",
    "    # Generate points for the best-fit line\n",
    "    x_fit = np.linspace(min(x), max(x), 100)\n",
    "    y_fit = slope * x_fit + intercept\n",
    "\n",
    "    # Plot the data points and the best-fit line\n",
    "    plt.scatter(x, y, label='Data')\n",
    "    plt.plot(x_fit, y_fit, color='red', label='Best Fit Line')\n",
    "\n",
    "    # Add labels and a legend\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "gjhgj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/auto/nest/nest/u/fns/github/PYSEQM',\n",
       " '/auto/nest/nest/u/fns',\n",
       " '/scratch/soft/conda/envs/se_exc/lib/python310.zip',\n",
       " '/scratch/soft/conda/envs/se_exc/lib/python3.10',\n",
       " '/scratch/soft/conda/envs/se_exc/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages',\n",
       " '/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/PyQt5_sip-12.11.0-py3.10-linux-x86_64.egg',\n",
       " '/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/numpy-1.24.2-py3.10-linux-x86_64.egg',\n",
       " '/scratch/soft/conda/envs/se_exc/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chech python path from which modules are imported\n",
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_module import test\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test()\n",
      "File \u001b[0;32m/auto/nest/nest/u/fns/github/PYSEQM/test_module.py:2\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTEEEST\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply 3d tensors of shape 1, 2, 2\n",
    "a = torch.tensor([[[1, 2], \n",
    "                   [3, 4]]])\n",
    "\n",
    "b = torch.tensor([[[1, 2], \n",
    "                   [3, 4]]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7, 10],\n",
       "         [15, 22]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums1 = [1,2,3,0,0,0]\n",
    "nums2 = [2,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(nums1, nums2):\n",
    "    \"\"\"\n",
    "    Do not return anything, modify nums1 in-place instead.\n",
    "    \"\"\"\n",
    "    if len(nums2) != 0:\n",
    "        nums1[len(nums2):] = nums2\n",
    "    else:\n",
    "        pass\n",
    "    print(nums1)\n",
    "    #nums1 = [x for x in nums1 if x != 0]\n",
    "    nums1.sort()\n",
    "    print(nums1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 2, 5, 6]\n",
      "[1, 2, 2, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "merge(nums1, nums2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "print(nums1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeElement(nums, val):\n",
    "\n",
    "    nums = list(filter(lambda x: x != val, nums))\n",
    "    print(nums)\n",
    "    return len(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeElement([3,2,2,3], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "a = 121\n",
    "\n",
    "print(str(a)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plusOne( digits):\n",
    "\n",
    "        num = int(\"\".join([str(x) for x in digits])) + 1\n",
    "        new_num = list(str(num))\n",
    "        new_num = [int(x) for x in new_num]\n",
    "        return new_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "num = plusOne([9,9])\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tril_indices(5, 5)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4],\n",
       "        [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 - 3 \n",
    "#3 - 6\n",
    "#4 - 10\n",
    "#5 - 15\n",
    "#6 - 21\n",
    "#7 - 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_triangular_number(number):\n",
    "    return number * (number + 1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_triangular_number(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros((1, 21, 21))\n",
    "B = torch.ones((21, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 21])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 21])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0] = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 21])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((1, 2, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0610, 0.4935, 0.2684, 0.0443, 0.0983, 0.8161, 0.2461, 0.4028,\n",
       "          0.0798, 0.2857, 0.5427, 0.5053, 0.8313, 0.7958, 0.1535, 0.9198,\n",
       "          0.2401, 0.5798, 0.1565, 0.4939, 0.5032],\n",
       "         [0.1665, 0.6340, 0.1537, 0.4337, 0.0639, 0.1458, 0.6765, 0.6893,\n",
       "          0.9654, 0.5101, 0.0032, 0.5954, 0.6509, 0.4628, 0.1142, 0.3407,\n",
       "          0.1416, 0.4421, 0.5329, 0.3409, 0.1585]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[:, :, 0]\n",
    "\n",
    "A = torch.rand((2, 8, 6))\n",
    "B = torch.rand((1, 6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.7908, 2.5006, 1.5290, 2.2222, 2.1031, 1.9600],\n",
       "         [2.3464, 2.0375, 1.5260, 1.5997, 1.4620, 1.5213],\n",
       "         [2.0339, 2.1004, 1.2126, 1.9571, 1.6693, 1.5626],\n",
       "         [2.1541, 1.9310, 1.1782, 1.4812, 1.7868, 1.4353],\n",
       "         [2.7768, 2.2477, 1.6642, 1.9844, 1.6790, 2.0041],\n",
       "         [2.5152, 2.2736, 1.5501, 2.0298, 1.7478, 1.9106],\n",
       "         [1.9680, 1.9348, 1.2931, 1.7543, 1.4125, 1.4572],\n",
       "         [1.6052, 1.5016, 1.0296, 1.3513, 1.1270, 1.2185]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7983, 0.7544, 0.3178, 0.8524, 0.6254, 0.2304],\n",
       "         [0.7409, 0.1969, 0.3356, 0.2516, 0.6514, 0.0749],\n",
       "         [0.1134, 0.7264, 0.3448, 0.3638, 0.0577, 0.1301],\n",
       "         [0.1862, 0.5720, 0.0977, 0.6208, 0.2112, 0.1799],\n",
       "         [0.1440, 0.1869, 0.2486, 0.9734, 0.3681, 0.7415],\n",
       "         [0.2524, 0.6714, 0.4978, 0.6523, 0.4419, 0.2859],\n",
       "         [0.8092, 0.0866, 0.7331, 0.9787, 0.9501, 0.3234],\n",
       "         [0.9634, 0.5732, 0.9926, 0.5362, 0.3210, 0.5119]],\n",
       "\n",
       "        [[0.3135, 0.5302, 0.0496, 0.7245, 0.5806, 0.8404],\n",
       "         [0.8174, 0.2210, 0.6348, 0.8099, 0.0050, 0.2634],\n",
       "         [0.6039, 0.0560, 0.5329, 0.7685, 0.2352, 0.1066],\n",
       "         [0.1675, 0.6466, 0.6753, 0.3063, 0.1340, 0.7555],\n",
       "         [0.6916, 0.7769, 0.6700, 0.6257, 0.1588, 0.7063],\n",
       "         [0.1887, 0.6260, 0.5200, 0.2493, 0.6574, 0.5433],\n",
       "         [0.9361, 0.0234, 0.5398, 0.0543, 0.0987, 0.2817],\n",
       "         [0.5155, 0.6970, 0.1319, 0.6312, 0.1158, 0.1322]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7983, 0.7409, 0.1134, 0.1862, 0.1440, 0.2524, 0.8092, 0.9634],\n",
       "        [0.3135, 0.8174, 0.6039, 0.1675, 0.6916, 0.1887, 0.9361, 0.5155]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "\n",
      "Padded Tensor:\n",
      "tensor([[[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 1, 2, 0],\n",
      "         [0, 3, 4, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 5, 6, 0],\n",
      "         [0, 7, 8, 0],\n",
      "         [0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a 3D tensor\n",
    "original_tensor = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "# Specify the desired padding for each dimension\n",
    "padding_dims = (1, 1, 2, 2, 0, 0)  # (dim0_start, dim0_end, dim1_start, dim1_end, dim2_start, dim2_end)\n",
    "\n",
    "# Pad the tensor with zeros along specific dimensions\n",
    "padded_tensor = F.pad(original_tensor, padding_dims, value=0)\n",
    "\n",
    "print(\"Original Tensor:\")\n",
    "print(original_tensor)\n",
    "\n",
    "print(\"\\nPadded Tensor:\")\n",
    "print(padded_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
