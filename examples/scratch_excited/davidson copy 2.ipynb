{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decorating your function! <function KSA_XL_BOMD.one_step at 0x7f25f0bf37f0>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# === IMPORTS ===\n",
    "\n",
    "import logging, sys\n",
    "import torch\n",
    "import seqm\n",
    "from ase.io import read as ase_read\n",
    "from seqm.seqm_functions.constants import Constants\n",
    "from seqm.Molecule import Molecule\n",
    "from seqm.ElectronicStructure import Electronic_Structure\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "from seqm.seqm_functions.fock import fock\n",
    "from seqm.seqm_functions.pack import unpack\n",
    "import seqm.seqm_functions.pack as pack\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#=== TORCH OPTIONS ===\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "# else:\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "torch.set_printoptions(precision=5, linewidth=200, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colored logging with custom level QM for deeper routines\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(funcName)s : %(lineno)d : %(levelname)s : %(message)s')\n",
    "\n",
    "QM1 = evel=logging.DEBUG - 3 # informal level of depth; QM1 - almost always, usually outside of loops\n",
    "QM2 = evel=logging.DEBUG - 4 #                          QM2 - sometimes, in the loops\n",
    "QM3 = evel=logging.DEBUG - 5\n",
    "\n",
    "logging.addLevelName(QM1, \"QM1\")\n",
    "def qm1(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM1 ):\n",
    "        self._log(QM1, message, args, **kwargs) \n",
    "        \n",
    "logging.addLevelName(QM2, \"QM2\")\n",
    "def qm2(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM2):\n",
    "        self._log(QM2, message, args, **kwargs) \n",
    " \n",
    "logging.addLevelName(QM3, \"QM3\")\n",
    "def qm3(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM3 ):\n",
    "        self._log(QM3, message, args, **kwargs) \n",
    "           \n",
    "        \n",
    "logging.Logger.qm1 = qm1   \n",
    "logging.Logger.qm2 = qm2\n",
    "logging.Logger.qm3 = qm3\n",
    "  \n",
    "logger = logging.getLogger()\n",
    "\n",
    "                              \n",
    "colors = {'qm'        : ('cyan',     None, None),\n",
    "          'matrix'    : ('blue',     None, ['bold']),\n",
    "          'vector'    : ('yellow',   None, ['bold']),\n",
    "          'evals'     : ('green',    None, ['bold']),\n",
    "          'warn'     : ('red',    None, ['bold'])\n",
    "          }\n",
    "\n",
    "def fmt_log(data, message, fmt):\n",
    "    # colored output for logging module\n",
    "    if type(data) is list or type(data) is tuple or type(data) is torch.Tensor:\n",
    "        \n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])}\\n' # add new line to align array\n",
    "    else:\n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])} : '\n",
    "        \n",
    "    if data == None:\n",
    "        return mes\n",
    "    else:\n",
    "        return mes + str(colored(data, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log\n",
    "\n",
    "07/13/23 - QM part seems to be wortking fine\n",
    "full diagonalization agrees with NEXMD\n",
    "small guess space misses relevant vectors, but large guess includes them\n",
    "\n",
    "\n",
    "PASCAL 1 COULD BE INCORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CH4\n",
    "\n",
    "# 6       -0.507912712      0.000000000     -2.289086059\n",
    "# 1        0.022521233     -0.951333822     -2.212896028\n",
    "# 1        0.208634973      0.821859802     -2.231579410\n",
    "# 1       -1.224460397      0.083844265     -1.469494278\n",
    "# 1       -1.038346658      0.045629756     -3.242374518\n",
    "\n",
    "\n",
    "# H2O\n",
    "                            #   [\n",
    "                            #    [-0.368758544,      0.000000000,     -1.211413881],\n",
    "                            #    [-0.368758544,      0.759337000,     -0.615370881],\n",
    "                            #    [-0.368758544,     -0.759337000,     -0.615370881],\n",
    "                            #   ]\n",
    "                            \n",
    "# HCOOH\n",
    "# species = torch.as_tensor([[8,8,6,1,1]], # zero-padding for batching\n",
    "#                           dtype=torch.int64, device=device)\n",
    "\n",
    "# coordinates = torch.tensor([\n",
    "#                               [\n",
    "#                                [ -1.050613966,      0.651864965,     -1.406654957],\n",
    "#                                [-1.050613966,     -1.250603880,     -0.176810070],\n",
    "#                                [-1.050613966,     -0.048302968,     -0.256576324],\n",
    "#                                [-1.050613966,      0.647041884,      0.595675596],\n",
    "#                                [-1.050613966,      0.000000000,     -2.130122812],\n",
    "#                               ]\n",
    "#                             ], device=device)\n",
    "                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seqm_1mol(xyz):\n",
    "    \n",
    "    # provide only xyz, ASE will read it\n",
    "    # works for 1 MOL only\n",
    "    # returns molecule object\n",
    "    \n",
    "    atoms = ase_read(xyz)\n",
    "    species = torch.tensor([atoms.get_atomic_numbers()], dtype=torch.long, device=device)\n",
    "    coordinates = torch.tensor([atoms.get_positions()], dtype=dtype, device=device)\n",
    "    \n",
    "    const = Constants().to(device)\n",
    "\n",
    "    elements = [0]+sorted(set(species.reshape(-1).tolist()))\n",
    "\n",
    "    seqm_parameters = {\n",
    "                    'method' : 'PM3',  # AM1, MNDO, PM#\n",
    "                    'scf_eps' : 1.0e-6,  # unit eV, change of electric energy, as nuclear energy doesnt' change during SCF\n",
    "                    'scf_converger' : [2,0.0], # converger used for scf loop\n",
    "                                            # [0, 0.1], [0, alpha] constant mixing, P = alpha*P + (1.0-alpha)*Pnew\n",
    "                                            # [1], adaptive mixing\n",
    "                                            # [2], adaptive mixing, then pulay\n",
    "                    'sp2' : [False, 1.0e-5],  # whether to use sp2 algorithm in scf loop,\n",
    "                                                #[True, eps] or [False], eps for SP2 conve criteria\n",
    "                    'elements' : elements, #[0,1,6,8],\n",
    "                    'learned' : [], # learned parameters name list, e.g ['U_ss']\n",
    "                    #'parameter_file_dir' : '../seqm/params/', # file directory for other required parameters\n",
    "                    'pair_outer_cutoff' : 1.0e10, # consistent with the unit on coordinates\n",
    "                    'eig' : True,\n",
    "                    'excited' : True,\n",
    "                    }\n",
    "\n",
    "    mol = seqm.Molecule.Molecule(const, seqm_parameters, coordinates, species).to(device)\n",
    "\n",
    "    ### Create electronic structure driver:\n",
    "    esdriver = Electronic_Structure(seqm_parameters).to(device)\n",
    "\n",
    "    ### Run esdriver on m:\n",
    "    esdriver(mol)\n",
    "    \n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_L_xi_no_split(vexp1, molecule, N_cis, N_rpa, CIS = True):\n",
    "\n",
    "    # WRONG\n",
    "    \n",
    "    m = molecule\n",
    "    gss = m.parameters['g_ss']\n",
    "    gsp = m.parameters['g_sp']\n",
    "    gpp = m.parameters['g_pp']\n",
    "    gp2 = m.parameters['g_p2']\n",
    "    hsp = m.parameters['h_sp']\n",
    "    \n",
    "    mask  = m.mask\n",
    "    maskd = m.maskd\n",
    "    idxi  = m.idxi\n",
    "    idxj  = m.idxj\n",
    "    nmol  = m.nmol\n",
    "    molsize = m.molsize\n",
    "    w       = m.w\n",
    "    nHeavy = m.nHeavy\n",
    "    nHydro = m.nHydro\n",
    "    \n",
    "    eta = torch.zeros((N_rpa), device=device) \n",
    "    \n",
    "    #print('vexp1.shape', vexp1.shape)\n",
    "   # print('eta.shape', eta.shape)\n",
    "    eta[:vexp1.size(0)] = vexp1 # eta is stored as |X|; dcopy?\n",
    "    \n",
    "    eta_orig = torch.clone(eta)\n",
    "    # print('eta_orig.shape', eta_orig.shape)\n",
    "    # print('eta_orig\\n', eta_orig)\n",
    "    \n",
    "   # print('m.C MO', m.C_mo[0])\n",
    "    # print('eta.shape', eta.shape)\n",
    "    # print(eta)\n",
    "    eta_ao =  mo2ao(N_cis, eta, m, full=False)     # mo to ao basis (mo2site)\n",
    "    # print('eta_ao.shape', eta_ao.shape)\n",
    "    # print(eta_ao)\n",
    "\n",
    "    # eta_ao_sym, eta_ao_asym = decompose_to_sym_antisym(eta_ao) # decompose to sym and asym\n",
    "\n",
    "    # Vxi - build 2e integrals in AO basis: G(guess density) in F = H_core + G\n",
    "    # note density is split into sym and anisym matrices\n",
    "    # sym is processed as padded 3d array in PYSEQM, see fock module \n",
    "    # antisym: 2c-2e works with modified PYSEQM routine; should be antisimmterized afterwards\n",
    "    # antisym: 1c-2e (diagonal) are taken from NEXMD for now - ugly code with loops\n",
    "    # TODO: vectorize 1c-2e part\n",
    "    \n",
    "    #------------------symmetric------------------------------\n",
    "    G_sym   =  build_G_sym(eta_ao,\n",
    "                        gss, gsp, gpp, gp2, hsp,\n",
    "                        mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                        w,\n",
    "                         nHeavy,\n",
    "                         nHydro)\n",
    "    # G sym is 1c-2e and 2c-2e of symmetric part of guess density\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pack 2c-2e part to standard shape\n",
    "    G_sym = pack.pack(G_sym, nHeavy, nHydro)\n",
    "    # print('G_sym \\n', G_sym)\n",
    "    \n",
    "    # G_tot = build_G_antisym(eta_ao, eta_ao_asym, G_sym,\n",
    "    #                         gss, gsp, gpp, gp2, hsp,\n",
    "    #                         mask, maskd, idxi, idxj, nmol, molsize,\n",
    "    #                         w, \n",
    "    #                         m,\n",
    "    #                         nHeavy,\n",
    "    #                         nHydro)\n",
    "                        \n",
    "    # build_G_antisym returns both sym and antisym!\n",
    "    # TODO: refactor into: 2c-2e antisym, 1c-2e antisym\n",
    "    # TODO: vectorize 1c-2e antisym, avoid ugly loops\n",
    "    #! remember about making 2c-2e diagonal 0\n",
    "\n",
    "    # print('G total \\n', G_tot)\n",
    "    4\n",
    "    # print('============================================')\n",
    "    # print('Converting Gao full back into MO basis')\n",
    "    G_mo = ao2mo(N_cis, N_rpa, m, G_sym[0], m.C_mo, full=False) # G in MO basis #! [0] not batched yet\n",
    "    \n",
    "    # multiply by MO differencies\n",
    "    ii=0\n",
    "    for p in range(m.nocc):\n",
    "    # print('p', p)\n",
    "        for h in range(m.nocc, m.norb):\n",
    "            # print('h', h)\n",
    "            # print('i', i)\n",
    "            f = m.e_mo[0][h] - m.e_mo[0][p]\n",
    "            G_mo[ii] = G_mo[ii] + f * eta_orig[ii]\n",
    "            G_mo[ii+N_cis] = -G_mo[ii + N_cis] + f * eta_orig[ii+N_cis]\n",
    "            ii += 1\n",
    "        \n",
    "    # print('G_mo.shape', G_mo.shape)\n",
    "    # print('G_mo\\n', G_mo)\n",
    "    return G_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_L_xi(vexp1, molecule, N_cis, N_rpa, CIS = True):\n",
    "\n",
    "    m = molecule\n",
    "    gss = m.parameters['g_ss']\n",
    "    gsp = m.parameters['g_sp']\n",
    "    gpp = m.parameters['g_pp']\n",
    "    gp2 = m.parameters['g_p2']\n",
    "    hsp = m.parameters['h_sp']\n",
    "    \n",
    "    mask  = m.mask\n",
    "    maskd = m.maskd\n",
    "    idxi  = m.idxi\n",
    "    idxj  = m.idxj\n",
    "    nmol  = m.nmol\n",
    "    molsize = m.molsize\n",
    "    w       = m.w\n",
    "    nHeavy = m.nHeavy\n",
    "    nHydro = m.nHydro\n",
    "    \n",
    "    eta = torch.zeros((N_rpa), device=device) \n",
    "    \n",
    "    #print('vexp1.shape', vexp1.shape)\n",
    "   # print('eta.shape', eta.shape)\n",
    "    eta[:vexp1.size(0)] = vexp1 # eta is stored as |X|; dcopy?\n",
    "    \n",
    "    eta_orig = torch.clone(eta)\n",
    "    # print('eta_orig.shape', eta_orig.shape)\n",
    "    # print('eta_orig\\n', eta_orig)\n",
    "    \n",
    "   # print('m.C MO', m.C_mo[0])\n",
    "    # print('eta.shape', eta.shape)\n",
    "    # print(eta)\n",
    "    eta_ao =  mo2ao(N_cis, eta, m, full=False)     # mo to ao basis (mo2site)\n",
    "    # print('eta_ao.shape', eta_ao.shape)\n",
    "    # print(eta_ao)\n",
    "\n",
    "    eta_ao_sym, eta_ao_asym = decompose_to_sym_antisym(eta_ao) # decompose to sym and asym\n",
    "\n",
    "    # Vxi - build 2e integrals in AO basis: G(guess density) in F = H_core + G\n",
    "    # note density is split into sym and anisym matrices\n",
    "    # sym is processed as padded 3d array in PYSEQM, see fock module \n",
    "    # antisym: 2c-2e works with modified PYSEQM routine; should be antisimmterized afterwards\n",
    "    # antisym: 1c-2e (diagonal) are taken from NEXMD for now - ugly code with loops\n",
    "    # TODO: vectorize 1c-2e part\n",
    "    \n",
    "    #------------------symmetric------------------------------\n",
    "    G_sym   =  build_G_sym(eta_ao_sym,\n",
    "                        gss, gsp, gpp, gp2, hsp,\n",
    "                        mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                        w,\n",
    "                         nHeavy,\n",
    "                         nHydro)\n",
    "    \n",
    "   # print('G_SYM\\n', G_sym)\n",
    "    # G sym is 1c-2e and 2c-2e of symmetric part of guess density\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pack 2c-2e part to standard shape\n",
    "    G_sym = pack.pack(G_sym, nHeavy, nHydro)\n",
    "    # print('G_sym \\n', G_sym)\n",
    "    \n",
    "    G_tot = build_G_antisym(eta_ao, eta_ao_asym, G_sym,\n",
    "                            gss, gsp, gpp, gp2, hsp,\n",
    "                            mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                            w, \n",
    "                            m,\n",
    "                            nHeavy,\n",
    "                            nHydro)\n",
    "    \n",
    "   # print('G_tot.shape', G_tot.shape)\n",
    "   # print('G_tot\\n', G_tot)                  \n",
    "    # build_G_antisym returns both sym and antisym!\n",
    "    # TODO: refactor into: 2c-2e antisym, 1c-2e antisym\n",
    "    # TODO: vectorize 1c-2e antisym, avoid ugly loops\n",
    "    #! remember about making 2c-2e diagonal 0\n",
    "\n",
    "    # print('G total \\n', G_tot)\n",
    "    \n",
    "    # print('============================================')\n",
    "    # print('Converting Gao full back into MO basis')\n",
    "    G_mo = ao2mo(N_cis, N_rpa, m, G_tot[0], m.C_mo, full=False) # G in MO basis #! [0] not batched yet\n",
    "    \n",
    "    # print('G_mo.shape', G_mo.shape)\n",
    "    # print('G_mo\\n', G_mo)\n",
    "    # multiply by MO differencies\n",
    "    ii=0\n",
    "    for p in range(m.nocc):\n",
    "    # print('p', p)\n",
    "        for h in range(m.nocc, m.norb):\n",
    "            # print('h', h)\n",
    "            # print('i', i)\n",
    "            f = m.e_mo[0][h] - m.e_mo[0][p]\n",
    "            G_mo[ii] = G_mo[ii] + f * eta_orig[ii]\n",
    "            G_mo[ii+N_cis] = -G_mo[ii + N_cis] + f * eta_orig[ii+N_cis]\n",
    "            ii += 1\n",
    "        \n",
    "    # print('G_mo.shape', G_mo.shape)\n",
    "    # print('G_mo\\n', G_mo)\n",
    "    return G_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_to_sym_antisym(A):\n",
    "        \n",
    "\n",
    "    A_sym = 0.5 * (A + A.T)\n",
    "    A_antisym = 0.5 * (A - A.T)\n",
    "    \n",
    "    return A_sym, A_antisym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_G_sym(M_ao,\n",
    "                gss, gsp, gpp, gp2, hsp,\n",
    "                mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                w,\n",
    "                nHydro,\n",
    "                nHeavy):\n",
    "    \n",
    "    \n",
    "      F = torch.zeros((nmol*molsize**2,4,4), device=device) # 0 Fock matrix to fill\n",
    "      # # TODO: feed params programmatically\n",
    "      \n",
    "      P0 = unpack(M_ao, nHydro, nHeavy, (nHeavy+nHydro)*4) # \n",
    "      P0 = torch.unsqueeze(P0, 0) # add dimension\n",
    "      \n",
    "      # print('P0.shape', P0.shape)\n",
    "      # print('P0\\n', P0)\n",
    "      #---------------fill diagonal 1c-2e -------------------\n",
    "      P = P0.reshape((nmol,molsize,4,molsize,4)) \\\n",
    "          .transpose(2,3).reshape(nmol*molsize*molsize,4,4)\n",
    "          \n",
    "      # print('P.shape', P.shape)\n",
    "      # print('P\\n', P)\n",
    "      \n",
    "      Pptot = P[...,1,1]+P[...,2,2]+P[...,3,3]\n",
    "      ## http://openmopac.net/manual/1c2e.html\n",
    "    #  (s,s)\n",
    "      TMP = torch.zeros_like(F)\n",
    "      TMP[maskd,0,0] = 0.5*P[maskd,0,0]*gss + Pptot[maskd]*(gsp-0.5*hsp)\n",
    "      for i in range(1,4):\n",
    "          #(p,p)\n",
    "          TMP[maskd,i,i] = P[maskd,0,0]*(gsp-0.5*hsp) + 0.5*P[maskd,i,i]*gpp \\\n",
    "                          + (Pptot[maskd] - P[maskd,i,i]) * (1.25*gp2-0.25*gpp)\n",
    "          #(s,p) = (p,s) upper triangle\n",
    "          TMP[maskd,0,i] = P[maskd,0,i]*(1.5*hsp - 0.5*gsp)\n",
    "      #(p,p*)\n",
    "      for i,j in [(1,2),(1,3),(2,3)]:\n",
    "          TMP[maskd,i,j] = P[maskd,i,j]* (0.75*gpp - 1.25*gp2)\n",
    "\n",
    "      F.add_(TMP)\n",
    "      \n",
    "           \n",
    "      #-----------------fill 2c-2e integrals----------------\n",
    "      weight = torch.tensor([1.0,\n",
    "                        2.0, 1.0,\n",
    "                        2.0, 2.0, 1.0,\n",
    "                        2.0, 2.0, 2.0, 1.0],dtype=dtype, device=device).reshape((-1,10))\n",
    "      \n",
    "      PA = (P[maskd[idxi]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,10,1))\n",
    "      PB = (P[maskd[idxj]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,1,10))\n",
    "      suma = torch.sum(PA*w,dim=1)\n",
    "      sumb = torch.sum(PB*w,dim=2)\n",
    "      sumA = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      sumB = torch.zeros_like(sumA)\n",
    "      \n",
    "      sumA[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = suma\n",
    "      sumB[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = sumb\n",
    "      F.index_add_(0,maskd[idxi],sumB)\n",
    "      #\\sum_A\n",
    "      F.index_add_(0,maskd[idxj],sumA)\n",
    "      \n",
    "      sum = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      # (ss ), (px s), (px px), (py s), (py px), (py py), (pz s), (pz px), (pz py), (pz pz)\n",
    "      #   0,     1         2       3       4         5       6      7         8        9\n",
    "\n",
    "      ind = torch.tensor([[0,1,3,6],\n",
    "                          [1,2,4,7],\n",
    "                          [3,4,5,8],\n",
    "                          [6,7,8,9]],dtype=torch.int64, device=device)\n",
    "      \n",
    "      Pp = -0.5*P[mask]\n",
    "      for i in range(4):\n",
    "        for j in range(4):\n",
    "            #\\sum_{nu \\in A} \\sum_{sigma \\in B} P_{nu, sigma} * (mu nu, lambda, sigma)\n",
    "            sum[...,i,j] = torch.sum(Pp*w[...,ind[i],:][...,:,ind[j]],dim=(1,2))\n",
    "      #print('mask', mask)    #! DIFFERS FROM PYSEQM, PROBABLY packing\n",
    "      F.index_add_(0,mask,sum)\n",
    "\n",
    "      F0 = F.reshape(nmol,molsize,molsize,4,4) \\\n",
    "             .transpose(2,3) \\\n",
    "             .reshape(nmol, 4*molsize, 4*molsize)\n",
    "    #\n",
    "      F0.add_(F0.triu(1).transpose(1,2))     \n",
    "      \n",
    "      F0 = 2 * F0 #! BE CAREFUL\n",
    "      # print('F0.shape', F0.shape)\n",
    "      # print(F0) \n",
    "      \n",
    "      return F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_G_antisym(eta_ao, eta_ao_asym, G_sym,\n",
    "                gss, gsp, gpp, gp2, hsp,\n",
    "                mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                w, \n",
    "                m,\n",
    "                nHydro,\n",
    "                nHeavy):\n",
    "\n",
    "      # TODO; figure how/why constants are defined in fock_skew\n",
    "      #!\n",
    "      #! CHECK\n",
    "      #!\n",
    "      # gss = torch.tensor([15.7558, 14.7942, 14.7942])  # GSSII   7.87788  7.39710   7.39710   1/2 # not used\n",
    "      # gpp = torch.tensor([13.6540,  0.0000,  0.0000])  # GPPII   6.82700   1/2\n",
    "      # gsp = torch.tensor([10.6212,  0.0000,  0.0000])  # GSPII   10.6211    \n",
    "      # gp2 = torch.tensor([12.4061,  0.0000,  0.0000])  # GP2II   15.5076\n",
    "      # hsp = torch.tensor([0.5939, 0.0000, 0.0000])     # HSPII   0.29694   1/2\n",
    "  \n",
    "      # see fock_skew in qm_fock in NEXMD\n",
    "      # create 1d array \n",
    "      eta_anti = torch.zeros((m.norb*(m.norb+1)//2), device=device)\n",
    "      \n",
    "      # l=0\n",
    "      # for i in range(m.norb):\n",
    "      #       # print('i', i)\n",
    "      #       for j in range(i+1):\n",
    "      #             print('i j', i, j)\n",
    "\n",
    "      #             eta_anti[l] = 0.5 * (eta_ao[i,j] - eta_ao[j,i])\n",
    "      #             l += 1   \n",
    "      #             # print('l', l) \n",
    "      # print('eta_anti NEXMD\\n')\n",
    "      # for e in eta_anti: print(f\"{e:.4f}\")\n",
    "      \n",
    "      eta_anti = torch.zeros((m.norb*(m.norb+1)//2), device=device)\n",
    "      indices = torch.tril_indices(int(m.norb), int(m.norb), offset = 0)  # Generate the upper triangular indices\n",
    "      \n",
    "      # print('indices\\n', indices)\n",
    "      # print('indices[0]\\n', indices[0].size())\n",
    "      eta_anti = 0.5 * (eta_ao[indices[0], indices[1]] - eta_ao[indices[1], indices[0]])\n",
    "\n",
    "      # print('eta_anti vectorized\\n')\n",
    "      # for e in eta_anti: print(f\"{e:.4f}\")\n",
    "      \n",
    "      \n",
    "      # print tensor, one element per line\n",
    "      \n",
    "      eta_anti_2d = torch.zeros((m.norb, m.norb), device='cpu') \n",
    "\n",
    "      #restore to 2d form to build G 2c-2e part\n",
    "      # l = 0\n",
    "      # for i in range(0, m.norb): # TODO\" vectorize\n",
    "      #     for j in range(0,i):\n",
    "      #         l += 1\n",
    "      #         eta_anti_2d[i,j] += eta_anti[l-1]\n",
    "      #         eta_anti_2d[j,i] -= eta_anti[l-1]\n",
    "      #     l += 1 \n",
    "      # print('eta_anti_2d\\n', eta_anti_2d)\n",
    "      \n",
    "      # eta_anti_2d_vec = torch.zeros((m.norb, m.norb))     \n",
    "      \n",
    "      \n",
    "      eta_anti_2d[indices[1], indices[0]] = -eta_anti \n",
    "      eta_anti_2d = eta_anti_2d - eta_anti_2d.T # antisymmetrize\n",
    "      \n",
    "      # print('eta_anti_2d VEC\\n', eta_anti_2d)\n",
    "      #eta_anti_2d_vec.fill_diagonal_(0)\n",
    "\n",
    "      # print('maskd\\n', maskd)\n",
    "\n",
    "      # TODO: should be vectorized as in build G\n",
    "      # below explicit working example for H2O taken from NEXMD\n",
    "      # pascal2 = [1, 3, 6, 10, 15, 21]  # -1 pascal triangle\n",
    "      # pascal2 = [x -1 for x in pascal2]\n",
    "      # pascal1 = [0, 1, 3, 6, 10, 15] # -1 for python indexing\n",
    "      # pascal1 = [x -1 for x in pascal1]\n",
    "      # orb_loc1 = [0,4,5] # O orbs\n",
    "      # orb_loc2 = [3,4,5]\n",
    "      pascal2 = torch.cumsum(torch.arange(1, eta_anti_2d.shape[0]+1), dim=0)  # -1 pascal triangle\n",
    "      # pascal2 = [-1, 0, 2, 5, 9, 14]\n",
    "      pascal2 = pascal2 -1 \n",
    "      # print('pascal2\\n', pascal2)\n",
    "      \n",
    "      pascal1 = torch.cumsum(torch.arange(0, eta_anti_2d.shape[0]), dim=0) \n",
    "      pascal1 = pascal1 - 1\n",
    "      # pascal1 = [0,  2,  5,  9, 14, 20, 27]\n",
    "      # pascal1 = [-1, 0, 2, 5, 9, 14]\n",
    "      # print('pascal1\\n', pascal1)\n",
    "      # print(m.Z)\n",
    "      \n",
    "      # ! TODO: write programamtically\n",
    "      \n",
    "      # orb_loc1 = [0] # [start of orbitals index of x]\n",
    "      # orb_loc2 = [1 if m.Z[0] == 1 else 3] # [end of orbitals index of x]\n",
    "      orb_loc1 = []\n",
    "      orb_loc2 = []\n",
    "      # orb_loc1 = [orb_loc1[i - 1] + 4 if m.Z[i] != 1 else orb_loc1[i - 1] + 1 for i, x in enumerate(m.Z)]\n",
    "      Z = m.Z\n",
    "      for i,z in enumerate(Z):\n",
    "\n",
    "        if i == 0:\n",
    "          orb_loc1.append(0)\n",
    "          if Z[i] != 1:\n",
    "            orb_loc2.append(3)\n",
    "          else:\n",
    "            orb_loc2.append(0)\n",
    "          continue\n",
    "        \n",
    "        if Z[i-1] != 1:\n",
    "            orb_loc1.append(orb_loc1[i-1]+4)\n",
    "        elif Z[i-1] == 1:\n",
    "            orb_loc1.append(orb_loc1[i-1]+1)\n",
    "          \n",
    "        if Z[i] != 1:\n",
    "            orb_loc2.append(orb_loc2[i-1]+4)\n",
    "        elif Z[i] == 1:    \n",
    "            orb_loc2.append(orb_loc2[i-1]+1)\n",
    "          \n",
    "             \n",
    "      # print('orb_loc1\\n', orb_loc1)\n",
    "      # print('orb_loc2\\n', orb_loc2)\n",
    "            \n",
    "      # orb_loc1 = [0,4,5]\n",
    "      G_1c2e = torch.zeros((m.norb*(m.norb+1)//2))\n",
    "    \n",
    "      for ii in range(molsize): # n_atoms?\n",
    "      # print('ii', ii)\n",
    "        if m.Z[ii] == 1:\n",
    "          pass\n",
    "        \n",
    "        else:\n",
    "          gsp_ii = gsp[ii]\n",
    "          gpp_ii = gpp[ii]\n",
    "          gp2_ii = gp2[ii]\n",
    "          hsp_ii = hsp[ii]\n",
    "          \n",
    "          ia = orb_loc1[ii]\n",
    "          # print('ia', ia)\n",
    "          ib = orb_loc2[ii]\n",
    "          \n",
    "          iplus = ia+1\n",
    "          ka = pascal2[ia]\n",
    "          l = ka\n",
    "\n",
    "          for j in range(iplus, ib+1):\n",
    "          # print('j', j)\n",
    "          # print('ia', ia)\n",
    "          # print('ib', ib)\n",
    "          # print('l', l)\n",
    "\n",
    "            mm = l+ia+1\n",
    "            l = l+j+1\n",
    "            \n",
    "            # print(type(mm)) \n",
    "            G_1c2e[mm] = G_1c2e[mm] + 0.5*eta_anti[mm] * (hsp_ii - gsp_ii)\n",
    "            # print('(hsp - gsp)', (hsp - gsp))\n",
    "            # print('F(M) FIRST', G_1c2e[mm])\n",
    "           # print('G_1c2e[mm]', G_1c2e[mm])\n",
    "          #  print('===================')\n",
    "            \n",
    "          iminus = ib-1\n",
    "        \n",
    "          for j in range(iplus, iminus+1):\n",
    "            icc = j\n",
    "            # print('icc', icc)\n",
    "            for l in range(icc, ib):\n",
    "              # print('l', l)\n",
    "              mm = pascal1[l+1] + j+1\n",
    "              # print('mm', mm)\n",
    "              # print('(0.25*gpp_ii - 0.6*gp2_ii)', (0.25*gpp_ii - 0.6*gp2_ii) )\n",
    "              G_1c2e[mm] = G_1c2e[mm]+ eta_anti[mm] * (0.25*gpp_ii - 1.25*0.6*gp2_ii) \n",
    "              # print('F(M) SECOND', G_1c2e[mm])\n",
    "            #  print('G_1c2e[mm]', G_1c2e[mm])\n",
    "            \n",
    "      G_1c2e = G_1c2e*2 # antisym 1c2e part\n",
    "      # print('G_1c2e\\n', G_1c2e)\n",
    "      \n",
    "\n",
    "      # buils antisymmetric part as Vxi_packA, requires G_sym\n",
    "      \n",
    "   #   print('G_sym shape', G_sym.shape)\n",
    "   #   print('G_sym\\n', G_sym)\n",
    "      \n",
    "     # print('molsize', molsize)\n",
    "     \n",
    "      G_sym = G_sym[0] #! works for one mol only?\n",
    "      G_sym_orig = G_sym.clone()\n",
    "      # print('G_sym shape', G_sym.shape)\n",
    "      # print('G_sym\\n', G_sym)\n",
    "      \n",
    "      # l = 0\n",
    "      # for i in range(0, m.norb): # TODO vectorize\n",
    "      #     for j in range(0,i):\n",
    "      #         l += 1\n",
    "\n",
    "      #         G_sym[i,j] += G_1c2e[l-1]\n",
    "      #         G_sym[j,i] -= G_1c2e[l-1]             \n",
    "      #     l += 1 # skip diagonal\n",
    "      # print('G sym after ADDING ANTSYM PART')\n",
    "      # print('G_sym\\n', G_sym)\n",
    "      \n",
    "      # pack from 1d to 2d ANTISYM 1c2e part of G\n",
    "      G_anti_1c2e = torch.zeros(m.norb, m.norb)\n",
    "      G_anti_1c2e[indices[1], indices[0]] = -G_1c2e\n",
    "      G_anti_1c2e = G_anti_1c2e - G_anti_1c2e.T\n",
    "      \n",
    "      # print('G_anti_1c2e\\n', G_anti_1c2e)\n",
    "      # print(G_anti_1c2e.shape)\n",
    "      \n",
    "      # print('SUM')\n",
    "      # print(G_sym_orig + G_anti_1c2e)\n",
    "    \n",
    "      # print('G_anti_1c shape', G_anti_1c.shape)\n",
    "      # print('G_anti_1c\\n', G_anti_1c)\n",
    "      \n",
    "    #  print('eta_ao_asym shape', eta_ao_asym.shape)\n",
    "    #  print('eta_ao_asym\\n', eta_ao_asym)\n",
    "      \n",
    "      # G_tmp = 2* G_anti_1c + G_sym\n",
    "      G_sym = torch.unsqueeze(G_sym, 0) #  add dimension\n",
    "\n",
    "      # build 2c-2e part of antisymmetric G\n",
    "      # copied from FOCK\n",
    "      \n",
    "      F = torch.zeros((nmol*molsize**2,4,4), device=device) # 0 Fock matrix to fill\n",
    "      P0 = unpack(eta_ao_asym, nHydro, nHeavy, (nHeavy+nHydro)*4) # \n",
    "      P0 = torch.unsqueeze(P0, 0) # add dimension\n",
    "      \n",
    "      P = P0.reshape((nmol,molsize,4,molsize,4)) \\\n",
    "          .transpose(2,3).reshape(nmol*molsize*molsize,4,4)\n",
    "          \n",
    "      #P = P[...,1,1]+P[...,2,2]+P[...,3,3] #! MODIFIED\n",
    "      #-----------------fill 2c-2e integrals----------------\n",
    "      weight = torch.tensor([1.0,\n",
    "                        2.0, 1.0,\n",
    "                        2.0, 2.0, 1.0,\n",
    "                        2.0, 2.0, 2.0, 1.0],dtype=dtype, device=device).reshape((-1,10))\n",
    "      \n",
    "      PA = (P[maskd[idxi]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,10,1))\n",
    "      PB = (P[maskd[idxj]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,1,10))\n",
    "      suma = torch.sum(PA*w,dim=1)\n",
    "      sumb = torch.sum(PB*w,dim=2)\n",
    "      sumA = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      sumB = torch.zeros_like(sumA)\n",
    "      \n",
    "      sumA[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = suma\n",
    "      sumB[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = sumb\n",
    "      F.index_add_(0,maskd[idxi],sumB)\n",
    "      #\\sum_A\n",
    "      F.index_add_(0,maskd[idxj],sumA)\n",
    "      \n",
    "      sum = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      # (ss ), (px s), (px px), (py s), (py px), (py py), (pz s), (pz px), (pz py), (pz pz)\n",
    "      #   0,     1         2       3       4         5       6      7         8        9\n",
    "\n",
    "      ind = torch.tensor([[0,1,3,6],\n",
    "                          [1,2,4,7],\n",
    "                          [3,4,5,8],\n",
    "                          [6,7,8,9]],dtype=torch.int64, device=device)\n",
    "      \n",
    "      Pp = -0.5*P[mask]\n",
    "      for i in range(4):\n",
    "        for j in range(4):\n",
    "            #\\sum_{nu \\in A} \\sum_{sigma \\in B} P_{nu, sigma} * (mu nu, lambda, sigma)\n",
    "            sum[...,i,j] = torch.sum(Pp*w[...,ind[i],:][...,:,ind[j]],dim=(1,2))\n",
    "     # print('mask', mask)    #! DIFFERS FROM PYSEQM, PROBABLY packing\n",
    "      F.index_add_(0,mask,sum)\n",
    "\n",
    "      F0 = F.reshape(nmol,molsize,molsize,4,4) \\\n",
    "             .transpose(2,3) \\\n",
    "             .reshape(nmol, 4*molsize, 4*molsize)\n",
    "    #\n",
    "      F0.add_(F0.triu(1).transpose(1,2))     \n",
    "      \n",
    "      # F0 is still symmetric, probably symmetrized above\n",
    "      # here we make it antisymmetric back\n",
    "      #F0 = 2 * F0 \n",
    "      F0 = pack.pack(F0, m.nHeavy, m.nHydro)\n",
    "      \n",
    "      rows, cols = torch.tril_indices(F0.shape[1], F0.shape[2])\n",
    "      F0[0][rows, cols] *= -1\n",
    "      F0[0][torch.eye(F0.shape[1]).bool()] *= -1\n",
    "      \n",
    "      \n",
    "\n",
    "      F0[0].diagonal().fill_(0) #! BE WARNED, THIS iS TAKEN FROM OLD NEXMD, PYSEQM produces non-zero diagonal\n",
    "      F0 = F0*2\n",
    "    #  print('G ANTISYM shape', F0.shape)\n",
    "    #  print('G ANTISYM\\n', F0*2)\n",
    "      G_full = G_sym + G_anti_1c2e + F0 # summ of G_sym(sym 1c2e + 2c2e) + antisym 1c2e and 2c2e (F0)\n",
    "      # print('G_full shape', G_full.shape)\n",
    "      # print('G_full\\n', G_full)\n",
    "      \n",
    "      return G_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ao2mo(N_cis, N_rpa, molecule, M_ao, C, full=False):\n",
    "    \"\"\"\n",
    "    transform matrix from AO to MO basis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M_AO : torch tensor # TODO add size\n",
    "        matrix in AO basis\n",
    "    C : torch tensor # TODO add size\n",
    "        matrix of MO coefficients # TODO row or columns, structure?\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    M_MO : torch tensor # TODO add size\n",
    "        matrix in MO basis\n",
    "    \"\"\"    \n",
    "    m = molecule\n",
    "    \n",
    "    if full == True:\n",
    "        M_mo = C.T @ M_ao @ C\n",
    "        return M_mo\n",
    "        \n",
    "    else:\n",
    "         # COPY of subroutine site2mo from Lioville\n",
    "         \n",
    "        G_ao = M_ao # TODO rename\n",
    "        \n",
    "        # eta1 = eta1.view(-1, m.nvirt[0]) # 1d -> 2d\n",
    "        # print(eta1.shape)\n",
    "        # print('eta1', eta1)\n",
    "        # print('==============')\n",
    "        \n",
    "        eta_mo = torch.zeros((N_rpa))\n",
    "       # eta_mo = torch.zeros((m.norb, m.norb), device=device)\n",
    "\n",
    "        dgemm1 = G_ao.T @ m.C_mo[0]\n",
    "\n",
    "        # print('dgemm1.shape', dgemm1.shape)\n",
    "        # print(dgemm1)\n",
    "        \n",
    "        dgemm2 =  m.C_mo[0][:, m.nocc:m.norb].T @ dgemm1[:,:m.nocc]\n",
    "        \n",
    "\n",
    "        dgemm2 = dgemm2.T.flatten()\n",
    "        eta_mo[:dgemm2.size(0)] = dgemm2 \n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "        \n",
    "        dgemm3 =  dgemm1[:, m.nocc:].T @ m.C_mo[0][:, :m.nocc]\n",
    "        \n",
    "        # print('dgemm3.T.shape', dgemm3.T.shape)\n",
    "        # print(dgemm3.T)\n",
    "        \n",
    "        eta_mo[N_cis:] = dgemm3.T.flatten() \n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "\n",
    "        M_mo = eta_mo\n",
    "    \n",
    "    return M_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mo2ao(N_cis, M_mo, molecule, full=False):\n",
    "    \"\"\"\n",
    "    transform matrix from AO to MO basis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M_AO : torch tensor # TODO add size\n",
    "        matrix in AO basis\n",
    "    C : torch tensor # TODO add size\n",
    "        matrix of MO coefficients # TODO row or columns, structure?\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    M_MO : torch tensor # TODO add size\n",
    "        matrix in MO basis\n",
    "    \"\"\"    \n",
    "    m = molecule\n",
    "    \n",
    "    # print('m C_mo', m.C_mo)\n",
    "    if full == True:\n",
    "        M_ao = C.T @ M_mo @ C #! does not currently work\n",
    "        \n",
    "        return M_ao\n",
    "    else:\n",
    "        \n",
    "        eta = M_mo # TODO rename\n",
    "        \n",
    "        eta1 = eta[:N_cis]\n",
    "        eta1 = eta1.view(-1, m.nvirt[0]) # 1d -> 2d\n",
    "        # print(eta1.shape)\n",
    "        # print('eta1', eta1)\n",
    "        # print('==============')\n",
    "        \n",
    "        eta_mo = torch.zeros((m.norb, m.norb), device=device)\n",
    "\n",
    "        dgemm1 = eta1 @ m.C_mo[0][:, m.nocc:m.norb].T # operations on |X| ?\n",
    "\n",
    "        # print('dgemm1.shape', dgemm1.shape)\n",
    "        # print(dgemm1)\n",
    "        \n",
    "        eta_mo[:m.nocc] = dgemm1\n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "        \n",
    "        \n",
    "        eta2 = eta[N_cis:]                            # operations on |Y| ?\n",
    "        eta2 = eta2.view(-1, m.nvirt[0]) # 1d -> 2d\n",
    "    \n",
    "        dgemm2 = eta2.T @ m.C_mo[0][:, :m.nocc].T\n",
    "        \n",
    "        # print('dgemm2.shape', dgemm2.shape)\n",
    "        # print(dgemm2)\n",
    "        \n",
    "        eta_mo[m.nocc:] = dgemm2\n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "        \n",
    "        dgemm3 = m.C_mo[0] @ eta_mo\n",
    "        eta_ao = dgemm3 \n",
    "    \n",
    "    \n",
    "        return eta_ao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUX routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize_torch(U, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Orthogonalizes the matrix U (d x n) using Gram-Schmidt Orthogonalization.\n",
    "    If the columns of U are linearly dependent with rank(U) = r, the last n-r columns \n",
    "    will be 0.\n",
    "    \n",
    "    Args:\n",
    "        U (numpy.array): A d x n matrix with columns that need to be orthogonalized.\n",
    "        eps (float): Threshold value below which numbers are regarded as 0 (default=1e-15).\n",
    "    \n",
    "    Returns:\n",
    "        (numpy.array): A d x n orthogonal matrix. If the input matrix U's cols were\n",
    "            not linearly independent, then the last n-r cols are zeros.\n",
    "    \n",
    "    Examples:\n",
    "    ```python\n",
    "    >>> import numpy as np\n",
    "    >>> import gram_schmidt as gs\n",
    "    >>> gs.orthogonalize(np.array([[10., 3.], [7., 8.]]))\n",
    "    array([[ 0.81923192, -0.57346234],\n",
    "       [ 0.57346234,  0.81923192]])\n",
    "    >>> gs.orthogonalize(np.array([[10., 3., 4., 8.], [7., 8., 6., 1.]]))\n",
    "    array([[ 0.81923192 -0.57346234  0.          0.        ]\n",
    "       [ 0.57346234  0.81923192  0.          0.        ]])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(U[0])\n",
    "    # numpy can readily reference rows using indices, but referencing full rows is a little\n",
    "    # dirty. So, work with transpose(U)\n",
    "    V = U.T\n",
    "    for i in range(n):\n",
    "        prev_basis = V[0:i]     # orthonormal basis before V[i]\n",
    "        coeff_vec = prev_basis @ V[i].T  # each entry is np.dot(V[j], V[i]) for all j < i\n",
    "        # subtract projections of V[i] onto already determined basis V[0:i]\n",
    "        V[i] -= (coeff_vec @ prev_basis).T\n",
    "        if torch.norm(V[i]) < eps:\n",
    "            V[i][V[i] < eps] = 0.   # set the small entries to 0\n",
    "        else:\n",
    "            V[i] /= torch.norm(V[i])\n",
    "    return V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_V(N_cis, N_rpa, n_V_start):\n",
    "    \n",
    "    # returns vexp1 - guess vector for L-xi routine\n",
    "    logger.qm2(fmt_log(n_V_start, 'n_V_start', 'qm'))\n",
    "    rrwork = torch.zeros(N_rpa * 4, device=device)\n",
    "    i = 0\n",
    "    for ip in range(mol.nocc):\n",
    "        for ih in range(mol.nvirt):\n",
    "            rrwork[i] = mol.e_mo[0][mol.nocc + ih] - mol.e_mo[0][ip]  # !Lancos vectors(i) ???\n",
    "            i += 1                                                                      #  TODO: [0] should be replaced by m batch index\n",
    "                                                                                    \n",
    "    rrwork_sorted, indices = torch.sort(rrwork[:N_cis], descending=False, stable=True) # preserve order of degenerate\n",
    "    logger.qm2(fmt_log(rrwork_sorted, 'rrwork_sorted', 'qm'))\n",
    "\n",
    "\n",
    "    # vexp1 = torch.zeros((N_cis, N_cis), device=device)\n",
    "    vexp1 = torch.zeros((N_cis, N_cis), device=device)\n",
    "    \n",
    "    row_idx = torch.arange(0, int(N_cis), device=device)\n",
    "    col_idx = indices[:N_cis]\n",
    "\n",
    "    vexp1[row_idx, col_idx] = 1.0 \n",
    "    logger.qm2(fmt_log(vexp1, 'V  BEFORE SELECTING PART', 'qm'))\n",
    "    logger.qm2(fmt_log(vexp1.shape, 'V shape', 'qm'))\n",
    "   #! THIS IS NEW, TAKE only part \n",
    "\n",
    "    V = vexp1[:,  :n_V_start]\n",
    "    logger.qm2(fmt_log(V, 'V = vexp1', 'qm'))\n",
    "    logger.qm2(fmt_log(V.shape, 'V shape', 'qm'))\n",
    "\n",
    "    return V\n",
    "\n",
    "# TODO: check whether L_xi should be regenerated during expansion each time or just part of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAVIDSON routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(QM1)  # custom logging level; lower than DEBUG\n",
    "                               # printed above QM (QM, DEBUG, INFO, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.qm3('Hello from QM2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<module> : 3 : DEBUG : i = 0\n",
      "<module> : 3 : DEBUG : i = 1\n",
      "<module> : 3 : DEBUG : i = 2\n",
      "<module> : 3 : DEBUG : i = 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    \n",
    "    logger.debug('i = %d', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def davidson(n_V_max,  max_iter, tol):\n",
    "    \n",
    "    N_exc = 2\n",
    "    n_V_start = N_exc * 2 # dimension of Krylov subspace, analogue of nd1  \n",
    "    k_dim = n_V_start # initialize\n",
    "    N_cis = mol.nocc * mol.nvirt\n",
    "    N_rpa = 2 * mol.nocc * mol.nvirt\n",
    "    keep_n = 3\n",
    "    \n",
    "    term = False  # terminate algorithm\n",
    "    iter = 0\n",
    "    L_xi = torch.zeros((N_rpa, n_V_start), device=device)\n",
    "\n",
    "    V = gen_V(N_cis, N_rpa, n_V_start) # generate initial guess, V here #! should be renamed\n",
    "\n",
    "    #V = V[:k_dim] # take only part\n",
    "\n",
    "    while iter < max_iter and not term: # big loop\n",
    "        \n",
    "        print('=================================', flush=True)\n",
    "        print(colored(f' ITERATION : {iter} ', 'red', 'on_white', attrs=['bold']), flush=True)\n",
    "        print('SUBSPACE SIZE V: ', V.shape, flush=True)\n",
    "        print('=================================')\n",
    "       \n",
    "#        V = torch.linalg.qr(V)[0]    # most likely, do not need. create_subspace_matrix already orthogonalizes; \n",
    "                                      # later on done before adding to matrix\n",
    "        ### project original matrix in a subspace matrix\n",
    "        L_xi = torch.zeros((N_rpa, V.shape[1] ), device=device) #! NOT iter here\n",
    "        \n",
    "        for i in range(V.shape[1]):  # i=nd1_old+1,nd1   \n",
    "            logger.qm3('Lxi iterations=%s', i)\n",
    "            L_xi[:,i] = form_L_xi(V[:,i], mol, N_cis, N_rpa)\n",
    "            logger.qm3(fmt_log(L_xi[:,i], 'L_xi[:,i]', 'qm'))\n",
    "        iter += 1\n",
    "        # raise ValueError('stop')\n",
    "        \n",
    "        L_xi[N_cis:, :] = L_xi[:N_cis] #! TODO: make sure that this A+B, A-B, not just copy for RPA\n",
    "    \n",
    "        right_V = L_xi[N_cis:] # (A+B)b \n",
    "        left_V = L_xi[:N_cis]\n",
    "        \n",
    "        # print('right_V shape', right_V.shape)\n",
    "        # print('right_V\\n', right_V)\n",
    "        # M_plus =  V[:k_dim] @ right_V\n",
    "        # M_minus = V[:k_dim] @ left_V\n",
    "        V_ort = orthogonalize_torch(V)\n",
    "        \n",
    "        \n",
    "        M_plus =  V.T @ right_V\n",
    "        M_minus = V.T @ left_V\n",
    "        \n",
    "        logger.qm2(fmt_log(M_minus, 'M_minus', 'qm'))\n",
    "        logger.qm2(fmt_log(M_plus, 'M_plus', 'qm'))\n",
    "        \n",
    "        # raise ValueError('stop')\n",
    "        \n",
    "        M = M_minus.T @ M_plus\n",
    "        \n",
    "        logger.qm1(fmt_log(M.shape, 'M shape', 'qm'))\n",
    "        logger.qm1(fmt_log(M, 'M', 'qm'))\n",
    " \n",
    "        diag = torch.diag(M)\n",
    "        logger.qm1(fmt_log(diag, 'diag', 'qm'))\n",
    "        \n",
    "        r_eval, r_evec = torch.linalg.eig(M) # find eigenvalues and eigenvectors\n",
    "       \n",
    "        r_eval = r_eval.real\n",
    "        r_evec = r_evec.real\n",
    "        r_eval, r_idx = torch.sort(r_eval, descending=False) # sort eigenvalues in ascending order\n",
    "        # ! NOT SURE that padding is a good idea\n",
    "        logger.debug(fmt_log(r_eval, 'r_eval', 'evals'))\n",
    "        logger.debug(fmt_log(torch.sqrt(r_eval), 'SQRT r_eval', 'evals'))\n",
    "        # print('sqrt EVAl', torch.sqrt(r_eval))\n",
    "        \n",
    "        # print('r_eval shape', r_eval.shape)\n",
    "        # print('r_eval', r_eval)\n",
    "        # raise ValueError('stop')\n",
    "        # print('reconstructed M from RIGHT\\n', r_evec[:, r_idx] @ torch.diag(r_eval) @ r_evec[:, r_idx].inverse())\n",
    "        # l_eval, l_evec = torch.linalg.eig(M.T) # find left eigenvectors\n",
    "        # l_eval = l_eval.real\n",
    "        # l_evec = l_evec.real\n",
    "        # l_eval, l_idx = torch.sort(l_eval, descending=False) # sort eigenvalues in ascending order\n",
    "        # print('reconstructed M from LEFT\\n', (l_evec[:, l_idx] @ torch.diag(l_eval) @ l_evec[:, l_idx].inverse()).T)\n",
    "        \n",
    "        e_val_n = r_eval[:keep_n] # keep only the lowest keep_n eigenvalues; full are still stored as e_val\n",
    "        e_vec_n = r_evec[:, :keep_n]\n",
    "        resids = torch.zeros(V.shape[0], len(e_val_n)) # account for left and right evecs\n",
    "        \n",
    "        logger.debug(fmt_log(right_V.shape, 'right_V shape', 'matrix'))\n",
    "        logger.debug(fmt_log(right_V, 'right_V', 'matrix'))\n",
    "        \n",
    "        print('e_val_n', e_val_n)\n",
    "\n",
    "        print('V shape', V.shape)\n",
    "        for j in range(len(e_val_n)): # calc residuals \n",
    "            # print('e_vec_n[:,j]', e_vec_n[:,j])\n",
    "            # logger.debug(fmt_log(e_vec_n[:,j].shape, 'e_vec_n[:,j] shape', 'vector'))\n",
    "            # logger.debug(fmt_log(e_vec_n[:,j], 'e_vec_n[:,j]', 'vector'))\n",
    "            \n",
    "            resids[:,j] = right_V @ e_vec_n[:,j] - (e_val_n[j] * (V @ e_vec_n[:,j]))\n",
    "            \n",
    "            \n",
    "            # logger.debug(fmt_log(resids[:,j], 'resids[:,j]', 'vector'))\n",
    "\n",
    "        # Left = |X-Y>\n",
    "        # Right = |X+Y>\n",
    "        # (A+B)|X+Y>\n",
    "        # L_xi_right = torch.zeros((N_rpa, n_V_start), device=device)\n",
    "        # L_xi_left = torch.zeros((N_rpa, n_V_start), device=device)\n",
    "        # for i in range(k_dim):  # i=nd1_old+1,nd1  \n",
    "        #      # !should be +/- most likely as A+B, A-B\n",
    "        #     L_xi_right[:,i] = form_L_xi(r_evec[:,i], mol, N_cis, N_rpa)\n",
    "        #     L_xi_left[:,i] = form_L_xi(l_evec[:,i], mol, N_cis, N_rpa)\n",
    "        # L_xi_right[N_cis:, :] = L_xi_right[:N_cis]    #! not sure that needed\n",
    "        # L_xi_left[N_cis:, :] = L_xi_left[:N_cis]\n",
    "        \n",
    "        # print('L_xi_right\\n', L_xi_right[:k_dim, :keep_n])\n",
    "        # print('torch.diag(e_val_n)\\n', torch.diag(e_val_n).T)\n",
    "        # print('l_evec[:, :keep_n]', l_evec[:, :keep_n])\n",
    "        \n",
    "\n",
    "        # W_l = L_xi_right[:k_dim, :keep_n] - e_val_n * l_evec[:, :keep_n]\n",
    "        # W_r = L_xi_left[N_cis:][:k_dim, :keep_n] - e_val_n * r_evec[:, :keep_n] # check dims\n",
    "       \n",
    "        #resids = torch.cat((W_l, W_r), dim=1)\n",
    "    \n",
    "        resids_norms_r = torch.tensor([resids[:,x].norm() for x in range(resids.shape[1])])\n",
    "        # resids_norms_l = torch.tensor([W_l[:,x].norm() for x in range(W_l.shape[1])])\n",
    "        # ic(resids_norms, resids_norms.shape)\n",
    "        # if torch.any(resids_norms_r > tol) or torch.any(resids_norms_l > tol): # check if any residual norm is larger than tol\n",
    "#            print('at least one residual is too big', flush=True)\n",
    "        if torch.any(resids_norms_r > tol):\n",
    "            mask_r = resids_norms_r >= tol\n",
    "            large_res_r = resids[:,mask_r] # residuals larger than tol\n",
    "            logger.debug(fmt_log(large_res_r, 'LARGE RESIDUALS', 'vector'))           \n",
    "            large_res_r.to(device)\n",
    "            cor_e_val_r = e_val_n[mask_r] # corresponding eigenvalues !!! check if matches\n",
    "            \n",
    "            # mask_l = resids_norms_l >= tol\n",
    "            # large_res_l = W_l[:,mask_l]\n",
    "            # large_res_l.to(device)\n",
    "            # print('large residual norms ', resids_norms[mask])\n",
    "            # cor_e_val_l = e_val_n[mask_l]\n",
    "           # ic('V shape before adding residuals', V.shape)\n",
    "            \n",
    "           # if large_res_r.shape[1] <= n_V_max - V.shape[1] or large_res_l.shape[1] <= n_V_max - V.shape[1]: # startin adding residulas only if there is enough space\n",
    "            if large_res_r.shape[1] <= n_V_max - V.shape[1]:     \n",
    "                # # free_space = n_V_max - V.shape[1]\n",
    "                # i = 0\n",
    "                # j = 0\n",
    "                # while free_space <= n_V_max and i < keep_n and j < keep_n:\n",
    "                #     free_space +=1\n",
    "                    \n",
    "                    # print('i', i)\n",
    "                    for j in range(large_res_r.shape[1]):\n",
    "                        if V.shape[1] < n_V_max:\n",
    "                    # right eigenvecs - PRECONDITIONER\n",
    "                    # print('diag', diag)\n",
    "                    # print('cor_e_val_r', cor_e_val_r)\n",
    "                            denom = (diag[j] - cor_e_val_r[j])\n",
    "                            denom.to(device) \n",
    "                            s = large_res_r[:,j]/denom # conditioned residuals > tol\n",
    "                            s.to(device)\n",
    "                    # s = s/s.norm() # normalize\n",
    "                    # print('s shape', s.shape)\n",
    "                    # logger.debug(fmt_log(s, 's', 'vector'))\n",
    "                    \n",
    "                    # !!! PROBABLY HIGHLY INEFFICIENT !!! \n",
    "                    # torch QR does not help! Returns square matrix only, we have m, m+1\n",
    "                    # V_new = torch.cat((V, torch.unsqueeze(s, dim=1)), dim=1) # add new vector as column to V\n",
    "                    # V_ort = orthogonalize_torch(V_new) # orthogonalize V space with new vector, again should be translated to PyTorch\n",
    "                    # s_ort = V_ort[:, -1] # orthogonalized against current subspace\n",
    "\n",
    "                    # ratio = s_ort.norm()/resids[:,i].norm() # ratio of old and otogonalized norms\n",
    "                    \n",
    "                            if s.norm() >= tol:\n",
    "                                logger.debug(fmt_log(s.norm(), 'NORM OF NEW RESIDUAAL', 'vector'))\n",
    "                            # if s_ort.norm() >= tol:\n",
    "                                V = torch.column_stack((V, s))\n",
    "                            else:\n",
    "                                pass\n",
    "                            # i += 1\n",
    "                            # if free_space >= n_V_max: # do not add left if no space\n",
    "                            #     break\n",
    "                    # else:\n",
    "                    # # left eigenvecs\n",
    "                    #     denom = (diag[j] - cor_e_val_l[j])\n",
    "                    #     denom.to(device) \n",
    "                    #     s = large_res_l[:,j]/denom # conditioned residuals > tol\n",
    "                    #     s.to(device)\n",
    "                    #     s = s/s.norm() # normalize\n",
    "                    #   #  s = F.pad(s, (0, V.shape[1] - s.shape[0]), 'constant', 0) \n",
    "                    #     # !!! PROBABLY HIGHLY INEFFICIENT !!! \n",
    "                    #     # torch QR does not help! Returns square matrix only, we have m, m+1\n",
    "\n",
    "                    #     V_new = torch.cat((V, torch.unsqueeze(s, dim=1)), dim=1) # add new vector as column to V\n",
    "                    #     V_ort = orthogonalize_torch(V_new) # orthogonalize V space with new vector, again should be translated to PyTorch\n",
    "                    #     s_ort = V_ort[:, -1] # orthogonalized against current subspace\n",
    "\n",
    "                    #     ratio = s_ort.norm()/resids[:,j].norm() # ratio of old and otogonalized norms\n",
    "                    #     if ratio >= tol:\n",
    "                    #         V = torch.column_stack((V, s_ort))\n",
    "                    #     else:\n",
    "                    #         pass                \n",
    "                    #     j += 1\n",
    "            else:\n",
    "                logger.debug(fmt_log(None, '!!!! MAX subspace reached !!!!', 'warn'))\n",
    "                # print(colored('!!!! MAX subspace reached !!!!', 'red', attrs=['bold']))\n",
    "              #  ic('current shape of eigenvector matrix', r_evec.shape) \n",
    "                # ! TODO: use both LEFT and RIGHT after collapse\n",
    "                # r_evec_padded = F.pad(r_evec, (0, V.shape[1] - r_evec.shape[1]), 'constant', 0) # pad with zeros to match n_V_max\n",
    "                # V = V @ r_evec_padded # collapse subspace matrix to n_V_start vectors based on current eigenvectors\n",
    "                \n",
    "                # V = torch.zeros(N_cis, r_evec.shape[0])\n",
    "                # for j in range(len(r_evec.shape[1]))\n",
    "                #     V[:,j] = right_V @ e_vec_n[:,j] - (e_val_n[j] * (V @ e_vec_n[:,j]))\n",
    "                \n",
    "                \n",
    "                # V = torch.zeros(N_cis, r_evec.shape[0])\n",
    "                logger.debug(fmt_log(V, 'V before collapse', 'qm'))\n",
    "                # print('V shape', V.shape)\n",
    "                # print('r_evec shape', r_evec.shape)\n",
    "                V =  V @ r_evec # [:,:n_V_start]\n",
    "                V = V[:, :n_V_start]\n",
    "                logger.debug(fmt_log(V, 'V AFTER collapse', 'qm'))\n",
    "                \n",
    "                # print('V[:k_dim] shape', V[:k_dim].shape)\n",
    "                # V[:, :k_dim] = r_evec\n",
    "                # V[:r_evec.shape[0], :] = r_evec\n",
    "                # logger.debug(fmt_log(V, 'V AFTER COLLAPSE', 'qm'))\n",
    "              #  ic(V.shape, r_evec[:,:n_V_start].shape)\n",
    "                # ic('forming new subspace from current eigenvectors')\n",
    "                # ic('subspace after max reached', V.shape)\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            term = True\n",
    "            print('============================')\n",
    "            print('all residuals are below tolerance')\n",
    "            print('DAVIDSON ALGORITHM CONVERGED')\n",
    "            print('============================', flush=True)\n",
    "            # print('eigenvalues: ', e_val, flush=True)\n",
    "            # print('eigenvectors: ', e_vec, flush=True)\n",
    "            return r_eval, r_evec\n",
    "\n",
    "    # runs after big loop if did not converge\n",
    "    print('============================')\n",
    "    print('!!! DAVIDSON ALGORITHM DID NOT CONVERGE !!!')\n",
    "    print('============================', flush=True)\n",
    "    # print('eigenvalues: ', e_val, flush=True)\n",
    "    # print('eigenvectors: ', e_vec, flush=True)\n",
    "    return r_eval, r_evec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 0 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([16, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 59 : QM1 : \u001b[36mM shape\u001b[0m : \u001b[36mtorch.Size([4, 4])\u001b[0m\n",
      "davidson : 60 : QM1 : \u001b[36mM\u001b[0m\n",
      "\u001b[36mtensor([[ 86.97470,   9.87442,  -9.70269,   2.60694],\n",
      "        [  9.87442,  94.14780,   1.65452,   0.65665],\n",
      "        [ -9.70269,   1.65452, 105.37683,  -0.28590],\n",
      "        [  2.60694,   0.65665,  -0.28590, 105.93791]], grad_fn=<MmBackward0>)\u001b[0m\n",
      "davidson : 63 : QM1 : \u001b[36mdiag\u001b[0m\n",
      "\u001b[36mtensor([ 86.97470,  94.14780, 105.37683, 105.93791], grad_fn=<DiagonalBackward0_copy>)\u001b[0m\n",
      "davidson : 71 : DEBUG : \u001b[1m\u001b[32mr_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 77.07416,  99.01877, 105.79141, 110.55290], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 72 : DEBUG : \u001b[1m\u001b[32mSQRT r_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 8.77919,  9.95082, 10.28550, 10.51441], grad_fn=<SqrtBackward0>)\u001b[0m\n",
      "davidson : 89 : DEBUG : \u001b[1m\u001b[34mright_V shape\u001b[0m : \u001b[1m\u001b[34mtorch.Size([16, 4])\u001b[0m\n",
      "davidson : 90 : DEBUG : \u001b[1m\u001b[34mright_V\u001b[0m\n",
      "\u001b[1m\u001b[34mtensor([[     0.00000,      1.11825,     -0.16007,     -0.08597],\n",
      "        [     0.93682,     -0.54490,      0.65914,     -0.10355],\n",
      "        [    -0.13410,      0.65914,      0.26478,      0.24063],\n",
      "        [    -0.07202,     -0.10355,      0.24063,      0.28012],\n",
      "        [     0.00000,     -0.06421,      0.27637,      0.24757],\n",
      "        [    -0.06421,      0.06672,     -0.02749,     -0.21041],\n",
      "        [     0.27637,      0.09499,     -0.00573,      0.00810],\n",
      "        [     0.24757,      0.73549,     -0.11788,     -0.06099],\n",
      "        [    -0.00000,     -0.49456,     -0.26494,     -0.15709],\n",
      "        [    -0.49456,     -0.06394,      0.18532,     -0.02429],\n",
      "        [    -0.26494,     -0.76613,      0.05664,      0.05927],\n",
      "        [    -0.15709,      0.08704,     -0.02981,      0.00730],\n",
      "        [     9.29707,      0.52242,     -0.49880,      0.13202],\n",
      "        [     0.52242,      9.68838,      0.09605,      0.02947],\n",
      "        [    -0.49880,      0.09605,     10.25274,     -0.01085],\n",
      "        [     0.13202,      0.02947,     -0.01085,     10.29172]], grad_fn=<SliceBackward0>)\u001b[0m\n",
      "davidson : 139 : DEBUG : \u001b[1m\u001b[33mLARGE RESIDUALS\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor([[     0.59869,      0.90257,      0.32860],\n",
      "        [    -1.23740,      0.13465,     -0.25043],\n",
      "        [     0.36874,      0.54398,     -0.05724],\n",
      "        [    -0.05020,     -0.08978,     -0.15289],\n",
      "        [    -0.10191,     -0.01122,     -0.15176],\n",
      "        [     0.08116,      0.06614,     -0.06371],\n",
      "        [    -0.17461,      0.17875,      0.15179],\n",
      "        [     0.19674,      0.68217,      0.33673],\n",
      "        [    -0.17346,     -0.47277,      0.05836],\n",
      "        [     0.31088,     -0.16952,     -0.39340],\n",
      "        [    -0.17922,     -0.73474,     -0.30239],\n",
      "        [     0.18003,      0.00319,     -0.02691],\n",
      "        [    55.34337,    -33.06108,    -43.10251],\n",
      "        [   -33.87934,    -74.79931,    -19.52210],\n",
      "        [    20.91266,    -30.16483,     76.31597],\n",
      "        [    -4.02066,     18.30877,    -31.85342]], grad_fn=<IndexBackward0>)\u001b[0m\n",
      "davidson : 181 : DEBUG : \u001b[1m\u001b[33mNORM OF NEW RESIDUAAL\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor(6.89981, grad_fn=<LinalgVectorNormBackward0>)\u001b[0m\n",
      "davidson : 181 : DEBUG : \u001b[1m\u001b[33mNORM OF NEW RESIDUAAL\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor(18.28828, grad_fn=<LinalgVectorNormBackward0>)\u001b[0m\n",
      "davidson : 181 : DEBUG : \u001b[1m\u001b[33mNORM OF NEW RESIDUAAL\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor(229.82023, grad_fn=<LinalgVectorNormBackward0>)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "e_val_n tensor([ 77.07416,  99.01877, 105.79141], grad_fn=<SliceBackward0>)\n",
      "V shape torch.Size([16, 4])\n",
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 1 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([16, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 59 : QM1 : \u001b[36mM shape\u001b[0m : \u001b[36mtorch.Size([7, 7])\u001b[0m\n",
      "davidson : 60 : QM1 : \u001b[36mM\u001b[0m\n",
      "\u001b[36mtensor([[    88.35651,      9.70869,     -9.26573,      2.46107,    435.74337,    679.87731,  11537.20998],\n",
      "        [     9.70869,     97.54024,      1.22332,      0.68561,   -271.00776,   1554.77538,   5369.13344],\n",
      "        [    -9.26573,      1.22332,    106.12185,     -0.18907,    165.35021,    613.38671, -20423.18944],\n",
      "        [     2.46107,      0.68561,     -0.18907,    105.99022,    -31.75408,   -372.53565,   8472.25350],\n",
      "        [   435.74337,   -271.00776,    165.35021,    -31.75408,   3686.70381,    -33.45290,   -142.40951],\n",
      "        [   679.87731,   1554.77538,    613.38671,   -372.53565,    -33.45290,  33482.76762,   1666.88358],\n",
      "        [ 11537.20998,   5369.13344, -20423.18944,   8472.25350,   -142.40951,   1666.88358, 5855183.64602]], grad_fn=<MmBackward0>)\u001b[0m\n",
      "davidson : 63 : QM1 : \u001b[36mdiag\u001b[0m\n",
      "\u001b[36mtensor([   88.35651,    97.54024,   106.12185,   105.99022,  3686.70381, 33482.76762, 5855183.64602], grad_fn=<DiagonalBackward0_copy>)\u001b[0m\n",
      "davidson : 71 : DEBUG : \u001b[1m\u001b[32mr_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([    0.00632,     0.05992,     0.28352,   106.01534,  3765.77547, 33583.70603, 5855295.27967], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 72 : DEBUG : \u001b[1m\u001b[32mSQRT r_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([    0.07947,     0.24479,     0.53246,    10.29638,    61.36591,   183.25858,  2419.77174], grad_fn=<SqrtBackward0>)\u001b[0m\n",
      "davidson : 89 : DEBUG : \u001b[1m\u001b[34mright_V shape\u001b[0m : \u001b[1m\u001b[34mtorch.Size([16, 7])\u001b[0m\n",
      "davidson : 90 : DEBUG : \u001b[1m\u001b[34mright_V\u001b[0m\n",
      "\u001b[1m\u001b[34mtensor([[     0.00000,      1.11825,     -0.16007,     -0.08597,     -2.63829,     12.15061,     57.34179],\n",
      "        [     0.93682,     -0.54490,      0.65914,     -0.10355,      5.33096,      1.87319,    -40.87213],\n",
      "        [    -0.13410,      0.65914,      0.26478,      0.24063,     -1.56951,      7.08766,     -9.95355],\n",
      "        [    -0.07202,     -0.10355,      0.24063,      0.28012,      0.20180,     -1.18135,    -26.08259],\n",
      "        [     0.00000,     -0.06421,      0.27637,      0.24757,      0.60206,     -0.18522,    -31.92336],\n",
      "        [    -0.06421,      0.06672,     -0.02749,     -0.21041,     -0.49905,      1.15597,    -13.07238],\n",
      "        [     0.27637,      0.09499,     -0.00573,      0.00810,      1.02757,      2.99885,     31.59492],\n",
      "        [     0.24757,      0.73549,     -0.11788,     -0.06099,     -1.18470,     11.31092,     71.25566],\n",
      "        [    -0.00000,     -0.49456,     -0.26494,     -0.15709,      1.02478,     -7.80704,     12.27653],\n",
      "        [    -0.49456,     -0.06394,      0.18532,     -0.02429,     -1.87625,     -2.88617,    -80.30162],\n",
      "        [    -0.26494,     -0.76613,      0.05664,      0.05927,      1.09853,    -12.31445,    -63.58760],\n",
      "        [    -0.15709,      0.08704,     -0.02981,      0.00730,     -1.06086,      0.03269,     -5.54876],\n",
      "        [     9.29707,      0.52242,     -0.49880,      0.13202,     48.93929,     67.42668,   1092.66937],\n",
      "        [     0.52242,      9.68838,      0.09605,      0.02947,    -29.84505,    152.26526,    492.75254],\n",
      "        [    -0.49880,      0.09605,     10.25274,     -0.01085,     18.46422,     61.61657,  -1934.37071],\n",
      "        [     0.13202,      0.02947,     -0.01085,     10.29172,     -3.55457,    -37.40109,    808.13230]], grad_fn=<SliceBackward0>)\u001b[0m\n",
      "davidson : 139 : DEBUG : \u001b[1m\u001b[33mLARGE RESIDUALS\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor([[    57.34645,    -12.19661,     -2.79212],\n",
      "        [   -40.87423,     -1.90731,      5.68391],\n",
      "        [    -9.95422,     -7.12841,     -1.66646],\n",
      "        [   -26.08466,      1.18016,      0.21416],\n",
      "        [   -31.92526,      0.17599,      0.62827],\n",
      "        [   -13.07314,     -1.16368,     -0.51951],\n",
      "        [    31.59738,     -3.00383,      1.07863],\n",
      "        [    71.26109,    -11.33732,     -1.22193],\n",
      "        [    12.27659,      7.84280,      1.06229],\n",
      "        [   -80.30734,      2.87750,     -1.96440],\n",
      "        [   -63.59273,     12.34674,      1.12990],\n",
      "        [    -5.54927,     -0.03308,     -1.10913],\n",
      "        [  1092.69778,    -67.15511,     49.52680],\n",
      "        [   492.80178,   -152.38518,    -29.95054],\n",
      "        [ -1934.37172,    -62.24158,     18.66882],\n",
      "        [   808.12918,     37.68353,     -3.61302]], grad_fn=<IndexBackward0>)\u001b[0m\n",
      "davidson : 211 : DEBUG : \u001b[1m\u001b[31m!!!! MAX subspace reached !!!!\u001b[0m : \n",
      "davidson : 224 : DEBUG : \u001b[36mV before collapse\u001b[0m\n",
      "\u001b[36mtensor([[ 0.00000,  0.00000,  0.00000,  0.00000,  0.39429, -0.46860, -0.03138],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000, -0.81494, -0.40602,  0.28306],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000,  0.24285, -0.28043,  0.41133],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000, -0.03306,  0.04898,  0.20661],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000, -0.06712, -0.01811,  0.23319],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000,  0.05345, -0.02489,  0.15126],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000, -0.11500, -0.16708, -0.19951],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000,  0.12957, -0.41897, -0.21373],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000, -0.11424,  0.28096, -0.34808],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000,  0.20474,  0.19526,  0.63193],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000, -0.11803,  0.45954,  0.13420],\n",
      "        [ 0.00000,  0.00000,  0.00000,  0.00000,  0.11857,  0.04342,  0.07594],\n",
      "        [ 1.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000],\n",
      "        [ 0.00000,  1.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000],\n",
      "        [ 0.00000,  0.00000,  1.00000,  0.00000,  0.00000,  0.00000,  0.00000],\n",
      "        [ 0.00000,  0.00000,  0.00000,  1.00000,  0.00000,  0.00000,  0.00000]], grad_fn=<CopySlices>)\u001b[0m\n",
      "davidson : 229 : DEBUG : \u001b[36mV AFTER collapse\u001b[0m\n",
      "\u001b[36mtensor([[    -0.03152,      0.46833,      0.38957,      0.00005],\n",
      "        [     0.28296,      0.40457,     -0.80681,      0.00005],\n",
      "        [     0.41124,      0.28039,      0.23996,      0.00003],\n",
      "        [     0.20662,     -0.04889,     -0.03265,     -0.00000],\n",
      "        [     0.23318,      0.01807,     -0.06642,      0.00000],\n",
      "        [     0.15125,      0.02496,      0.05286,      0.00000],\n",
      "        [    -0.19956,      0.16664,     -0.11398,      0.00002],\n",
      "        [    -0.21385,      0.41842,      0.12770,      0.00004],\n",
      "        [    -0.34800,     -0.28076,     -0.11271,     -0.00003],\n",
      "        [     0.63197,     -0.19455,      0.20283,     -0.00002],\n",
      "        [     0.13434,     -0.45894,     -0.11624,     -0.00005],\n",
      "        [     0.07595,     -0.04320,      0.11737,     -0.00000],\n",
      "        [     0.00197,     -0.02016,      0.11723,     -0.02938],\n",
      "        [     0.00092,     -0.04633,     -0.07224,      0.08196],\n",
      "        [    -0.00349,     -0.01846,      0.04444,      0.38800],\n",
      "        [     0.00145,      0.01118,     -0.00858,      0.91754]], grad_fn=<SliceBackward0>)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "e_val_n tensor([0.00632, 0.05992, 0.28352], grad_fn=<SliceBackward0>)\n",
      "V shape torch.Size([16, 7])\n",
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 2 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([16, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 59 : QM1 : \u001b[36mM shape\u001b[0m : \u001b[36mtorch.Size([4, 4])\u001b[0m\n",
      "davidson : 60 : QM1 : \u001b[36mM\u001b[0m\n",
      "\u001b[36mtensor([[229.75246, 112.99511, -95.64603,  11.75250],\n",
      "        [112.99511, 247.33856, -65.67276,   5.34888],\n",
      "        [-95.64603, -65.67276, 526.59965,  -1.53949],\n",
      "        [ 11.75250,   5.34888,  -1.53949, 106.01747]], grad_fn=<MmBackward0>)\u001b[0m\n",
      "davidson : 63 : QM1 : \u001b[36mdiag\u001b[0m\n",
      "\u001b[36mtensor([229.75246, 247.33856, 526.59965, 106.01747], grad_fn=<DiagonalBackward0_copy>)\u001b[0m\n",
      "davidson : 71 : DEBUG : \u001b[1m\u001b[32mr_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([103.98119, 124.76719, 297.50854, 583.45122], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 72 : DEBUG : \u001b[1m\u001b[32mSQRT r_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([10.19712, 11.16992, 17.24844, 24.15473], grad_fn=<SqrtBackward0>)\u001b[0m\n",
      "davidson : 89 : DEBUG : \u001b[1m\u001b[34mright_V shape\u001b[0m : \u001b[1m\u001b[34mtorch.Size([16, 4])\u001b[0m\n",
      "davidson : 90 : DEBUG : \u001b[1m\u001b[34mright_V\u001b[0m\n",
      "\u001b[1m\u001b[34mtensor([[    -0.31399,     10.77620,      9.52262,     -0.04817],\n",
      "        [     7.80985,      9.96639,    -20.50794,      0.08975],\n",
      "        [    10.32942,      7.09756,      6.22252,      0.38234],\n",
      "        [     5.02168,     -1.17166,     -0.92332,      0.34393],\n",
      "        [     1.84902,      0.18672,     -0.63640,      0.32916],\n",
      "        [     1.31201,      0.02348,      0.38068,     -0.19635],\n",
      "        [    -1.95849,      1.33280,     -1.11771,      0.00500],\n",
      "        [    -1.33395,      3.41821,      1.07975,     -0.04832],\n",
      "        [    -2.98134,     -2.34619,     -1.08008,     -0.28775],\n",
      "        [     6.98135,     -1.37231,      1.68718,      0.05881],\n",
      "        [     1.33079,     -3.36092,     -0.85823,      0.02099],\n",
      "        [     0.79228,     -0.36638,      1.14222,      0.00684],\n",
      "        [    -0.26209,      0.51661,      0.15200,     -0.30264],\n",
      "        [    -0.07588,      0.85552,      0.63517,      0.84317],\n",
      "        [     0.61479,      0.03805,     -0.12506,      3.99063],\n",
      "        [     0.23062,      0.07923,     -0.00633,      9.43738]], grad_fn=<SliceBackward0>)\u001b[0m\n",
      "davidson : 139 : DEBUG : \u001b[1m\u001b[33mLARGE RESIDUALS\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor([[ -17.85823,   48.41068,   81.37999],\n",
      "        [  73.34273,    8.90113,   30.47011],\n",
      "        [   0.41816,   52.90592,  -36.21847],\n",
      "        [   6.60116,    6.82532,  -48.48577],\n",
      "        [  13.61187,   13.41447,  -43.90397],\n",
      "        [   0.98492,   14.75750,  -27.81320],\n",
      "        [   7.94008,   -4.97166,   74.36510],\n",
      "        [  -6.39405,   26.42333,  119.62830],\n",
      "        [  -9.31413,  -51.28847,   22.66153],\n",
      "        [  -3.15795,   34.64773, -170.36955],\n",
      "        [   1.52403,  -35.05373, -110.56225],\n",
      "        [  -8.72984,    7.24756,  -25.96165],\n",
      "        [ -11.43339,    4.30124,   -4.90868],\n",
      "        [   5.83991,   -8.27595,  -14.29736],\n",
      "        [  -4.62567,    2.48693,  -32.60410],\n",
      "        [   2.31517,    5.74278,  -67.31535]], grad_fn=<IndexBackward0>)\u001b[0m\n",
      "davidson : 181 : DEBUG : \u001b[1m\u001b[33mNORM OF NEW RESIDUAAL\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor(0.63602, grad_fn=<LinalgVectorNormBackward0>)\u001b[0m\n",
      "davidson : 181 : DEBUG : \u001b[1m\u001b[33mNORM OF NEW RESIDUAAL\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor(0.87974, grad_fn=<LinalgVectorNormBackward0>)\u001b[0m\n",
      "davidson : 181 : DEBUG : \u001b[1m\u001b[33mNORM OF NEW RESIDUAAL\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor(1.25009, grad_fn=<LinalgVectorNormBackward0>)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "e_val_n tensor([103.98119, 124.76719, 297.50854], grad_fn=<SliceBackward0>)\n",
      "V shape torch.Size([16, 4])\n",
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 3 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([16, 7])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 59 : QM1 : \u001b[36mM shape\u001b[0m : \u001b[36mtorch.Size([7, 7])\u001b[0m\n",
      "davidson : 60 : QM1 : \u001b[36mM\u001b[0m\n",
      "\u001b[36mtensor([[   264.28492,    127.58622,    -88.84989,     12.78252,    127.63025,    165.62762,   -130.13233],\n",
      "        [   127.58622,    300.52103,    -51.13270,      4.54876,    109.10984,    213.92360,    124.61826],\n",
      "        [   -88.84989,    -51.13270,    560.28981,     -0.24471,   -340.66667,    130.85685,     -8.59689],\n",
      "        [    12.78252,      4.54876,     -0.24471,    106.14706,      4.78911,     13.39429,    -42.57144],\n",
      "        [   127.63025,    109.10984,   -340.66667,      4.78911,    236.64266,     -0.09401,     -0.33298],\n",
      "        [   165.62762,    213.92360,    130.85685,     13.39429,     -0.09401,    248.75323,      5.39409],\n",
      "        [  -130.13233,    124.61826,     -8.59689,    -42.57144,     -0.33298,      5.39409,    226.65739]], grad_fn=<MmBackward0>)\u001b[0m\n",
      "davidson : 63 : QM1 : \u001b[36mdiag\u001b[0m\n",
      "\u001b[36mtensor([264.28492, 300.52103, 560.28981, 106.14706, 236.64266, 248.75323, 226.65739], grad_fn=<DiagonalBackward0_copy>)\u001b[0m\n",
      "davidson : 71 : DEBUG : \u001b[1m\u001b[32mr_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([    0.06882,     0.10568,     0.40976,   105.02345,   377.29639,   613.81487,   846.57712], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 72 : DEBUG : \u001b[1m\u001b[32mSQRT r_eval\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 0.26233,  0.32509,  0.64013, 10.24809, 19.42412, 24.77529, 29.09600], grad_fn=<SqrtBackward0>)\u001b[0m\n",
      "davidson : 89 : DEBUG : \u001b[1m\u001b[34mright_V shape\u001b[0m : \u001b[1m\u001b[34mtorch.Size([16, 7])\u001b[0m\n",
      "davidson : 90 : DEBUG : \u001b[1m\u001b[34mright_V\u001b[0m\n",
      "\u001b[1m\u001b[34mtensor([[    -0.31399,     10.77620,      9.52262,     -0.04817,     -3.46979,      9.37937,      7.72244],\n",
      "        [     7.80985,      9.96639,    -20.50794,      0.08975,     14.84775,      1.96040,      2.53573],\n",
      "        [    10.32942,      7.09756,      6.22252,      0.38234,     -0.03258,     10.91431,     -4.06191],\n",
      "        [     5.02168,     -1.17166,     -0.92332,      0.34393,      1.35110,      1.31367,     -5.21316],\n",
      "        [     1.84902,      0.18672,     -0.63640,      0.32916,      0.96349,      0.88318,     -1.59552],\n",
      "        [     1.31201,      0.02348,      0.38068,     -0.19635,      0.07335,      0.90516,     -1.14046],\n",
      "        [    -1.95849,      1.33280,     -1.11771,      0.00500,      0.57572,     -0.58192,      2.95029],\n",
      "        [    -1.33395,      3.41821,      1.07975,     -0.04832,     -0.34464,      1.99296,      3.93279],\n",
      "        [    -2.98134,     -2.34619,     -1.08008,     -0.28775,     -0.56022,     -3.66337,      1.00510],\n",
      "        [     6.98135,     -1.37231,      1.68718,      0.05881,      0.19520,      3.37527,     -7.56288],\n",
      "        [     1.33079,     -3.36092,     -0.85823,      0.02099,      0.16115,     -1.99566,     -3.87038],\n",
      "        [     0.79228,     -0.36638,      1.14222,      0.00684,     -0.66357,      0.63270,     -1.11487],\n",
      "        [    -0.26209,      0.51661,      0.15200,     -0.30264,     -0.23359,      0.25429,      0.70167],\n",
      "        [    -0.07588,      0.85552,      0.63517,      0.84317,     -0.09532,      0.61716,      0.38192],\n",
      "        [     0.61479,      0.03805,     -0.12506,      3.99063,      0.14619,      0.44290,     -1.81429],\n",
      "        [     0.23062,      0.07923,     -0.00633,      9.43738,      0.18775,      0.59324,     -3.24687]], grad_fn=<SliceBackward0>)\u001b[0m\n",
      "davidson : 139 : DEBUG : \u001b[1m\u001b[33mLARGE RESIDUALS\u001b[0m\n",
      "\u001b[1m\u001b[33mtensor([[-6.55286, 15.38599,  9.34815],\n",
      "        [28.06972,  3.31905,  3.20432],\n",
      "        [-0.06822, 17.03718, -6.42654],\n",
      "        [ 2.57456,  1.69675, -7.04786],\n",
      "        [ 1.54939,  1.13166, -2.01970],\n",
      "        [ 0.12753,  1.17538, -1.48918],\n",
      "        [ 0.90157, -0.65999,  3.72042],\n",
      "        [-0.54849,  3.13700,  4.66054],\n",
      "        [-0.88358, -5.12126,  1.51298],\n",
      "        [ 0.50060,  4.42092, -9.77338],\n",
      "        [ 0.30537, -3.01639, -4.63819],\n",
      "        [-1.06640,  0.84334, -1.44385],\n",
      "        [-0.16784,  0.41242,  0.94608],\n",
      "        [-0.35405,  1.11569,  0.51959],\n",
      "        [ 0.41286,  0.54644, -2.23651],\n",
      "        [ 0.28635,  0.63727, -3.87741]], grad_fn=<IndexBackward0>)\u001b[0m\n",
      "davidson : 211 : DEBUG : \u001b[1m\u001b[31m!!!! MAX subspace reached !!!!\u001b[0m : \n",
      "davidson : 224 : DEBUG : \u001b[36mV before collapse\u001b[0m\n",
      "\u001b[36mtensor([[    -0.03152,      0.46833,      0.38957,      0.00005,      0.02062,     -0.39521,     -0.55610],\n",
      "        [     0.28296,      0.40457,     -0.80681,      0.00005,     -0.25184,     -0.10592,     -0.16190],\n",
      "        [     0.41124,      0.28039,      0.23996,      0.00003,      0.02921,     -0.52842,      0.47839],\n",
      "        [     0.20662,     -0.04889,     -0.03265,     -0.00000,     -0.02691,     -0.05481,      0.48469],\n",
      "        [     0.23318,      0.01807,     -0.06642,      0.00000,      0.42733,      0.12990,     -0.10188],\n",
      "        [     0.15125,      0.02496,      0.05286,      0.00000,      0.02917,      0.15312,     -0.06144],\n",
      "        [    -0.19956,      0.16664,     -0.11398,      0.00002,      0.24899,     -0.01549,      0.11525],\n",
      "        [    -0.21385,      0.41842,      0.12770,      0.00004,     -0.22387,      0.22367,      0.25667],\n",
      "        [    -0.34800,     -0.28076,     -0.11271,     -0.00003,     -0.31388,     -0.46425,      0.09376],\n",
      "        [     0.63197,     -0.19455,      0.20283,     -0.00002,     -0.20652,      0.19547,     -0.15603],\n",
      "        [     0.13434,     -0.45894,     -0.11624,     -0.00005,      0.03179,     -0.38292,     -0.17051],\n",
      "        [     0.07595,     -0.04320,      0.11737,     -0.00000,     -0.26460,      0.05042,     -0.03885],\n",
      "        [     0.00197,     -0.02016,      0.11723,     -0.02938,     -0.50327,      0.04540,     -0.16639],\n",
      "        [     0.00092,     -0.04633,     -0.07224,      0.08196,      0.31323,     -0.23218,     -0.12202],\n",
      "        [    -0.00349,     -0.01846,      0.04444,      0.38800,     -0.26596,     -0.01226,      0.03288],\n",
      "        [     0.00145,      0.01118,     -0.00858,      0.91754,      0.06837,      0.02738,     -0.00833]], grad_fn=<CopySlices>)\u001b[0m\n",
      "davidson : 229 : DEBUG : \u001b[36mV AFTER collapse\u001b[0m\n",
      "\u001b[36mtensor([[    -0.17978,      0.10557,     -0.24639,     -0.06745],\n",
      "        [     0.66068,      0.00007,     -0.05405,     -0.01362],\n",
      "        [     0.01794,      0.08972,      0.28248,      0.07559],\n",
      "        [     0.05708,      0.03158,      0.26035,      0.06306],\n",
      "        [     0.34763,      0.16123,     -0.18498,      0.04276],\n",
      "        [     0.02454,      0.18793,     -0.12037,      0.02934],\n",
      "        [     0.20173,     -0.03032,      0.25683,     -0.06534],\n",
      "        [    -0.17531,      0.34405,      0.44348,     -0.08896],\n",
      "        [    -0.24795,     -0.62832,      0.15896,     -0.03959],\n",
      "        [    -0.13008,      0.33291,     -0.52052,      0.16425],\n",
      "        [     0.03236,     -0.49296,     -0.34859,      0.08017],\n",
      "        [    -0.21798,      0.07606,     -0.09278,      0.02486],\n",
      "        [    -0.35969,      0.04939,     -0.14284,      0.03165],\n",
      "        [     0.21063,     -0.20018,     -0.10990,     -0.07855],\n",
      "        [    -0.17600,      0.00757,     -0.03671,     -0.37079],\n",
      "        [     0.05516,      0.04706,     -0.13145,     -0.88907]], grad_fn=<SliceBackward0>)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_val_n tensor([0.06882, 0.10568, 0.40976], grad_fn=<SliceBackward0>)\n",
      "V shape torch.Size([16, 7])\n",
      "============================\n",
      "!!! DAVIDSON ALGORITHM DID NOT CONVERGE !!!\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<module> : 4 : DEBUG : \u001b[1m\u001b[32m FINAL eval sqrt\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 0.26233,  0.32509,  0.64013, 10.24809, 19.42412, 24.77529, 29.09600], grad_fn=<SqrtBackward0>)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mol = run_seqm_1mol('ch4.xyz')\n",
    "eval, _ = davidson(n_V_max = 8, max_iter = 4, tol = 1e-3)\n",
    "\n",
    "logger.debug(fmt_log(torch.sqrt(eval), ' FINAL eval sqrt', 'evals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.97462,  8.58262,  9.07584, 10.22214, 10.42812, 10.79614, 10.99266, 25.56659, 25.88856, 25.92413], grad_fn=<SqrtBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
