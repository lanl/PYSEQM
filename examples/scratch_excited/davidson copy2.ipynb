{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decorating your function! <function KSA_XL_BOMD.one_step at 0x7fb83102ae60>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# === IMPORTS ===\n",
    "\n",
    "import logging, sys\n",
    "import torch\n",
    "import seqm\n",
    "from ase.io import read as ase_read\n",
    "from seqm.seqm_functions.constants import Constants\n",
    "from seqm.Molecule import Molecule\n",
    "from seqm.ElectronicStructure import Electronic_Structure\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "from seqm.seqm_functions.fock import fock\n",
    "from seqm.seqm_functions.pack import unpack\n",
    "import seqm.seqm_functions.pack as pack\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#=== TORCH OPTIONS ===\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "# else:\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "torch.set_printoptions(precision=5, linewidth=200, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2.xyz\t\t  examples\t nodes.html\t requirements.txt\n",
      "c60.xyz\t\t  h2o.xyz\t __pycache__\t seqm\n",
      "c6h6.xyz\t  isoindigo.xyz  pyseqm.dot\t setup.dot\n",
      "ch4.xyz\t\t  lib\t\t pyseqm.dot.png  setup.py\n",
      "coronene.xyz\t  LICENSE\t pyseqm.dot.svg  Test1_SinglePointProp_FNS.png\n",
      "davidson_0723.py  model.pt\t pyseqm.json\t test_module.py\n",
      "doc\t\t  My_d_combined  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colored logging with custom level QM for deeper routines\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(funcName)s : %(lineno)d : %(levelname)s : %(message)s')\n",
    "\n",
    "QM1 = evel=logging.DEBUG - 3 # informal level of depth; QM1 - almost always, usually outside of loops\n",
    "QM2 = evel=logging.DEBUG - 4 #                          QM2 - sometimes, in the loops\n",
    "QM3 = evel=logging.DEBUG - 5\n",
    "\n",
    "logging.addLevelName(QM1, \"QM1\")\n",
    "def qm1(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM1 ):\n",
    "        self._log(QM1, message, args, **kwargs) \n",
    "        \n",
    "logging.addLevelName(QM2, \"QM2\")\n",
    "def qm2(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM2):\n",
    "        self._log(QM2, message, args, **kwargs) \n",
    " \n",
    "logging.addLevelName(QM3, \"QM3\")\n",
    "def qm3(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM3 ):\n",
    "        self._log(QM3, message, args, **kwargs) \n",
    "           \n",
    "        \n",
    "logging.Logger.qm1 = qm1   \n",
    "logging.Logger.qm2 = qm2\n",
    "logging.Logger.qm3 = qm3\n",
    "  \n",
    "logger = logging.getLogger()\n",
    "\n",
    "                              \n",
    "colors = {'qm'        : ('cyan',     None, None),\n",
    "          'matrix'    : ('blue',     None, ['bold']),\n",
    "          'vector'    : ('yellow',   None, ['bold']),\n",
    "          'evals'     : ('green',    None, ['bold']),\n",
    "          'warn'     : ('red',    None, ['bold'])\n",
    "          }\n",
    "\n",
    "def fmt_log(data, message, fmt):\n",
    "    \"\"\"\n",
    "    fmt_log : formats log message with color and style using termcolor module\n",
    "\n",
    "    Args:\n",
    "        data (any): data to print\n",
    "        message (str or None): message to print, pass None if no message is needed\n",
    "        fmt (str): style from colors dict\n",
    "\n",
    "    Returns:\n",
    "        str: formatted string with color and style\n",
    "    \"\"\"    \n",
    "\n",
    "    if type(data) is list or type(data) is tuple or type(data) is torch.Tensor:\n",
    "        \n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])}\\n' # add new line to align array\n",
    "    else:\n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])} : '\n",
    "        \n",
    "    if data == None:\n",
    "        return mes\n",
    "    else:\n",
    "        return mes + str(colored(data, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log\n",
    "\n",
    "07/13/23 - QM part seems to be wortking fine\n",
    "full diagonalization agrees with NEXMD\n",
    "small guess space misses relevant vectors, but large guess includes them\n",
    "\n",
    "\n",
    "PASCAL 1 COULD BE INCORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seqm_1mol(xyz):\n",
    "    \"\"\"\n",
    "    run_seqm_1mol : run PYSEQM for a single molecule\n",
    "\n",
    "    Args:\n",
    "        xyz (str): path to xyz file\n",
    "\n",
    "    Returns:\n",
    "        Molecule object: PYSEQM object with molecule data\n",
    "    \"\"\"    \n",
    "    \n",
    "    atoms = ase_read(xyz)\n",
    "    species = torch.tensor([atoms.get_atomic_numbers()], dtype=torch.long, device=device)\n",
    "    coordinates = torch.tensor([atoms.get_positions()], dtype=dtype, device=device)\n",
    "    \n",
    "    const = Constants().to(device)\n",
    "\n",
    "    elements = [0]+sorted(set(species.reshape(-1).tolist()))\n",
    "\n",
    "    seqm_parameters = {\n",
    "                    'method' : 'PM3',  # AM1, MNDO, PM#\n",
    "                    'scf_eps' : 1.0e-6,  # unit eV, change of electric energy, as nuclear energy doesnt' change during SCF\n",
    "                    'scf_converger' : [2,0.0], # converger used for scf loop\n",
    "                                            # [0, 0.1], [0, alpha] constant mixing, P = alpha*P + (1.0-alpha)*Pnew\n",
    "                                            # [1], adaptive mixing\n",
    "                                            # [2], adaptive mixing, then pulay\n",
    "                    'sp2' : [False, 1.0e-5],  # whether to use sp2 algorithm in scf loop,\n",
    "                                                #[True, eps] or [False], eps for SP2 conve criteria\n",
    "                    'elements' : elements, #[0,1,6,8],\n",
    "                    'learned' : [], # learned parameters name list, e.g ['U_ss']\n",
    "                    #'parameter_file_dir' : '../seqm/params/', # file directory for other required parameters\n",
    "                    'pair_outer_cutoff' : 1.0e10, # consistent with the unit on coordinates\n",
    "                    'eig' : True,\n",
    "                    'excited' : True,\n",
    "                    }\n",
    "\n",
    "    mol = seqm.Molecule.Molecule(const, seqm_parameters, coordinates, species).to(device)\n",
    "\n",
    "    ### Create electronic structure driver:\n",
    "    esdriver = Electronic_Structure(seqm_parameters).to(device)\n",
    "\n",
    "    ### Run esdriver on m:\n",
    "    esdriver(mol)\n",
    "    \n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_L_xi_no_split(vexp1, molecule, N_cis, N_rpa, CIS = True):\n",
    "\n",
    "    # WRONG\n",
    "    \n",
    "    m = molecule\n",
    "    gss = m.parameters['g_ss']\n",
    "    gsp = m.parameters['g_sp']\n",
    "    gpp = m.parameters['g_pp']\n",
    "    gp2 = m.parameters['g_p2']\n",
    "    hsp = m.parameters['h_sp']\n",
    "    \n",
    "    mask  = m.mask\n",
    "    maskd = m.maskd\n",
    "    idxi  = m.idxi\n",
    "    idxj  = m.idxj\n",
    "    nmol  = m.nmol\n",
    "    molsize = m.molsize\n",
    "    w       = m.w\n",
    "    nHeavy = m.nHeavy\n",
    "    nHydro = m.nHydro\n",
    "    \n",
    "    eta = torch.zeros((N_rpa), device=device) \n",
    "    \n",
    "    #print('vexp1.shape', vexp1.shape)\n",
    "   # print('eta.shape', eta.shape)\n",
    "    eta[:vexp1.size(0)] = vexp1 # eta is stored as |X|; dcopy?\n",
    "    \n",
    "    eta_orig = torch.clone(eta)\n",
    "    # print('eta_orig.shape', eta_orig.shape)\n",
    "    # print('eta_orig\\n', eta_orig)\n",
    "    \n",
    "   # print('m.C MO', m.C_mo[0])\n",
    "    # print('eta.shape', eta.shape)\n",
    "    # print(eta)\n",
    "    eta_ao =  mo2ao(N_cis, eta, m, full=False)     # mo to ao basis (mo2site)\n",
    "    # print('eta_ao.shape', eta_ao.shape)\n",
    "    # print(eta_ao)\n",
    "\n",
    "    # eta_ao_sym, eta_ao_asym = decompose_to_sym_antisym(eta_ao) # decompose to sym and asym\n",
    "\n",
    "    # Vxi - build 2e integrals in AO basis: G(guess density) in F = H_core + G\n",
    "    # note density is split into sym and anisym matrices\n",
    "    # sym is processed as padded 3d array in PYSEQM, see fock module \n",
    "    # antisym: 2c-2e works with modified PYSEQM routine; should be antisimmterized afterwards\n",
    "    # antisym: 1c-2e (diagonal) are taken from NEXMD for now - ugly code with loops\n",
    "    # TODO: vectorize 1c-2e part\n",
    "    \n",
    "    #------------------symmetric------------------------------\n",
    "    G_sym   =  build_G_sym(eta_ao,\n",
    "                        gss, gsp, gpp, gp2, hsp,\n",
    "                        mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                        w,\n",
    "                         nHeavy,\n",
    "                         nHydro)\n",
    "    # G sym is 1c-2e and 2c-2e of symmetric part of guess density\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pack 2c-2e part to standard shape\n",
    "    G_sym = pack.pack(G_sym, nHeavy, nHydro)\n",
    "    # print('G_sym \\n', G_sym)\n",
    "    \n",
    "    # G_tot = build_G_antisym(eta_ao, eta_ao_asym, G_sym,\n",
    "    #                         gss, gsp, gpp, gp2, hsp,\n",
    "    #                         mask, maskd, idxi, idxj, nmol, molsize,\n",
    "    #                         w, \n",
    "    #                         m,\n",
    "    #                         nHeavy,\n",
    "    #                         nHydro)\n",
    "                        \n",
    "    # build_G_antisym returns both sym and antisym!\n",
    "    # TODO: refactor into: 2c-2e antisym, 1c-2e antisym\n",
    "    # TODO: vectorize 1c-2e antisym, avoid ugly loops\n",
    "    #! remember about making 2c-2e diagonal 0\n",
    "\n",
    "    # print('G total \\n', G_tot)\n",
    "    4\n",
    "    # print('============================================')\n",
    "    # print('Converting Gao full back into MO basis')\n",
    "    G_mo = ao2mo(N_cis, N_rpa, m, G_sym[0], m.C_mo, full=False) # G in MO basis #! [0] not batched yet\n",
    "    \n",
    "  #  multiply by MO differencies\n",
    "    ii=0\n",
    "    for p in range(m.nocc):\n",
    "    # print('p', p)\n",
    "        for h in range(m.nocc, m.norb):\n",
    "            # print('h', h)\n",
    "            # print('i', i)\n",
    "            f = m.e_mo[0][h] - m.e_mo[0][p]\n",
    "            G_mo[ii] = G_mo[ii] + f * eta_orig[ii]\n",
    "            G_mo[ii+N_cis] = -G_mo[ii + N_cis] + f * eta_orig[ii+N_cis]\n",
    "            ii += 1\n",
    "        \n",
    "    # print('G_mo.shape', G_mo.shape)\n",
    "    # print('G_mo\\n', G_mo)\n",
    "    return G_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_L_xi(vexp1, molecule, N_cis, N_rpa, CIS = True):\n",
    "    \"\"\"\n",
    "    form_L_xi: build A matrix for CIS\n",
    "               splits guess density into symmetric and antisymmetric parts\n",
    "               unclear why returns A @ b (guess vector)\n",
    "               see NEXMD code for QM details\n",
    "               #! RPA is not implemented yet\n",
    "               \n",
    "    Args:\n",
    "        vexp1 (tensor): guess vector\n",
    "        molecule (PYSEQM object): _description_\n",
    "        N_cis (int): dimension of CIS space, nocch*nvirt\n",
    "        N_rpa (int): N_cis *2\n",
    "        CIS (bool, optional): CIS or TDHF (RPA) Defaults to True.\n",
    "\n",
    "    \"\"\"        \n",
    "\n",
    "    m = molecule\n",
    "    gss = m.parameters['g_ss']\n",
    "    gsp = m.parameters['g_sp']\n",
    "    gpp = m.parameters['g_pp']\n",
    "    gp2 = m.parameters['g_p2']\n",
    "    hsp = m.parameters['h_sp']\n",
    "    \n",
    "    mask  = m.mask\n",
    "    maskd = m.maskd\n",
    "    idxi  = m.idxi\n",
    "    idxj  = m.idxj\n",
    "    nmol  = m.nmol\n",
    "    molsize = m.molsize\n",
    "    w       = m.w\n",
    "    nHeavy = m.nHeavy\n",
    "    nHydro = m.nHydro\n",
    "    \n",
    "    eta = torch.zeros((N_rpa), device=device) \n",
    "    \n",
    "    #print('vexp1.shape', vexp1.shape)\n",
    "   # print('eta.shape', eta.shape)\n",
    "    eta[:vexp1.size(0)] = vexp1 # eta is stored as |X|; dcopy?\n",
    "    \n",
    "    eta_orig = torch.clone(eta)\n",
    "    # print('eta_orig.shape', eta_orig.shape)\n",
    "    # print('eta_orig\\n', eta_orig)\n",
    "    \n",
    "   # print('m.C MO', m.C_mo[0])\n",
    "    # print('eta.shape', eta.shape)\n",
    "    # print(eta)\n",
    "    eta_ao =  mo2ao(N_cis, eta, m, full=False)     # mo to ao basis (mo2site)\n",
    "    print('eta_ao.shape', eta_ao.shape)\n",
    "    print(eta_ao)\n",
    "\n",
    "    eta_ao_sym, eta_ao_asym = decompose_to_sym_antisym(eta_ao) # decompose to sym and asym\n",
    "\n",
    "    # Vxi - build 2e integrals in AO basis: G(guess density) in F = H_core + G\n",
    "    # note density is split into sym and anisym matrices\n",
    "    # sym is processed as padded 3d array in PYSEQM, see fock module \n",
    "    # antisym: 2c-2e works with modified PYSEQM routine; should be antisimmterized afterwards\n",
    "    # antisym: 1c-2e (diagonal) are taken from NEXMD for now - ugly code with loops\n",
    "    # TODO: vectorize 1c-2e part\n",
    "    \n",
    "    #------------------symmetric------------------------------\n",
    "    G_sym   =  build_G_sym(eta_ao_sym,\n",
    "                        gss, gsp, gpp, gp2, hsp,\n",
    "                        mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                        w,\n",
    "                         nHeavy,\n",
    "                         nHydro)\n",
    "    \n",
    "    \n",
    "    # G sym is 1c-2e and 2c-2e of symmetric part of guess density\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pack 2c-2e part to standard shape\n",
    "    G_sym = pack.pack(G_sym, nHeavy, nHydro)\n",
    "    print('!!! G_SYM\\n', G_sym)\n",
    "    \n",
    "    G_tot = build_G_antisym(eta_ao, eta_ao_asym, G_sym,\n",
    "                            gss, gsp, gpp, gp2, hsp,\n",
    "                            mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                            w, \n",
    "                            m,\n",
    "                            nHeavy,\n",
    "                            nHydro)\n",
    "    \n",
    "   # print('G_tot.shape', G_tot.shape)\n",
    "   # print('G_tot\\n', G_tot)                  \n",
    "    # build_G_antisym returns both sym and antisym!\n",
    "    # TODO: refactor into: 2c-2e antisym, 1c-2e antisym\n",
    "    # TODO: vectorize 1c-2e antisym, avoid ugly loops\n",
    "    #! remember about making 2c-2e diagonal 0\n",
    "\n",
    "    # print('G total \\n', G_tot)\n",
    "    \n",
    "    # print('============================================')\n",
    "    # print('Converting Gao full back into MO basis')\n",
    "    G_mo = ao2mo(N_cis, N_rpa, m, G_tot[0], m.C_mo, full=False) # G in MO basis #! [0] not batched yet\n",
    "    \n",
    "    print('G_mo.shape', G_mo.shape)\n",
    "    print('G_mo\\n', G_mo)\n",
    "    # multiply by MO differencies\n",
    "    # ii=0\n",
    "    # for p in range(m.nocc):\n",
    "    # # print('p', p)\n",
    "    #     for h in range(m.nocc, m.norb):\n",
    "    #         # print('h', h)\n",
    "    #         # print('i', i)\n",
    "    #         f = m.e_mo[0][h] - m.e_mo[0][p]\n",
    "    #         G_mo[ii] = G_mo[ii] + f * eta_orig[ii]\n",
    "    #         G_mo[ii+N_cis] = -G_mo[ii + N_cis] + f * eta_orig[ii+N_cis]\n",
    "    #         ii += 1\n",
    "        \n",
    "    print('G_mo.shape', G_mo.shape)\n",
    "    print('G_mo\\n', G_mo)\n",
    "    return G_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ao_full(vexp1, molecule, N_cis, N_rpa, CIS = True):\n",
    "\n",
    "    m = molecule\n",
    "\n",
    "    eta = torch.zeros((N_rpa), device=device) \n",
    "    \n",
    "    #print('vexp1.shape', vexp1.shape)\n",
    "   # print('eta.shape', eta.shape)\n",
    "    eta[:vexp1.size(0)] = vexp1 # eta is stored as |X|; dcopy?\n",
    "    \n",
    "    eta_orig = torch.clone(eta)\n",
    "    # print('eta_orig.shape', eta_orig.shape)\n",
    "    # print('eta_orig\\n', eta_orig)\n",
    "    \n",
    "   # print('m.C MO', m.C_mo[0])\n",
    "    # print('eta.shape', eta.shape)\n",
    "    # print(eta)\n",
    "    eta_ao =  mo2ao(N_cis, eta, m, full=False)     # mo to ao basis (mo2site)\n",
    "    return eta_ao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_to_sym_antisym(A):\n",
    "    \"\"\"\n",
    "    decomposes matrix into symmetric and antisymmetric parts\n",
    "\n",
    "    Args:\n",
    "        A (tensor): some matrix\n",
    "\n",
    "    Returns:\n",
    "        tuple of tensors: sym and antisym parts\n",
    "    \"\"\"   \n",
    "\n",
    "    A_sym = 0.5 * (A + A.T)\n",
    "    A_antisym = 0.5 * (A - A.T)\n",
    "    \n",
    "    return A_sym, A_antisym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_G_sym(M_ao,\n",
    "                gss, gsp, gpp, gp2, hsp,\n",
    "                mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                w,\n",
    "                nHydro,\n",
    "                nHeavy):\n",
    "    \n",
    "    \n",
    "      F = torch.zeros((nmol*molsize**2,4,4), device=device) # 0 Fock matrix to fill\n",
    "      # # TODO: feed params programmatically\n",
    "      \n",
    "      P0 = unpack(M_ao, nHydro, nHeavy, (nHeavy+nHydro)*4) # \n",
    "      P0 = torch.unsqueeze(P0, 0) # add dimension\n",
    "      \n",
    "      # print('P0.shape', P0.shape)\n",
    "      # print('P0\\n', P0)\n",
    "      #---------------fill diagonal 1c-2e -------------------\n",
    "      P = P0.reshape((nmol,molsize,4,molsize,4)) \\\n",
    "          .transpose(2,3).reshape(nmol*molsize*molsize,4,4)\n",
    "          \n",
    "      # print('P.shape', P.shape)\n",
    "      # print('P\\n', P)\n",
    "      \n",
    "      Pptot = P[...,1,1]+P[...,2,2]+P[...,3,3]\n",
    "      ## http://openmopac.net/manual/1c2e.html\n",
    "    #  (s,s)\n",
    "      TMP = torch.zeros_like(F)\n",
    "      TMP[maskd,0,0] = 0.5*P[maskd,0,0]*gss + Pptot[maskd]*(gsp-0.5*hsp)\n",
    "      for i in range(1,4):\n",
    "          #(p,p)\n",
    "          TMP[maskd,i,i] = P[maskd,0,0]*(gsp-0.5*hsp) + 0.5*P[maskd,i,i]*gpp \\\n",
    "                          + (Pptot[maskd] - P[maskd,i,i]) * (1.25*gp2-0.25*gpp)\n",
    "          #(s,p) = (p,s) upper triangle\n",
    "          TMP[maskd,0,i] = P[maskd,0,i]*(1.5*hsp - 0.5*gsp)\n",
    "      #(p,p*)\n",
    "      for i,j in [(1,2),(1,3),(2,3)]:\n",
    "          TMP[maskd,i,j] = P[maskd,i,j]* (0.75*gpp - 1.25*gp2)\n",
    "\n",
    "      F.add_(TMP)\n",
    "      \n",
    "           \n",
    "      #-----------------fill 2c-2e integrals----------------\n",
    "      weight = torch.tensor([1.0,\n",
    "                        2.0, 1.0,\n",
    "                        2.0, 2.0, 1.0,\n",
    "                        2.0, 2.0, 2.0, 1.0],dtype=dtype, device=device).reshape((-1,10))\n",
    "      \n",
    "      PA = (P[maskd[idxi]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,10,1))\n",
    "      PB = (P[maskd[idxj]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,1,10))\n",
    "      suma = torch.sum(PA*w,dim=1)\n",
    "      sumb = torch.sum(PB*w,dim=2)\n",
    "      sumA = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      sumB = torch.zeros_like(sumA)\n",
    "      \n",
    "      sumA[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = suma\n",
    "      sumB[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = sumb\n",
    "      F.index_add_(0,maskd[idxi],sumB)\n",
    "      #\\sum_A\n",
    "      F.index_add_(0,maskd[idxj],sumA)\n",
    "      \n",
    "      sum = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      # (ss ), (px s), (px px), (py s), (py px), (py py), (pz s), (pz px), (pz py), (pz pz)\n",
    "      #   0,     1         2       3       4         5       6      7         8        9\n",
    "\n",
    "      ind = torch.tensor([[0,1,3,6],\n",
    "                          [1,2,4,7],\n",
    "                          [3,4,5,8],\n",
    "                          [6,7,8,9]],dtype=torch.int64, device=device)\n",
    "      \n",
    "      Pp = -0.5*P[mask]\n",
    "      for i in range(4):\n",
    "        for j in range(4):\n",
    "            #\\sum_{nu \\in A} \\sum_{sigma \\in B} P_{nu, sigma} * (mu nu, lambda, sigma)\n",
    "            sum[...,i,j] = torch.sum(Pp*w[...,ind[i],:][...,:,ind[j]],dim=(1,2))\n",
    "      #print('mask', mask)    #! DIFFERS FROM PYSEQM, PROBABLY packing\n",
    "      F.index_add_(0,mask,sum)\n",
    "\n",
    "      F0 = F.reshape(nmol,molsize,molsize,4,4) \\\n",
    "             .transpose(2,3) \\\n",
    "             .reshape(nmol, 4*molsize, 4*molsize)\n",
    "    #\n",
    "      F0.add_(F0.triu(1).transpose(1,2))     \n",
    "      \n",
    "      F0 = 2 * F0 #! BE CAREFUL\n",
    "      # print('F0.shape', F0.shape)\n",
    "      # print(F0) \n",
    "      \n",
    "      return F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_G_antisym(eta_ao, eta_ao_asym, G_sym,\n",
    "                gss, gsp, gpp, gp2, hsp,\n",
    "                mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                w, \n",
    "                m,\n",
    "                nHydro,\n",
    "                nHeavy):\n",
    "\n",
    "      # TODO; figure how/why constants are defined in fock_skew\n",
    "      #!\n",
    "      #! CHECK\n",
    "      #!\n",
    "      # gss = torch.tensor([15.7558, 14.7942, 14.7942])  # GSSII   7.87788  7.39710   7.39710   1/2 # not used\n",
    "      # gpp = torch.tensor([13.6540,  0.0000,  0.0000])  # GPPII   6.82700   1/2\n",
    "      # gsp = torch.tensor([10.6212,  0.0000,  0.0000])  # GSPII   10.6211    \n",
    "      # gp2 = torch.tensor([12.4061,  0.0000,  0.0000])  # GP2II   15.5076\n",
    "      # hsp = torch.tensor([0.5939, 0.0000, 0.0000])     # HSPII   0.29694   1/2\n",
    "  \n",
    "      # see fock_skew in qm_fock in NEXMD\n",
    "      # create 1d array \n",
    "      eta_anti = torch.zeros((m.norb*(m.norb+1)//2), device=device)\n",
    "      \n",
    "      # l=0\n",
    "      # for i in range(m.norb):\n",
    "      #       # print('i', i)\n",
    "      #       for j in range(i+1):\n",
    "      #             print('i j', i, j)\n",
    "\n",
    "      #             eta_anti[l] = 0.5 * (eta_ao[i,j] - eta_ao[j,i])\n",
    "      #             l += 1   \n",
    "      #             # print('l', l) \n",
    "      # print('eta_anti NEXMD\\n')\n",
    "      # for e in eta_anti: print(f\"{e:.4f}\")\n",
    "      \n",
    "      eta_anti = torch.zeros((m.norb*(m.norb+1)//2), device=device)\n",
    "      indices = torch.tril_indices(int(m.norb), int(m.norb), offset = 0)  # Generate the upper triangular indices\n",
    "      \n",
    "      print('indices\\n', indices)\n",
    "      print('indices[0]\\n', indices[0].size())\n",
    "      eta_anti = 0.5 * (eta_ao[indices[0], indices[1]] - eta_ao[indices[1], indices[0]])\n",
    "      print('(eta_ao[indices[0], indices[1]]', eta_ao[indices[0], indices[1]])\n",
    "      # print('eta_anti vectorized\\n')\n",
    "      # for e in eta_anti: print(f\"{e:.4f}\")\n",
    "      \n",
    "      \n",
    "      # print tensor, one element per line\n",
    "      \n",
    "      eta_anti_2d = torch.zeros((m.norb, m.norb), device='cpu') \n",
    "\n",
    "      #restore to 2d form to build G 2c-2e part\n",
    "      # l = 0\n",
    "      # for i in range(0, m.norb): # TODO\" vectorize\n",
    "      #     for j in range(0,i):\n",
    "      #         l += 1\n",
    "      #         eta_anti_2d[i,j] += eta_anti[l-1]\n",
    "      #         eta_anti_2d[j,i] -= eta_anti[l-1]\n",
    "      #     l += 1 \n",
    "      # print('eta_anti_2d\\n', eta_anti_2d)\n",
    "      \n",
    "      # eta_anti_2d_vec = torch.zeros((m.norb, m.norb))     \n",
    "      \n",
    "      \n",
    "      eta_anti_2d[indices[1], indices[0]] = -eta_anti \n",
    "      eta_anti_2d = eta_anti_2d - eta_anti_2d.T # antisymmetrize\n",
    "      \n",
    "      # print('eta_anti_2d VEC\\n', eta_anti_2d)\n",
    "      #eta_anti_2d_vec.fill_diagonal_(0)\n",
    "\n",
    "      # print('maskd\\n', maskd)\n",
    "\n",
    "      # TODO: should be vectorized as in build G\n",
    "      # below explicit working example for H2O taken from NEXMD\n",
    "      # pascal2 = [1, 3, 6, 10, 15, 21]  # -1 pascal triangle\n",
    "      # pascal2 = [x -1 for x in pascal2]\n",
    "      # pascal1 = [0, 1, 3, 6, 10, 15] # -1 for python indexing\n",
    "      # pascal1 = [x -1 for x in pascal1]\n",
    "      # orb_loc1 = [0,4,5] # O orbs\n",
    "      # orb_loc2 = [3,4,5]\n",
    "      pascal2 = torch.cumsum(torch.arange(1, eta_anti_2d.shape[0]+1), dim=0)  # -1 pascal triangle\n",
    "      # pascal2 = [-1, 0, 2, 5, 9, 14]\n",
    "      pascal2 = pascal2 -1 \n",
    "      # print('pascal2\\n', pascal2)\n",
    "      \n",
    "      pascal1 = torch.cumsum(torch.arange(0, eta_anti_2d.shape[0]), dim=0) \n",
    "      pascal1 = pascal1 - 1\n",
    "      # pascal1 = [0,  2,  5,  9, 14, 20, 27]\n",
    "      # pascal1 = [-1, 0, 2, 5, 9, 14]\n",
    "      # print('pascal1\\n', pascal1)\n",
    "      # print(m.Z)\n",
    "      \n",
    "      # ! TODO: write programamtically\n",
    "      \n",
    "      # orb_loc1 = [0] # [start of orbitals index of x]\n",
    "      # orb_loc2 = [1 if m.Z[0] == 1 else 3] # [end of orbitals index of x]\n",
    "      orb_loc1 = []\n",
    "      orb_loc2 = []\n",
    "      # orb_loc1 = [orb_loc1[i - 1] + 4 if m.Z[i] != 1 else orb_loc1[i - 1] + 1 for i, x in enumerate(m.Z)]\n",
    "      Z = m.Z\n",
    "      for i,z in enumerate(Z):\n",
    "\n",
    "        if i == 0:\n",
    "          orb_loc1.append(0)\n",
    "          if Z[i] != 1:\n",
    "            orb_loc2.append(3)\n",
    "          else:\n",
    "            orb_loc2.append(0)\n",
    "          continue\n",
    "        \n",
    "        if Z[i-1] != 1:\n",
    "            orb_loc1.append(orb_loc1[i-1]+4)\n",
    "        elif Z[i-1] == 1:\n",
    "            orb_loc1.append(orb_loc1[i-1]+1)\n",
    "          \n",
    "        if Z[i] != 1:\n",
    "            orb_loc2.append(orb_loc2[i-1]+4)\n",
    "        elif Z[i] == 1:    \n",
    "            orb_loc2.append(orb_loc2[i-1]+1)\n",
    "          \n",
    "             \n",
    "      # print('orb_loc1\\n', orb_loc1)\n",
    "      # print('orb_loc2\\n', orb_loc2)\n",
    "            \n",
    "      # orb_loc1 = [0,4,5]\n",
    "      G_1c2e = torch.zeros((m.norb*(m.norb+1)//2))\n",
    "    \n",
    "      for ii in range(molsize): # n_atoms?\n",
    "      # print('ii', ii)\n",
    "        if m.Z[ii] == 1:\n",
    "          pass\n",
    "        \n",
    "        else:\n",
    "          gsp_ii = gsp[ii]\n",
    "          gpp_ii = gpp[ii]\n",
    "          gp2_ii = gp2[ii]\n",
    "          hsp_ii = hsp[ii]\n",
    "          \n",
    "          ia = orb_loc1[ii]\n",
    "          # print('ia', ia)\n",
    "          ib = orb_loc2[ii]\n",
    "          \n",
    "          iplus = ia+1\n",
    "          ka = pascal2[ia]\n",
    "          l = ka\n",
    "\n",
    "          for j in range(iplus, ib+1):\n",
    "          # print('j', j)\n",
    "          # print('ia', ia)\n",
    "          # print('ib', ib)\n",
    "          # print('l', l)\n",
    "\n",
    "            mm = l+ia+1\n",
    "            l = l+j+1\n",
    "            \n",
    "            # print(type(mm)) \n",
    "            G_1c2e[mm] = G_1c2e[mm] + 0.5*eta_anti[mm] * (hsp_ii - gsp_ii)\n",
    "            # print('(hsp - gsp)', (hsp - gsp))\n",
    "            # print('F(M) FIRST', G_1c2e[mm])\n",
    "           # print('G_1c2e[mm]', G_1c2e[mm])\n",
    "          #  print('===================')\n",
    "            \n",
    "          iminus = ib-1\n",
    "        \n",
    "          for j in range(iplus, iminus+1):\n",
    "            icc = j\n",
    "            # print('icc', icc)\n",
    "            for l in range(icc, ib):\n",
    "              # print('l', l)\n",
    "              mm = pascal1[l+1] + j+1\n",
    "              # print('mm', mm)\n",
    "              # print('(0.25*gpp_ii - 0.6*gp2_ii)', (0.25*gpp_ii - 0.6*gp2_ii) )\n",
    "              G_1c2e[mm] = G_1c2e[mm]+ eta_anti[mm] * (0.25*gpp_ii - 1.25*0.6*gp2_ii) \n",
    "              # print('F(M) SECOND', G_1c2e[mm])\n",
    "            #  print('G_1c2e[mm]', G_1c2e[mm])\n",
    "            \n",
    "      G_1c2e = G_1c2e*2 # antisym 1c2e part\n",
    "      # print('G_1c2e\\n', G_1c2e)\n",
    "      \n",
    "\n",
    "      # buils antisymmetric part as Vxi_packA, requires G_sym\n",
    "      \n",
    "   #   print('G_sym shape', G_sym.shape)\n",
    "   #   print('G_sym\\n', G_sym)\n",
    "      \n",
    "     # print('molsize', molsize)\n",
    "     \n",
    "      G_sym = G_sym[0] #! works for one mol only?\n",
    "      G_sym_orig = G_sym.clone()\n",
    "      # print('G_sym shape', G_sym.shape)\n",
    "      # print('G_sym\\n', G_sym)\n",
    "      \n",
    "      # l = 0\n",
    "      # for i in range(0, m.norb): # TODO vectorize\n",
    "      #     for j in range(0,i):\n",
    "      #         l += 1\n",
    "\n",
    "      #         G_sym[i,j] += G_1c2e[l-1]\n",
    "      #         G_sym[j,i] -= G_1c2e[l-1]             \n",
    "      #     l += 1 # skip diagonal\n",
    "      # print('G sym after ADDING ANTSYM PART')\n",
    "      # print('G_sym\\n', G_sym)\n",
    "      \n",
    "      # pack from 1d to 2d ANTISYM 1c2e part of G\n",
    "      G_anti_1c2e = torch.zeros(m.norb, m.norb)\n",
    "      G_anti_1c2e[indices[1], indices[0]] = -G_1c2e\n",
    "      G_anti_1c2e = G_anti_1c2e - G_anti_1c2e.T\n",
    "      \n",
    "      # print('G_anti_1c2e\\n', G_anti_1c2e)\n",
    "      # print(G_anti_1c2e.shape)\n",
    "      #print(G_sym.shape)\n",
    "      # print('G SYM\\n', G_sym)\n",
    "      # print('SUM')\n",
    "      # print(G_sym_orig + G_anti_1c2e)\n",
    "    \n",
    "      # print('G_anti_1c shape', G_anti_1c.shape)\n",
    "      # print('G_anti_1c\\n', G_anti_1c)\n",
    "      \n",
    "    #  print('eta_ao_asym shape', eta_ao_asym.shape)\n",
    "    #  print('eta_ao_asym\\n', eta_ao_asym)\n",
    "      \n",
    "      # G_tmp = 2* G_anti_1c + G_sym\n",
    "      G_sym = torch.unsqueeze(G_sym, 0) #  add dimension\n",
    "\n",
    "      # build 2c-2e part of antisymmetric G\n",
    "      # copied from FOCK\n",
    "      \n",
    "      F = torch.zeros((nmol*molsize**2,4,4), device=device) # 0 Fock matrix to fill\n",
    "      P0 = unpack(eta_ao_asym, nHydro, nHeavy, (nHeavy+nHydro)*4) # \n",
    "      P0 = torch.unsqueeze(P0, 0) # add dimension\n",
    "      \n",
    "      P = P0.reshape((nmol,molsize,4,molsize,4)) \\\n",
    "          .transpose(2,3).reshape(nmol*molsize*molsize,4,4)\n",
    "          \n",
    "      #P = P[...,1,1]+P[...,2,2]+P[...,3,3] #! MODIFIED\n",
    "      #-----------------fill 2c-2e integrals----------------\n",
    "      weight = torch.tensor([1.0,\n",
    "                        2.0, 1.0,\n",
    "                        2.0, 2.0, 1.0,\n",
    "                        2.0, 2.0, 2.0, 1.0],dtype=dtype, device=device).reshape((-1,10))\n",
    "      \n",
    "      PA = (P[maskd[idxi]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,10,1))\n",
    "      PB = (P[maskd[idxj]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,1,10))\n",
    "      suma = torch.sum(PA*w,dim=1)\n",
    "      sumb = torch.sum(PB*w,dim=2)\n",
    "      sumA = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      sumB = torch.zeros_like(sumA)\n",
    "      \n",
    "      sumA[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = suma\n",
    "      sumB[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = sumb\n",
    "      F.index_add_(0,maskd[idxi],sumB)\n",
    "      #\\sum_A\n",
    "      F.index_add_(0,maskd[idxj],sumA)\n",
    "      \n",
    "      sum = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      # (ss ), (px s), (px px), (py s), (py px), (py py), (pz s), (pz px), (pz py), (pz pz)\n",
    "      #   0,     1         2       3       4         5       6      7         8        9\n",
    "\n",
    "      ind = torch.tensor([[0,1,3,6],\n",
    "                          [1,2,4,7],\n",
    "                          [3,4,5,8],\n",
    "                          [6,7,8,9]],dtype=torch.int64, device=device)\n",
    "      \n",
    "      Pp = -0.5*P[mask]\n",
    "      for i in range(4):\n",
    "        for j in range(4):\n",
    "            #\\sum_{nu \\in A} \\sum_{sigma \\in B} P_{nu, sigma} * (mu nu, lambda, sigma)\n",
    "            sum[...,i,j] = torch.sum(Pp*w[...,ind[i],:][...,:,ind[j]],dim=(1,2))\n",
    "     # print('mask', mask)    #! DIFFERS FROM PYSEQM, PROBABLY packing\n",
    "      F.index_add_(0,mask,sum)\n",
    "\n",
    "      F0 = F.reshape(nmol,molsize,molsize,4,4) \\\n",
    "             .transpose(2,3) \\\n",
    "             .reshape(nmol, 4*molsize, 4*molsize)\n",
    "    #\n",
    "      F0.add_(F0.triu(1).transpose(1,2))     \n",
    "      \n",
    "      # F0 is still symmetric, probably symmetrized above\n",
    "      # here we make it antisymmetric back\n",
    "      #F0 = 2 * F0 \n",
    "      F0 = pack.pack(F0, m.nHeavy, m.nHydro)\n",
    "      \n",
    "      rows, cols = torch.tril_indices(F0.shape[1], F0.shape[2])\n",
    "      F0[0][rows, cols] *= -1\n",
    "      F0[0][torch.eye(F0.shape[1]).bool()] *= -1\n",
    "      \n",
    "      \n",
    "\n",
    "      F0[0].diagonal().fill_(0) #! BE WARNED, THIS iS TAKEN FROM OLD NEXMD, PYSEQM produces non-zero diagonal\n",
    "      F0 = F0*2\n",
    "    #  print('G ANTISYM shape', F0.shape)\n",
    "    #  print('G ANTISYM\\n', F0*2)\n",
    "      G_full = G_sym + G_anti_1c2e + F0 # summ of G_sym(sym 1c2e + 2c2e) + antisym 1c2e and 2c2e (F0)\n",
    "      # print('G ANTISYM\\n', G_anti_1c2e + F0)\n",
    "      # print('G_full shape', G_full.shape)\n",
    "      # print('G_full\\n', G_full)\n",
    "      \n",
    "      return G_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ao2mo(N_cis, N_rpa, molecule, M_ao, C, full=False):\n",
    "    \"\"\"\n",
    "    transform matrix from AO to MO basis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M_AO : torch tensor # TODO add size\n",
    "        matrix in AO basis\n",
    "    C : torch tensor # TODO add size\n",
    "        matrix of MO coefficients # TODO row or columns, structure?\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    M_MO : torch tensor # TODO add size\n",
    "        matrix in MO basis\n",
    "    \"\"\"    \n",
    "    m = molecule\n",
    "    \n",
    "    if full == True:\n",
    "        M_mo = m.C_mo[0].T @ M_ao @ m.C_mo[0]\n",
    "        return M_mo\n",
    "        \n",
    "    else:\n",
    "         # COPY of subroutine site2mo from Lioville\n",
    "         \n",
    "        G_ao = M_ao # TODO rename\n",
    "        \n",
    "        # eta1 = eta1.view(-1, m.nvirt[0]) # 1d -> 2d\n",
    "        # print(eta1.shape)\n",
    "        # print('eta1', eta1)\n",
    "        # print('==============')\n",
    "        \n",
    "        eta_mo = torch.zeros((N_rpa))\n",
    "       # eta_mo = torch.zeros((m.norb, m.norb), device=device)\n",
    "\n",
    "        dgemm1 = G_ao.T @ m.C_mo[0]\n",
    "\n",
    "        # print('dgemm1.shape', dgemm1.shape)\n",
    "        # print(dgemm1)\n",
    "        \n",
    "        dgemm2 =  m.C_mo[0][:, m.nocc:m.norb].T @ dgemm1[:,:m.nocc]\n",
    "        \n",
    "\n",
    "        dgemm2 = dgemm2.T.flatten()\n",
    "        eta_mo[:dgemm2.size(0)] = dgemm2 \n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "        \n",
    "        dgemm3 =  dgemm1[:, m.nocc:].T @ m.C_mo[0][:, :m.nocc]\n",
    "        \n",
    "        # print('dgemm3.T.shape', dgemm3.T.shape)\n",
    "        # print(dgemm3.T)\n",
    "        \n",
    "        eta_mo[N_cis:] = dgemm3.T.flatten() \n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "\n",
    "        M_mo = eta_mo\n",
    "    \n",
    "    return M_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mo2ao(N_cis, M_mo, molecule, full=False):\n",
    "    \"\"\"\n",
    "    transform matrix from AO to MO basis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M_AO : torch tensor # TODO add size\n",
    "        matrix in AO basis\n",
    "    C : torch tensor # TODO add size\n",
    "        matrix of MO coefficients # TODO row or columns, structure?\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    M_MO : torch tensor # TODO add size\n",
    "        matrix in MO basis\n",
    "    \"\"\"    \n",
    "    m = molecule\n",
    "    \n",
    "    # print('m C_mo', m.C_mo)\n",
    "    if full == True:\n",
    "        M_ao = m.C_mo[0].T @ M_mo @ m.C_mo[0] #! does not currently work\n",
    "        \n",
    "        return M_ao\n",
    "    else:\n",
    "        \n",
    "        eta = M_mo # TODO rename\n",
    "        \n",
    "        eta1 = eta[:N_cis]\n",
    "        eta1 = eta1.view(-1, m.nvirt[0]) # 1d -> 2d\n",
    "        # print(eta1.shape)\n",
    "        # print('eta1', eta1)\n",
    "        # print('==============')\n",
    "        \n",
    "        eta_mo = torch.zeros((m.norb, m.norb), device=device)\n",
    "\n",
    "        dgemm1 = eta1 @ m.C_mo[0][:, m.nocc:m.norb].T # operations on |X| ?\n",
    "\n",
    "        # print('dgemm1.shape', dgemm1.shape)\n",
    "        # print(dgemm1)\n",
    "        \n",
    "        eta_mo[:m.nocc] = dgemm1\n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "        \n",
    "        \n",
    "        eta2 = eta[N_cis:]                            # operations on |Y| ?\n",
    "        eta2 = eta2.view(-1, m.nvirt[0]) # 1d -> 2d\n",
    "    \n",
    "        dgemm2 = eta2.T @ m.C_mo[0][:, :m.nocc].T\n",
    "        \n",
    "        # print('dgemm2.shape', dgemm2.shape)\n",
    "        # print(dgemm2)\n",
    "        \n",
    "        eta_mo[m.nocc:] = dgemm2\n",
    "        # print('eta_mo', eta_mo.shape)\n",
    "        # print(eta_mo)\n",
    "        \n",
    "        dgemm3 = m.C_mo[0] @ eta_mo\n",
    "        eta_ao = dgemm3 \n",
    "    \n",
    "    \n",
    "        return eta_ao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUX routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize_torch(U, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Orthogonalizes the matrix U (d x n) using Gram-Schmidt Orthogonalization.\n",
    "    If the columns of U are linearly dependent with rank(U) = r, the last n-r columns \n",
    "    will be 0.\n",
    "    \n",
    "    Args:\n",
    "        U (numpy.array): A d x n matrix with columns that need to be orthogonalized.\n",
    "        eps (float): Threshold value below which numbers are regarded as 0 (default=1e-15).\n",
    "    \n",
    "    Returns:\n",
    "        (numpy.array): A d x n orthogonal matrix. If the input matrix U's cols were\n",
    "            not linearly independent, then the last n-r cols are zeros.\n",
    "    \n",
    "    Examples:\n",
    "    ```python\n",
    "    >>> import numpy as np\n",
    "    >>> import gram_schmidt as gs\n",
    "    >>> gs.orthogonalize(np.array([[10., 3.], [7., 8.]]))\n",
    "    array([[ 0.81923192, -0.57346234],\n",
    "       [ 0.57346234,  0.81923192]])\n",
    "    >>> gs.orthogonalize(np.array([[10., 3., 4., 8.], [7., 8., 6., 1.]]))\n",
    "    array([[ 0.81923192 -0.57346234  0.          0.        ]\n",
    "       [ 0.57346234  0.81923192  0.          0.        ]])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(U[0])\n",
    "    # numpy can readily reference rows using indices, but referencing full rows is a little\n",
    "    # dirty. So, work with transpose(U)\n",
    "    V = U.T\n",
    "    for i in range(n):\n",
    "        prev_basis = V[0:i]     # orthonormal basis before V[i]\n",
    "        coeff_vec = prev_basis @ V[i].T  # each entry is np.dot(V[j], V[i]) for all j < i\n",
    "        # subtract projections of V[i] onto already determined basis V[0:i]\n",
    "        V[i] -= (coeff_vec @ prev_basis).T\n",
    "        if torch.norm(V[i]) < eps:\n",
    "            V[i][V[i] < eps] = 0.   # set the small entries to 0\n",
    "        else:\n",
    "            V[i] /= torch.norm(V[i])\n",
    "    return V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_V(N_cis, N_rpa, n_V_start):\n",
    "    \n",
    "    # returns vexp1 - guess vector for L-xi routine\n",
    "    logger.qm2(fmt_log(n_V_start, 'n_V_start', 'qm'))\n",
    "    rrwork = torch.zeros(N_rpa * 4, device=device)\n",
    "    i = 0\n",
    "    for ip in range(mol.nocc):\n",
    "        for ih in range(mol.nvirt):\n",
    "            rrwork[i] = mol.e_mo[0][mol.nocc + ih] - mol.e_mo[0][ip]  # !Lancos vectors(i) ???\n",
    "            i += 1                                                                      #  TODO: [0] should be replaced by m batch index\n",
    "                                                                                    \n",
    "    rrwork_sorted, indices = torch.sort(rrwork[:N_cis], descending=False, stable=True) # preserve order of degenerate\n",
    "    logger.qm2(fmt_log(rrwork_sorted, 'rrwork_sorted', 'qm'))\n",
    "\n",
    "\n",
    "    # vexp1 = torch.zeros((N_cis, N_cis), device=device)\n",
    "    vexp1 = torch.zeros((N_cis, N_cis), device=device)\n",
    "    \n",
    "    row_idx = torch.arange(0, int(N_cis), device=device)\n",
    "    col_idx = indices[:N_cis]\n",
    "\n",
    "    vexp1[row_idx, col_idx] = 1.0 \n",
    "    logger.qm2(fmt_log(vexp1, 'V  BEFORE SELECTING PART', 'qm'))\n",
    "    logger.qm2(fmt_log(vexp1.shape, 'V shape', 'qm'))\n",
    "   #! THIS IS NEW, TAKE only part \n",
    "\n",
    "    V = vexp1[:,  :n_V_start]\n",
    "    logger.qm2(fmt_log(V, 'V = vexp1', 'qm'))\n",
    "    logger.qm2(fmt_log(V.shape, 'V shape', 'qm'))\n",
    "\n",
    "    return V\n",
    "\n",
    "# TODO: check whether L_xi should be regenerated during expansion each time or just part of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAVIDSON routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)  # custom logging level; lower than DEBUG\n",
    "                               # printed above QM (QM, DEBUG, INFO, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<module> : 3 : DEBUG : i = 0\n",
      "<module> : 3 : DEBUG : i = 1\n",
      "<module> : 3 : DEBUG : i = 2\n",
      "<module> : 3 : DEBUG : i = 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    logger.debug('i = %d', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def davidson(mol, N_exc, keep_n, n_V_max,  max_iter, tol):\n",
    "    \"\"\"\n",
    "    Davidson algorithm for solving eigenvalue problem of large sparse diagonally dominant matrices\n",
    "    Hamiltonian is not generated or stored explicitly, only matrix-vector products are used on-the fly:\n",
    "    guess space V should be orthogonalized at each iteration\n",
    "    M (projection of smaller size) is V.T @ H @ V \n",
    "    #! RPA (TDHF) is not implemented yet, non-Hermitian (non-symmetric), requires also left eigenvectors \n",
    "    note that notation differes between implementations: V.T x A x V is bAb\n",
    "    # TODO: 1) check if convergence of e_vals is needed\n",
    "    # TODO: 2) vectorize and optimize orthogonalization\n",
    "    # TODO: 3) check if some vectors should be dropped \n",
    "    # TODO: 4) eliminate loops \n",
    "    # TODO: 5) check if whole M should be regenerated, or only sub-blocks corresponding to new guess vectors\n",
    "    # TODO: 6) add parameter checker like Krylov dims << N_cis\n",
    "\n",
    "    Args:\n",
    "        mol (PYSEQM object): object to hold all qm data from PYSEQM\n",
    "        N_exc (int)        : number of excited states to calculate\n",
    "        keep_n (int)       : number of e_vals, e_vecs to keep at each iteration\n",
    "        n_V_max (int)      : maximum size of Krylov subspace, \n",
    "                             projected matrix will be no more than M(n_V_max x n_V_max)\n",
    "        max_iter (int)     : maximum number of iterations in Davidson\n",
    "        tol (float)        : treshold for residual\n",
    "        \n",
    "    Returns:\n",
    "        tuple of tensors: eigenvalues (excitation energies in default units, eV) and eigenvectors \n",
    "    \"\"\"    \n",
    "    \n",
    "    n_V_start = N_exc * 2 # dimension of Krylov subspace, analogue of nd1  \n",
    "    N_cis = mol.nocc * mol.nvirt\n",
    "    N_rpa = 2 * N_cis\n",
    "    \n",
    "    term = False  # terminate algorithm\n",
    "    iter = 0\n",
    "    L_xi = torch.zeros((N_rpa, n_V_start), device=device)\n",
    "\n",
    "    V = gen_V(N_cis, N_rpa, n_V_start) # generate initial guess, V here #! should be renamed\n",
    "    diag = None # create diagonal of M only once\n",
    "    \n",
    "    while iter < max_iter and not term: # Davidson loop\n",
    "        \n",
    "        if iter > 0: # skip first step, as initial V is orthogonal\n",
    "            V = orthogonalize_torch(V)\n",
    "            \n",
    "        print('=================================', flush=True)\n",
    "        print(colored(f' ITERATION : {iter} ', 'red', 'on_white', attrs=['bold']), flush=True)\n",
    "        print('SUBSPACE SIZE V: ', V.shape, flush=True)\n",
    "        print('=================================')\n",
    "       \n",
    "        # ---------- form A x b product --------------------\n",
    "        L_xi = torch.zeros((N_rpa, V.shape[1] ), device=device) #! NOT iter here\n",
    "        logger.qm1(fmt_log(V, 'V BEFORE L_xi after ORTO', 'qm'))\n",
    "        ao_full = torch.zeros((N_cis, N_cis), device=device)\n",
    "        for i in range(V.shape[1]): \n",
    "            logger.qm3('Lxi iterations=%s', i)\n",
    "            L_xi[:,i] = form_L_xi(V[:,i], mol, N_cis, N_rpa)\n",
    "           # ao_full[:,i] = build_ao_full(V[:,i], mol, N_cis, N_rpa)\n",
    "            logger.qm3(fmt_log(L_xi[:,i], 'L_xi[:,i]', 'qm'))\n",
    "        print('**** AO_full', flush=True)\n",
    "        \n",
    "        print('!!!!! Lxi', L_xi)\n",
    "        print('!!!!! Lxi.shape', L_xi.shape)\n",
    "        L_xi[N_cis:, :] = L_xi[:N_cis] #! TODO: make sure that this A+B, A-B, not just copy for RPA\n",
    "    \n",
    "        right_V = L_xi[N_cis:] # (A)b \n",
    "        \n",
    "        logger.qm1(fmt_log(right_V.shape, 'right_V shape', 'matrix'))\n",
    "        logger.qm1(fmt_log(right_V, 'right_V', 'matrix'))       \n",
    "        # ---------- form b.T x Ab product --------------------\n",
    "        \n",
    "        M =  V.T @ right_V\n",
    "        \n",
    "        # logger.debug(fmt_log(M.shape, 'M shape', 'qm'))\n",
    "        # logger.debug(fmt_log(M, 'M', 'qm'))\n",
    "        if iter == 0:\n",
    "            diag = torch.diag(M) # create diagonal only once\n",
    "            \n",
    "        iter += 1\n",
    "        \n",
    "        logger.qm1(fmt_log(diag, 'diag', 'qm'))\n",
    "    \n",
    "        # ---------- diagonalize projection M --------------------\n",
    "        r_eval, r_evec = torch.linalg.eig(M) # find eigenvalues and eigenvectors\n",
    "       \n",
    "        r_eval = r_eval.real\n",
    "        r_evec = r_evec.real\n",
    "        r_eval, r_idx = torch.sort(r_eval, descending=False) # sort eigenvalues in ascending order\n",
    "        logger.debug(fmt_log(r_eval, 'RIGHT EVALS', 'evals'))\n",
    "        r_evec = r_evec[:, r_idx] # sort eigenvectors accordingly\n",
    "    \n",
    "        e_val_n = r_eval[:keep_n] # keep only the lowest keep_n eigenvalues; full are still stored as e_val\n",
    "        e_vec_n = r_evec[:, :keep_n]\n",
    "        resids = torch.zeros(V.shape[0], len(e_val_n)) # account for left and right evecs\n",
    "\n",
    "        # ---------- calculate residual vectors --------------------\n",
    "        for j in range(len(e_val_n)): # calc residuals \n",
    "            resids[:,j] = right_V @ e_vec_n[:,j] - e_val_n[j] * (V @ e_vec_n[:,j])\n",
    "            \n",
    "       # logger.debug(fmt_log(resids, 'resids', 'matrix'))     \n",
    "        resids_norms_r = torch.tensor([resids[:,x].norm() for x in range(resids.shape[1])])\n",
    "\n",
    "        # ---------- expand guess space V buy not-converged resids --------------------\n",
    "        # !!! PROBABLY HIGHLY INEFFICIENT !!! \n",
    "        if torch.any(resids_norms_r > tol):\n",
    "            mask_r = resids_norms_r >= tol\n",
    "            large_res_r = resids[:,mask_r] # residuals larger than tol\n",
    "           # logger.debug(fmt_log(large_res_r, 'LARGE RESIDUALS', 'vector'))           \n",
    "            large_res_r.to(device)\n",
    "            cor_e_val_r = e_val_n[mask_r] # corresponding eigenvalues !!! check if matches\n",
    "            \n",
    "            # ------keep adding new resids --------------------\n",
    "            if V.shape[1] <= n_V_max:     \n",
    "\n",
    "                    for j in range(large_res_r.shape[1]):\n",
    "                        if V.shape[1] <= n_V_max:\n",
    "                            s = large_res_r[:,j] # conditioned residuals > tol\n",
    "\n",
    "                            if s.norm() >= tol:\n",
    "                                logger.debug(fmt_log((s.norm().item()), 'NORM of RESIDUAL', 'warn'))\n",
    "                                denom = (diag[j] - cor_e_val_r[j])\n",
    "                                denom.to(device) \n",
    "                                s = s/denom # conditioned residuals\n",
    "                                s.to(device)\n",
    "                                # logger.debug(fmt_log(s.norm(), 'NORM OF NEW RESIDUAAL', 'vector'))\n",
    "                                V = torch.column_stack((V, s/s.norm()))\n",
    "                            else:\n",
    "                                pass\n",
    "            # ------ collapse (restart) if space V is too large; mix eigenvectors with V------------\n",
    "            else:\n",
    "                logger.debug(fmt_log(None, '!!!! MAX subspace reached !!!!', 'warn'))\n",
    "                #logger.debug(fmt_log(V, 'V before collapse', 'qm'))\n",
    "\n",
    "                V =  V @ r_evec[:, :n_V_start]\n",
    "                logger.debug(fmt_log(V.shape, 'V shape after restart', 'qm'))\n",
    "                #logger.debug(fmt_log(V, 'V AFTER collapse', 'qm'))\n",
    "\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            term = True\n",
    "            print('============================', flush=True)\n",
    "            print('all residuals are below tolerance')\n",
    "            print('DAVIDSON ALGORITHM CONVERGED', flush=True)\n",
    "            print('============================', flush=True)\n",
    "\n",
    "            return r_eval, r_evec\n",
    "\n",
    "    # runs after big loop if did not converge\n",
    "    print('============================', flush=True)\n",
    "    print('!!! DAVIDSON ALGORITHM DID NOT CONVERGE !!!', flush=True)\n",
    "    print('============================', flush=True)\n",
    "    \n",
    "    return r_eval, r_evec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 0 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([8, 6])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "eta_ao.shape torch.Size([6, 6])\n",
      "tensor([[    -0.00000,     -0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000],\n",
      "        [    -0.33445,     -0.00000,     -0.00000,     -0.55482,      0.53866,      0.53866],\n",
      "        [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000],\n",
      "        [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000],\n",
      "        [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000],\n",
      "        [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000]])\n",
      "!!! G_SYM\n",
      " tensor([[[    -0.00000,      1.47818,     -0.00000,     -0.00000,     -0.00000,     -0.00000],\n",
      "         [     1.47818,     -0.00000,      0.00000,      2.92232,     -2.74748,     -2.74748],\n",
      "         [    -0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000],\n",
      "         [    -0.00000,      2.92232,     -0.00000,     -0.00000,      0.00000,      0.00000],\n",
      "         [    -0.00000,     -2.74748,      0.00000,      0.00000,      0.00000,      0.00000],\n",
      "         [    -0.00000,     -2.74748,      0.00000,      0.00000,      0.00000,     -0.00000]]], grad_fn=<StackBackward0>)\n",
      "indices\n",
      " tensor([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "        [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5]])\n",
      "indices[0]\n",
      " torch.Size([21])\n",
      "(eta_ao[indices[0], indices[1]] tensor([    -0.00000,     -0.33445,     -0.00000,      0.00000,      0.00000,      0.00000,      0.00000,      0.00000,      0.00000,      0.00000,      0.00000,      0.00000,      0.00000,\n",
      "             0.00000,     -0.00000,      0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000])\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([    -0.00000,      0.00000,      0.00000,     -0.00000,      0.00000,     -0.00000,    -10.40985,      0.00000,     -0.00000,     -0.00000,      0.00000,     -0.00000,      0.00000,\n",
      "            -0.00000,      0.25850,     -0.00000], grad_fn=<CopySlices>)\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([    -0.00000,      0.00000,      0.00000,     -0.00000,      0.00000,     -0.00000,    -10.40985,      0.00000,     -0.00000,     -0.00000,      0.00000,     -0.00000,      0.00000,\n",
      "            -0.00000,      0.25850,     -0.00000], grad_fn=<CopySlices>)\n",
      "eta_ao.shape torch.Size([6, 6])\n",
      "tensor([[    -0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000],\n",
      "        [    -0.00000,      0.00000,      0.63698,     -0.00000,     -0.54510,      0.54510],\n",
      "        [     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000]])\n",
      "!!! G_SYM\n",
      " tensor([[[     0.00000,      0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "         [     0.00000,      0.00000,     -3.35503,      0.00000,      2.78029,     -2.78029],\n",
      "         [    -0.00000,     -3.35503,      0.00000,      0.00000,      0.00000,      0.00000],\n",
      "         [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,      0.00000],\n",
      "         [     0.00000,      2.78029,      0.00000,     -0.00000,      0.00000,      0.00000],\n",
      "         [    -0.00000,     -2.78029,      0.00000,      0.00000,      0.00000,      0.00000]]], grad_fn=<StackBackward0>)\n",
      "indices\n",
      " tensor([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "        [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5]])\n",
      "indices[0]\n",
      " torch.Size([21])\n",
      "(eta_ao[indices[0], indices[1]] tensor([    -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000,\n",
      "             0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000])\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([     0.00000,      0.00000,     -0.00000,      0.00000,      0.00000,      0.00000,      0.00000,    -10.58943,      0.00000,      0.00000,     -0.00000,      0.00000,      0.00000,\n",
      "             0.00000,     -0.00000,      0.25317], grad_fn=<CopySlices>)\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([     0.00000,      0.00000,     -0.00000,      0.00000,      0.00000,      0.00000,      0.00000,    -10.58943,      0.00000,      0.00000,     -0.00000,      0.00000,      0.00000,\n",
      "             0.00000,     -0.00000,      0.25317], grad_fn=<CopySlices>)\n",
      "eta_ao.shape torch.Size([6, 6])\n",
      "tensor([[    -0.11378,     -0.00000,     -0.00000,     -0.18875,      0.18326,      0.18326],\n",
      "        [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000],\n",
      "        [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000],\n",
      "        [     0.27589,      0.00000,      0.00000,      0.45768,     -0.44434,     -0.44434],\n",
      "        [     0.10676,      0.00000,      0.00000,      0.17711,     -0.17195,     -0.17195],\n",
      "        [     0.10676,      0.00000,      0.00000,      0.17711,     -0.17195,     -0.17195]])\n",
      "!!! G_SYM\n",
      " tensor([[[     0.32119,     -0.00000,     -0.00000,     -0.86754,     -1.45302,     -1.45302],\n",
      "         [    -0.00000,      1.70476,     -0.00000,     -0.00000,      0.00000,      0.00000],\n",
      "         [    -0.00000,     -0.00000,      1.36648,     -0.00000,     -0.07799,      0.07799],\n",
      "         [    -0.86754,     -0.00000,     -0.00000,     -3.32494,      1.30184,      1.30184],\n",
      "         [    -1.45302,      0.00000,     -0.07799,      1.30184,      2.02095,      1.37258],\n",
      "         [    -1.45302,      0.00000,      0.07799,      1.30184,      1.37258,      2.02095]]], grad_fn=<StackBackward0>)\n",
      "indices\n",
      " tensor([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "        [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5]])\n",
      "indices[0]\n",
      " torch.Size([21])\n",
      "(eta_ao[indices[0], indices[1]] tensor([    -0.11378,      0.00000,      0.00000,      0.00000,      0.00000,      0.00000,      0.27589,      0.00000,      0.00000,      0.45768,      0.10676,      0.00000,      0.00000,\n",
      "             0.17711,     -0.17195,      0.10676,      0.00000,      0.00000,      0.17711,     -0.17195,     -0.17195])\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([    -0.66552,      0.00000,     -0.00000,     -0.25209,     -9.47437,     -0.00000,      0.00000,      0.00000,     -0.09729,     -0.00000,      0.00000,     -0.49472,      0.79536,\n",
      "            -0.00000,      0.00000,      0.00000], grad_fn=<CopySlices>)\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([    -0.66552,      0.00000,     -0.00000,     -0.25209,     -9.47437,     -0.00000,      0.00000,      0.00000,     -0.09729,     -0.00000,      0.00000,     -0.49472,      0.79536,\n",
      "            -0.00000,      0.00000,      0.00000], grad_fn=<CopySlices>)\n",
      "eta_ao.shape torch.Size([6, 6])\n",
      "tensor([[    -0.00000,      0.00000,      0.21670,     -0.00000,     -0.18544,      0.18544],\n",
      "        [     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.52545,      0.00000,      0.44965,     -0.44965],\n",
      "        [     0.00000,     -0.00000,     -0.20333,      0.00000,      0.17400,     -0.17400],\n",
      "        [     0.00000,     -0.00000,     -0.20333,      0.00000,      0.17400,     -0.17400]])\n",
      "!!! G_SYM\n",
      " tensor([[[    -0.00000,     -0.00000,     -0.33583,      0.00000,      0.92218,     -0.92218],\n",
      "         [    -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000,      0.00000],\n",
      "         [    -0.33583,      0.00000,      0.00000,      3.03628,      1.08316,      1.08316],\n",
      "         [     0.00000,      0.00000,      3.03628,     -0.00000,     -2.25732,      2.25732],\n",
      "         [     0.92218,     -0.00000,      1.08316,     -2.25732,     -0.22215,      0.00000],\n",
      "         [    -0.92218,      0.00000,      1.08316,      2.25732,      0.00000,      0.22215]]], grad_fn=<StackBackward0>)\n",
      "indices\n",
      " tensor([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "        [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5]])\n",
      "indices[0]\n",
      " torch.Size([21])\n",
      "(eta_ao[indices[0], indices[1]] tensor([    -0.00000,      0.00000,     -0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000,     -0.00000,     -0.52545,      0.00000,      0.00000,     -0.00000,     -0.20333,\n",
      "             0.00000,      0.17400,      0.00000,     -0.00000,     -0.20333,      0.00000,      0.17400,     -0.17400])\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([     0.00000,     -0.06206,     -0.29743,     -0.00000,     -0.00000,     -9.72224,     -0.00000,      0.00000,      0.00000,      0.18844,     -0.56274,     -0.00000,     -0.00000,\n",
      "             0.60622,     -0.00000,      0.00000], grad_fn=<CopySlices>)\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([     0.00000,     -0.06206,     -0.29743,     -0.00000,     -0.00000,     -9.72224,     -0.00000,      0.00000,      0.00000,      0.18844,     -0.56274,     -0.00000,     -0.00000,\n",
      "             0.60622,     -0.00000,      0.00000], grad_fn=<CopySlices>)\n",
      "eta_ao.shape torch.Size([6, 6])\n",
      "tensor([[     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000],\n",
      "        [     0.00000,      0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000],\n",
      "        [     0.25782,      0.00000,      0.00000,      0.42770,     -0.41525,     -0.41525],\n",
      "        [    -0.00000,     -0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000],\n",
      "        [     0.15064,      0.00000,      0.00000,      0.24990,     -0.24262,     -0.24262],\n",
      "        [    -0.15064,     -0.00000,     -0.00000,     -0.24990,      0.24262,      0.24262]])\n",
      "!!! G_SYM\n",
      " tensor([[[     0.00000,     -0.00000,     -2.00670,      0.00000,     -0.70552,      0.70552],\n",
      "         [    -0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000],\n",
      "         [    -2.00670,     -0.00000,     -0.00000,     -2.62744,      2.10455,      2.10455],\n",
      "         [     0.00000,     -0.00000,     -2.62744,      0.00000,     -1.28516,      1.28516],\n",
      "         [    -0.70552,      0.00000,      2.10455,     -1.28516,      1.07507,     -0.00000],\n",
      "         [     0.70552,      0.00000,      2.10455,      1.28516,     -0.00000,     -1.07507]]], grad_fn=<StackBackward0>)\n",
      "indices\n",
      " tensor([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "        [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5]])\n",
      "indices[0]\n",
      " torch.Size([21])\n",
      "(eta_ao[indices[0], indices[1]] tensor([     0.00000,      0.00000,      0.00000,      0.25782,      0.00000,      0.00000,     -0.00000,     -0.00000,     -0.00000,     -0.00000,      0.15064,      0.00000,      0.00000,\n",
      "             0.24990,     -0.24262,     -0.15064,     -0.00000,     -0.00000,     -0.24990,      0.24262,      0.24262])\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([    -0.00000,      0.56971,    -10.10270,      0.00000,     -0.00000,     -0.29743,      0.00000,     -0.00000,      0.00000,     -0.60755,      0.57172,      0.00000,      0.00000,\n",
      "            -0.56274,      0.00000,     -0.00000], grad_fn=<CopySlices>)\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([    -0.00000,      0.56971,    -10.10270,      0.00000,     -0.00000,     -0.29743,      0.00000,     -0.00000,      0.00000,     -0.60755,      0.57172,      0.00000,      0.00000,\n",
      "            -0.56274,      0.00000,     -0.00000], grad_fn=<CopySlices>)\n",
      "eta_ao.shape torch.Size([6, 6])\n",
      "tensor([[     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.00000,      0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.49103,      0.00000,      0.42020,     -0.42020],\n",
      "        [    -0.00000,      0.00000,      0.00000,     -0.00000,     -0.00000,      0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.28690,      0.00000,      0.24552,     -0.24552],\n",
      "        [    -0.00000,      0.00000,      0.28690,     -0.00000,     -0.24552,      0.24552]])\n",
      "!!! G_SYM\n",
      " tensor([[[     0.33626,     -0.00000,     -0.00000,      0.68884,     -0.05956,     -0.05956],\n",
      "         [    -0.00000,     -1.85907,      0.00000,     -0.00000,     -0.00000,      0.00000],\n",
      "         [    -0.00000,      0.00000,      3.79661,     -0.00000,     -0.71270,      0.71270],\n",
      "         [     0.68884,     -0.00000,     -0.00000,     -1.56146,     -0.02573,     -0.02573],\n",
      "         [    -0.05956,     -0.00000,     -0.71270,     -0.02573,     -2.94929,      1.95984],\n",
      "         [    -0.05956,      0.00000,      0.71270,     -0.02573,      1.95984,     -2.94929]]], grad_fn=<StackBackward0>)\n",
      "indices\n",
      " tensor([[0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5],\n",
      "        [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5]])\n",
      "indices[0]\n",
      " torch.Size([21])\n",
      "(eta_ao[indices[0], indices[1]] tensor([     0.00000,      0.00000,     -0.00000,      0.00000,     -0.00000,     -0.49103,     -0.00000,      0.00000,      0.00000,     -0.00000,      0.00000,     -0.00000,     -0.28690,\n",
      "             0.00000,      0.24552,     -0.00000,      0.00000,      0.28690,     -0.00000,     -0.24552,      0.24552])\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([     1.18730,     -0.00000,      0.00000,     -9.81131,     -0.25209,     -0.00000,     -0.00000,      0.00000,      0.31884,     -0.00000,      0.00000,      0.88163,     -0.49472,\n",
      "            -0.00000,     -0.00000,      0.00000], grad_fn=<CopySlices>)\n",
      "G_mo.shape torch.Size([16])\n",
      "G_mo\n",
      " tensor([     1.18730,     -0.00000,      0.00000,     -9.81131,     -0.25209,     -0.00000,     -0.00000,      0.00000,      0.31884,     -0.00000,      0.00000,      0.88163,     -0.49472,\n",
      "            -0.00000,     -0.00000,      0.00000], grad_fn=<CopySlices>)\n",
      "**** AO_full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 88 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([-10.58943, -10.40985, -10.26553,  -9.94604,  -9.55941,  -9.33964], grad_fn=<SortBackward0>)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!! Lxi tensor([[    -0.00000,      0.00000,     -0.66552,      0.00000,     -0.00000,      1.18730],\n",
      "        [     0.00000,      0.00000,      0.00000,     -0.06206,      0.56971,     -0.00000],\n",
      "        [     0.00000,     -0.00000,     -0.00000,     -0.29743,    -10.10270,      0.00000],\n",
      "        [    -0.00000,      0.00000,     -0.25209,     -0.00000,      0.00000,     -9.81131],\n",
      "        [     0.00000,      0.00000,     -9.47437,     -0.00000,     -0.00000,     -0.25209],\n",
      "        [    -0.00000,      0.00000,     -0.00000,     -9.72224,     -0.29743,     -0.00000],\n",
      "        [   -10.40985,      0.00000,      0.00000,     -0.00000,      0.00000,     -0.00000],\n",
      "        [     0.00000,    -10.58943,      0.00000,      0.00000,     -0.00000,      0.00000],\n",
      "        [    -0.00000,      0.00000,     -0.09729,      0.00000,      0.00000,      0.31884],\n",
      "        [    -0.00000,      0.00000,     -0.00000,      0.18844,     -0.60755,     -0.00000],\n",
      "        [     0.00000,     -0.00000,      0.00000,     -0.56274,      0.57172,      0.00000],\n",
      "        [    -0.00000,      0.00000,     -0.49472,     -0.00000,      0.00000,      0.88163],\n",
      "        [     0.00000,      0.00000,      0.79536,     -0.00000,      0.00000,     -0.49472],\n",
      "        [    -0.00000,      0.00000,     -0.00000,      0.60622,     -0.56274,     -0.00000],\n",
      "        [     0.25850,     -0.00000,      0.00000,     -0.00000,      0.00000,     -0.00000],\n",
      "        [    -0.00000,      0.25317,      0.00000,      0.00000,     -0.00000,      0.00000]], grad_fn=<CopySlices>)\n",
      "!!!!! Lxi.shape torch.Size([16, 6])\n",
      "============================\n",
      "all residuals are below tolerance\n",
      "DAVIDSON ALGORITHM CONVERGED\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<module> : 21 : DEBUG : \u001b[1m\u001b[32mFINAL eval \u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([-10.58943, -10.40985, -10.26553,  -9.94604,  -9.55941,  -9.33964], grad_fn=<SortBackward0>)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# mol = run_seqm_1mol('c6h6.xyz')\n",
    "# eval, _ = davidson(mol = mol, \n",
    "#                    N_exc = 8,\n",
    "#                    keep_n = 4,\n",
    "#                    n_V_max = 50, \n",
    "#                    max_iter = 50, \n",
    "#                    tol = 1e-6)\n",
    "\n",
    "# logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))\n",
    "\n",
    "\n",
    "\n",
    "mol = run_seqm_1mol('h2o.xyz')\n",
    "eval, _ = davidson(mol = mol, \n",
    "                   N_exc = 3,\n",
    "                   keep_n = 2,\n",
    "                   n_V_max = 10, \n",
    "                   max_iter = 1, \n",
    "                   tol = 1e-6)\n",
    "\n",
    "logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 0 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([8, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 83 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 5.94858,  6.83013,  9.23851,  9.99968, 11.31665, 12.61708], grad_fn=<SortBackward0>)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "============================\n",
      "all residuals are below tolerance\n",
      "DAVIDSON ALGORITHM CONVERGED\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<module> : 9 : DEBUG : \u001b[1m\u001b[32mFINAL eval \u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 5.94858,  6.83013,  9.23851,  9.99968, 11.31665, 12.61708], grad_fn=<SortBackward0>)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mol = run_seqm_1mol('h2o.xyz')\n",
    "eval, _ = davidson(mol = mol, \n",
    "                   N_exc = 3,\n",
    "                   keep_n = 2,\n",
    "                   n_V_max = 10, \n",
    "                   max_iter = 1, \n",
    "                   tol = 1e-6)``\n",
    "\n",
    "logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 0 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([2016, 16])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 83 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 3.25112,  4.92655,  6.20802,  6.79970,  7.07863,  8.66758,  8.95925,  9.24793,  9.93129, 10.13820, 10.36589, 10.50795, 10.79023, 10.87011, 11.05884, 11.42119], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.1911475401674505\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.4265047354572447\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.1065795668172644\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.5174218600667273\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 1 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([2016, 20])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 83 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.98798,  4.19882,  5.51399,  5.71843,  7.07701,  7.69312,  8.11497,  8.41606,  8.75616,  9.07366,  9.22410,  9.24999,  9.93701, 10.13832, 10.36699, 10.53824, 10.79061, 10.87012, 11.05894,\n",
      "        11.42134], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.8888387546217283\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.6804388231327319\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.5731166772912049\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.7899746434709942\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 2 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([2016, 24])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 83 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.92476,  3.97280,  5.33772,  5.41498,  6.84709,  7.07438,  7.28745,  7.65707,  8.22130,  8.72957,  9.03844,  9.24799,  9.93699, 10.13830, 10.36632, 10.53653, 10.78955, 10.87012, 11.05893,\n",
      "        11.42084, 19.23102, 20.87658, 21.08550, 23.84905], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.7229372592009828\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.2663760653603175\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.3594527668736196\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.7606537497401995\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 3 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([2016, 28])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 83 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.88592,  3.84857,  5.03270,  5.10642,  6.24589,  6.70941,  6.90710,  7.05558,  7.72155,  8.71227,  9.01313,  9.24795,  9.92506, 10.13829, 10.33938, 10.53502, 10.63210, 10.85299, 10.87010,\n",
      "        11.05890, 11.42907, 12.90507, 13.01746, 13.34912, 31.25258, 32.25032, 32.82449, 34.22563], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.5138407390662986\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.941964986852698\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.9393394233338896\u001b[0m\n",
      "davidson : 114 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.0341324090455424\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 4 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([2016, 32])\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "mol = run_seqm_1mol('isoindigo.xyz')\n",
    "%lprun -u 1 -f davidson davidson(mol = mol, N_exc = 8, keep_n = 4, n_V_max = 60, max_iter = 100, tol = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
