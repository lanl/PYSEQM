{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decorating your function! <function KSA_XL_BOMD.one_step at 0x7f7a22ab7d90>\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# === IMPORTS ===\n",
    "\n",
    "import logging, sys\n",
    "import torch\n",
    "import seqm\n",
    "from ase.io import read as ase_read\n",
    "from seqm.seqm_functions.constants import Constants\n",
    "from seqm.Molecule import Molecule\n",
    "from seqm.ElectronicStructure import Electronic_Structure\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "from seqm.seqm_functions.fock import fock\n",
    "from seqm.seqm_functions.pack import unpack\n",
    "import seqm.seqm_functions.pack as pack\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#=== TORCH OPTIONS ===\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "# else:\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "# torch.set_printoptions(precision=5, linewidth=200, profile=\"full\", sci_mode=False)\n",
    "torch.set_printoptions(precision=5, linewidth=200, sci_mode=False, profile = 'short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2.xyz\t\t  examples\t nodes.html\t requirements.txt\n",
      "c60.xyz\t\t  h2o.xyz\t __pycache__\t seqm\n",
      "c6h6.xyz\t  isoindigo.xyz  pyseqm.dot\t setup.dot\n",
      "ch4.xyz\t\t  lib\t\t pyseqm.dot.png  setup.py\n",
      "coronene.xyz\t  LICENSE\t pyseqm.dot.svg  Test1_SinglePointProp_FNS.png\n",
      "davidson_0723.py  model.pt\t pyseqm.json\t test_module.py\n",
      "doc\t\t  My_d_combined  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colored logging with custom level QM for deeper routines\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(funcName)s : %(lineno)d : %(levelname)s : %(message)s')\n",
    "\n",
    "QM1 = evel=logging.DEBUG - 3 # informal level of depth; QM1 - almost always, usually outside of loops\n",
    "QM2 = evel=logging.DEBUG - 4 #                          QM2 - sometimes, in the loops\n",
    "QM3 = evel=logging.DEBUG - 5\n",
    "\n",
    "logging.addLevelName(QM1, \"QM1\")\n",
    "def qm1(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM1 ):\n",
    "        self._log(QM1, message, args, **kwargs) \n",
    "        \n",
    "logging.addLevelName(QM2, \"QM2\")\n",
    "def qm2(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM2):\n",
    "        self._log(QM2, message, args, **kwargs) \n",
    " \n",
    "logging.addLevelName(QM3, \"QM3\")\n",
    "def qm3(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM3 ):\n",
    "        self._log(QM3, message, args, **kwargs) \n",
    "           \n",
    "        \n",
    "logging.Logger.qm1 = qm1   \n",
    "logging.Logger.qm2 = qm2\n",
    "logging.Logger.qm3 = qm3\n",
    "  \n",
    "logger = logging.getLogger()\n",
    "\n",
    "                              \n",
    "colors = {'qm'        : ('cyan',     None, None),\n",
    "          'matrix'    : ('blue',     None, ['bold']),\n",
    "          'vector'    : ('yellow',   None, ['bold']),\n",
    "          'evals'     : ('green',    None, ['bold']),\n",
    "          'warn'     : ('red',    None, ['bold'])\n",
    "          }\n",
    "\n",
    "def fmt_log(data, message, fmt):\n",
    "    \"\"\"\n",
    "    fmt_log : formats log message with color and style using termcolor module\n",
    "\n",
    "    Args:\n",
    "        data (any): data to print\n",
    "        message (str or None): message to print, pass None if no message is needed\n",
    "        fmt (str): style from colors dict\n",
    "\n",
    "    Returns:\n",
    "        str: formatted string with color and style\n",
    "    \"\"\"    \n",
    "\n",
    "    if type(data) is list or type(data) is tuple or type(data) is torch.Tensor:\n",
    "        \n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])}\\n' # add new line to align array\n",
    "    else:\n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])} : '\n",
    "        \n",
    "    if data == None:\n",
    "        return mes\n",
    "    else:\n",
    "        return mes + str(colored(data, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log\n",
    "\n",
    "07/13/23 - QM part seems to be wortking fine\n",
    "full diagonalization agrees with NEXMD\n",
    "small guess space misses relevant vectors, but large guess includes them\n",
    "\n",
    "\n",
    "PASCAL 1 COULD BE INCORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seqm_1mol(xyz):\n",
    "    \"\"\"\n",
    "    run_seqm_1mol : run PYSEQM for a single molecule\n",
    "\n",
    "    Args:\n",
    "        xyz (str): path to xyz file\n",
    "\n",
    "    Returns:\n",
    "        Molecule object: PYSEQM object with molecule data\n",
    "    \"\"\"    \n",
    "    \n",
    "    atoms = ase_read(xyz)\n",
    "    species = torch.tensor([atoms.get_atomic_numbers()], dtype=torch.long, device=device)\n",
    "    coordinates = torch.tensor([atoms.get_positions()], dtype=dtype, device=device)\n",
    "    \n",
    "    const = Constants().to(device)\n",
    "\n",
    "    elements = [0]+sorted(set(species.reshape(-1).tolist()))\n",
    "\n",
    "    seqm_parameters = {\n",
    "                    'method' : 'PM3',  # AM1, MNDO, PM#\n",
    "                    'scf_eps' : 1.0e-6,  # unit eV, change of electric energy, as nuclear energy doesnt' change during SCF\n",
    "                    'scf_converger' : [2,0.0], # converger used for scf loop\n",
    "                                            # [0, 0.1], [0, alpha] constant mixing, P = alpha*P + (1.0-alpha)*Pnew\n",
    "                                            # [1], adaptive mixing\n",
    "                                            # [2], adaptive mixing, then pulay\n",
    "                    'sp2' : [False, 1.0e-5],  # whether to use sp2 algorithm in scf loop,\n",
    "                                                #[True, eps] or [False], eps for SP2 conve criteria\n",
    "                    'elements' : elements, #[0,1,6,8],\n",
    "                    'learned' : [], # learned parameters name list, e.g ['U_ss']\n",
    "                    #'parameter_file_dir' : '../seqm/params/', # file directory for other required parameters\n",
    "                    'pair_outer_cutoff' : 1.0e10, # consistent with the unit on coordinates\n",
    "                    'eig' : True,\n",
    "                    'excited' : True,\n",
    "                    }\n",
    "\n",
    "    mol = seqm.Molecule.Molecule(const, seqm_parameters, coordinates, species).to(device)\n",
    "\n",
    "    ### Create electronic structure driver:\n",
    "    esdriver = Electronic_Structure(seqm_parameters).to(device)\n",
    "\n",
    "    ### Run esdriver on m:\n",
    "    esdriver(mol)\n",
    "    \n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_L_xi(vexp1, molecule, N_cis, N_rpa, CIS = True):\n",
    "    \"\"\"\n",
    "    form_L_xi: build A matrix for CIS\n",
    "               splits guess density into symmetric and antisymmetric parts\n",
    "               unclear why returns A @ b (guess vector)\n",
    "               see NEXMD code for QM details\n",
    "               #! RPA is not implemented yet\n",
    "               \n",
    "    Args:\n",
    "        vexp1 (tensor): guess vector\n",
    "        molecule (PYSEQM object): _description_\n",
    "        N_cis (int): dimension of CIS space, nocch*nvirt\n",
    "        N_rpa (int): N_cis *2\n",
    "        CIS (bool, optional): CIS or TDHF (RPA) Defaults to True.\n",
    "\n",
    "    \"\"\"        \n",
    "\n",
    "    m = molecule\n",
    "    gss = m.parameters['g_ss']\n",
    "    gsp = m.parameters['g_sp']\n",
    "    gpp = m.parameters['g_pp']\n",
    "    gp2 = m.parameters['g_p2']\n",
    "    hsp = m.parameters['h_sp']\n",
    "    \n",
    "    mask  = m.mask\n",
    "    maskd = m.maskd\n",
    "    idxi  = m.idxi\n",
    "    idxj  = m.idxj\n",
    "    nmol  = m.nmol\n",
    "    molsize = m.molsize\n",
    "    w       = m.w\n",
    "    nHeavy = m.nHeavy\n",
    "    nHydro = m.nHydro\n",
    "    \n",
    "    eta = torch.zeros((N_rpa), device=device) \n",
    "    \n",
    "    #print('vexp1.shape', vexp1.shape)\n",
    "   # print('eta.shape', eta.shape)\n",
    "    eta[:vexp1.size(0)] = vexp1 # eta is stored as |X|; dcopy?\n",
    "    \n",
    "    eta_orig = torch.clone(eta)\n",
    "    # print('eta_orig.shape', eta_orig.shape)\n",
    "    # print('eta_orig\\n', eta_orig)\n",
    "    \n",
    "   # print('m.C MO', m.C_mo[0])\n",
    "    # print('eta.shape', eta.shape)\n",
    "    # print(eta)\n",
    "    eta_ao =  mo2ao(N_cis, eta, m, full=False)     # mo to ao basis (mo2site)\n",
    "    # print('eta_ao.shape', eta_ao.shape)\n",
    "    # print(eta_ao)\n",
    "\n",
    "    eta_ao_sym, eta_ao_asym = decompose_to_sym_antisym(eta_ao) # decompose to sym and asym\n",
    "\n",
    "    # Vxi - build 2e integrals in AO basis: G(guess density) in F = H_core + G\n",
    "    # note density is split into sym and anisym matrices\n",
    "    # sym is processed as padded 3d array in PYSEQM, see fock module \n",
    "    # antisym: 2c-2e works with modified PYSEQM routine; should be antisimmterized afterwards\n",
    "    # antisym: 1c-2e (diagonal) are taken from NEXMD for now - ugly code with loops\n",
    "    # TODO: vectorize 1c-2e part\n",
    "    \n",
    "    #------------------symmetric------------------------------\n",
    "    G_sym   =  build_G_sym(eta_ao_sym,\n",
    "                        gss, gsp, gpp, gp2, hsp,\n",
    "                        mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                        w,\n",
    "                         nHeavy,\n",
    "                         nHydro)\n",
    "    \n",
    "   # print('G_SYM\\n', G_sym)\n",
    "    # G sym is 1c-2e and 2c-2e of symmetric part of guess density\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pack 2c-2e part to standard shape\n",
    "    G_sym = pack.pack(G_sym, nHeavy, nHydro)\n",
    "    # print('G_sym \\n', G_sym)\n",
    "    \n",
    "    G_tot = build_G_antisym_vect(eta_ao, eta_ao_asym, G_sym,\n",
    "                            gss, gsp, gpp, gp2, hsp,\n",
    "                            mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                            w, \n",
    "                            m,\n",
    "                            nHeavy,\n",
    "                            nHydro)\n",
    "    \n",
    "   # print('G_tot.shape', G_tot.shape)\n",
    "   # print('G_tot\\n', G_tot)                  \n",
    "    # build_G_antisym returns both sym and antisym!\n",
    "    # TODO: refactor into: 2c-2e antisym, 1c-2e antisym\n",
    "    # TODO: vectorize 1c-2e antisym, avoid ugly loops\n",
    "    #! remember about making 2c-2e diagonal 0\n",
    "\n",
    "    # print('G total \\n', G_tot)\n",
    "    \n",
    "    # print('============================================')\n",
    "    # print('Converting Gao full back into MO basis')\n",
    "    G_mo = ao2mo(N_cis, N_rpa, m, G_tot[0], m.C_mo, full=False) # G in MO basis #! [0] not batched yet\n",
    "    # print('G_mo.shape', G_mo.shape)\n",
    "    # print('G_mo\\n', G_mo)\n",
    "    \n",
    "    # multiply by MO differencies\n",
    "\n",
    "    return G_mo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_to_sym_antisym(A):\n",
    "    \"\"\"\n",
    "    decomposes matrix into symmetric and antisymmetric parts\n",
    "\n",
    "    Args:\n",
    "        A (tensor): some matrix\n",
    "\n",
    "    Returns:\n",
    "        tuple of tensors: sym and antisym parts\n",
    "    \"\"\"   \n",
    "\n",
    "    A_sym = 0.5 * (A + A.T)\n",
    "    A_antisym = 0.5 * (A - A.T)\n",
    "    \n",
    "    return A_sym, A_antisym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_by_mo_e(N_cis, mol, G_mo, eta_orig):\n",
    "    \n",
    "    # print('eta ORIG SLOW', eta_orig)\n",
    "    m = mol\n",
    "    ii=0\n",
    "    # kron = []\n",
    "    for p in range(m.nocc):\n",
    "        # print('p', p)\n",
    "        for h in range(m.nocc, m.norb):\n",
    "            # print('h', h)\n",
    "            # print('i', i)\n",
    "            f = m.e_mo[0][h] - m.e_mo[0][p]\n",
    "            # print('f SLOW', f)\n",
    "            G_mo[ii] = G_mo[ii] + f * eta_orig[ii]\n",
    "            # kron.append(f * eta_orig[ii])\n",
    "            # G_mo[ii+N_cis] = -G_mo[ii + N_cis] + f * eta_orig[ii+N_cis]\n",
    "            ii += 1\n",
    "    # print('kron', kron)\n",
    "    # print('G_mo AFTER SLOW', G_mo)\n",
    "    return G_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! seems that G_mo is a column of A matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_by_mo_e_vect(N_cis, mol, G_mo, eta_orig):\n",
    "    \n",
    "    # print('eta ORIG FASR', eta_orig)\n",
    "    # print('G_mo\\n', G_mo)\n",
    "    m = mol\n",
    "\n",
    "\n",
    "    occ_idx = torch.arange(int(m.nocc))\n",
    "    virt_idx = torch.arange(int(m.nocc), int(m.norb))\n",
    "\n",
    "    # print('occ_idx\\n', occ_idx)\n",
    "    # print('virt_idx\\n', virt_idx)\n",
    "    \n",
    "    combined_idx = torch.cartesian_prod(occ_idx, virt_idx)  # combinations similar to itertools\n",
    "    # print('combined_idx\\n', combined_idx)\n",
    "   # print(combined_idx[:, 1])\n",
    "    \n",
    "    mo_diff = m.e_mo[0][combined_idx[:, 1]] - m.e_mo[0][combined_idx[:, 0]]  # difference between virtual and occupied\n",
    "    # print('mo diff', mo_diff)                                                                         # see how elements of A matrix are defined in any CIS paper\n",
    "                                                                             # Aia,jb=δijδab(ϵa−ϵi)+⟨aj||ib⟩\n",
    "    mo_kronecker = torch.zeros((N_cis * 2), dtype=torch.float64)\n",
    "    mo_kronecker[:N_cis] = (mo_diff * eta_orig) \n",
    "    \n",
    "\n",
    "    G_mo += mo_kronecker \n",
    "    # print('mo_kronecker\\n', mo_kronecker)\n",
    "    # print('G_mo AFTER FAST', G_mo)\n",
    "    return G_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_G_sym(M_ao,\n",
    "                gss, gsp, gpp, gp2, hsp,\n",
    "                mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                w,\n",
    "                nHydro,\n",
    "                nHeavy):\n",
    "    \n",
    "    \n",
    "      F = torch.zeros((nmol*molsize**2,4,4), device=device) # 0 Fock matrix to fill\n",
    "      # # TODO: feed params programmatically\n",
    "      \n",
    "      P0 = unpack(M_ao, nHydro, nHeavy, (nHeavy+nHydro)*4) # \n",
    "      P0 = torch.unsqueeze(P0, 0) # add dimension\n",
    "      \n",
    "      # print('P0.shape', P0.shape)\n",
    "      # print('P0\\n', P0)\n",
    "      #---------------fill diagonal 1c-2e -------------------\n",
    "      P = P0.reshape((nmol,molsize,4,molsize,4)) \\\n",
    "          .transpose(2,3).reshape(nmol*molsize*molsize,4,4)\n",
    "          \n",
    "      # print('P.shape', P.shape)\n",
    "      # print('P\\n', P)\n",
    "      \n",
    "      Pptot = P[...,1,1]+P[...,2,2]+P[...,3,3]\n",
    "      ## http://openmopac.net/manual/1c2e.html\n",
    "    #  (s,s)\n",
    "      TMP = torch.zeros_like(F)\n",
    "      TMP[maskd,0,0] = 0.5*P[maskd,0,0]*gss + Pptot[maskd]*(gsp-0.5*hsp)\n",
    "      for i in range(1,4):\n",
    "          #(p,p)\n",
    "          TMP[maskd,i,i] = P[maskd,0,0]*(gsp-0.5*hsp) + 0.5*P[maskd,i,i]*gpp \\\n",
    "                          + (Pptot[maskd] - P[maskd,i,i]) * (1.25*gp2-0.25*gpp)\n",
    "          #(s,p) = (p,s) upper triangle\n",
    "          TMP[maskd,0,i] = P[maskd,0,i]*(1.5*hsp - 0.5*gsp)\n",
    "      #(p,p*)\n",
    "      for i,j in [(1,2),(1,3),(2,3)]:\n",
    "          TMP[maskd,i,j] = P[maskd,i,j]* (0.75*gpp - 1.25*gp2)\n",
    "\n",
    "      F.add_(TMP)\n",
    "      \n",
    "           \n",
    "      #-----------------fill 2c-2e integrals----------------\n",
    "      weight = torch.tensor([1.0,\n",
    "                        2.0, 1.0,\n",
    "                        2.0, 2.0, 1.0,\n",
    "                        2.0, 2.0, 2.0, 1.0],dtype=dtype, device=device).reshape((-1,10))\n",
    "      \n",
    "      PA = (P[maskd[idxi]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,10,1))\n",
    "      PB = (P[maskd[idxj]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,1,10))\n",
    "      suma = torch.sum(PA*w,dim=1)\n",
    "      sumb = torch.sum(PB*w,dim=2)\n",
    "      sumA = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      sumB = torch.zeros_like(sumA)\n",
    "      \n",
    "      sumA[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = suma\n",
    "      sumB[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = sumb\n",
    "      F.index_add_(0,maskd[idxi],sumB)\n",
    "      #\\sum_A\n",
    "      F.index_add_(0,maskd[idxj],sumA)\n",
    "      \n",
    "      sum = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      # (ss ), (px s), (px px), (py s), (py px), (py py), (pz s), (pz px), (pz py), (pz pz)\n",
    "      #   0,     1         2       3       4         5       6      7         8        9\n",
    "\n",
    "      ind = torch.tensor([[0,1,3,6],\n",
    "                          [1,2,4,7],\n",
    "                          [3,4,5,8],\n",
    "                          [6,7,8,9]],dtype=torch.int64, device=device)\n",
    "      \n",
    "      Pp = -0.5*P[mask]\n",
    "      for i in range(4):\n",
    "        for j in range(4):\n",
    "            #\\sum_{nu \\in A} \\sum_{sigma \\in B} P_{nu, sigma} * (mu nu, lambda, sigma)\n",
    "            sum[...,i,j] = torch.sum(Pp*w[...,ind[i],:][...,:,ind[j]],dim=(1,2))\n",
    "      #print('mask', mask)    #! DIFFERS FROM PYSEQM, PROBABLY packing\n",
    "      F.index_add_(0,mask,sum)\n",
    "\n",
    "      F0 = F.reshape(nmol,molsize,molsize,4,4) \\\n",
    "             .transpose(2,3) \\\n",
    "             .reshape(nmol, 4*molsize, 4*molsize)\n",
    "    #\n",
    "      F0.add_(F0.triu(1).transpose(1,2))     \n",
    "      \n",
    "      F0 = 2 * F0 #! BE CAREFUL\n",
    "      # print('F0.shape', F0.shape)\n",
    "      # print(F0) \n",
    "      \n",
    "      return F0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_G_antisym_vect(eta_ao, eta_ao_asym, G_sym,\n",
    "                gss, gsp, gpp, gp2, hsp,\n",
    "                mask, maskd, idxi, idxj, nmol, molsize,\n",
    "                w, \n",
    "                m,\n",
    "                nHydro,\n",
    "                nHeavy):\n",
    "\n",
    "      # TODO; figure how/why constants are defined in fock_skew\n",
    "      #!\n",
    "      #! CHECK\n",
    "      #!\n",
    "      \n",
    "      # print('G SYM\\n', G_sym)\n",
    "      \n",
    "      eta_anti = torch.zeros((m.norb*(m.norb+1)//2), device=device)\n",
    "      indices = torch.tril_indices(int(m.norb), int(m.norb), offset = 0)\n",
    "      eta_anti = 0.5 * (eta_ao[indices[0], indices[1]] - eta_ao[indices[1], indices[0]])\n",
    "      eta_anti_2d = torch.zeros((m.norb, m.norb), device='cpu') \n",
    "      eta_anti_2d[indices[1], indices[0]] = -eta_anti \n",
    "      eta_anti_2d = eta_anti_2d - eta_anti_2d.T\n",
    "      \n",
    "      # ===== VECTORIZED ============\n",
    "      F = torch.zeros((nmol*molsize**2,4,4), device=device) # 0 Fock matrix to fill\n",
    "      # # TODO: feed params programmatically\n",
    "      \n",
    "      P0 = unpack(eta_anti_2d, nHydro, nHeavy, (nHeavy+nHydro)*4) # \n",
    "    #   print('P0.shape', P0.shape)\n",
    "    #   print('P0\\n', P0)\n",
    "      \n",
    "      P0 = torch.unsqueeze(P0, 0) # add dimension\n",
    "      \n",
    "      # print('P0.shape', P0.shape)\n",
    "      # print('P0\\n', P0)\n",
    "      #---------------fill diagonal 1c-2e -------------------\n",
    "      P = P0.reshape((nmol,molsize,4,molsize,4)) \\\n",
    "          .transpose(2,3).reshape(nmol*molsize*molsize,4,4)\n",
    "          \n",
    "      Pptot = P[...,1,1]+P[...,2,2]+P[...,3,3]\n",
    "      \n",
    "      TMP = torch.zeros_like(F)\n",
    "      \n",
    "      #! MODIFIED BY FNS\n",
    "      for i in range(1,4):\n",
    "          #(p,p)\n",
    "          # ! TWO LINES BELOW COULD BE NEEDED, NOT SURE\n",
    "          # TMP[maskd,i,i] = P[maskd,0,0]*(gsp-hsp) + \\\n",
    "          #                 + (Pptot[maskd] - P[maskd,i,i]) * (0.6*1.25*gp2-0.25*gpp)\n",
    "        #   (s,p) = (p,s) upper triangle\n",
    "          # TMP[maskd,i,i] = 100\n",
    "          TMP[maskd,0,i] = P[maskd,0,i]*(hsp - gsp)\n",
    "      #(p,p*)\n",
    "      for i,j in [(1,2),(1,3),(2,3)]:\n",
    "          TMP[maskd,i,j] = 2*P[maskd,i,j]* (0.25*gpp - 0.6*1.25*gp2)\n",
    "\n",
    "    #   print('*** TMP *** \\n', TMP)\n",
    "     # ! MAYBE SHOULD be ANTISYMMETRIZD TMP = \n",
    "      # TMP = 0.5* ( TMP - TMP.T)\n",
    "      # F.add_(TMP)  \n",
    "          \n",
    "      TMP = TMP.reshape(nmol,molsize,molsize,4,4) \\\n",
    "             .transpose(2,3) \\\n",
    "             .reshape(nmol, 4*molsize, 4*molsize)\n",
    "             \n",
    "      TMP = pack.pack(TMP, m.nHeavy, m.nHydro)\n",
    "      TMP = TMP - TMP.transpose(1,2)\n",
    "      # print('TMP.shape', TMP.shape)\n",
    "      # print('TMP\\n', TMP)\n",
    "\n",
    "      # build 2c-2e part of antisymmetric G\n",
    "      # copied from FOCK\n",
    "      \n",
    "      #P = P[...,1,1]+P[...,2,2]+P[...,3,3] #! MODIFIED\n",
    "      #-----------------fill 2c-2e integrals----------------\n",
    "      weight = torch.tensor([1.0,\n",
    "                        2.0, 1.0,\n",
    "                        2.0, 2.0, 1.0,\n",
    "                        2.0, 2.0, 2.0, 1.0],dtype=dtype, device=device).reshape((-1,10))\n",
    "      \n",
    "      PA = (P[maskd[idxi]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,10,1))\n",
    "      PB = (P[maskd[idxj]][...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)]*weight).reshape((-1,1,10))\n",
    "      suma = torch.sum(PA*w,dim=1)\n",
    "      sumb = torch.sum(PB*w,dim=2)\n",
    "      sumA = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      sumB = torch.zeros_like(sumA)\n",
    "      \n",
    "      sumA[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = suma\n",
    "      sumB[...,(0,0,1,0,1,2,0,1,2,3),(0,1,1,2,2,2,3,3,3,3)] = sumb\n",
    "      F.index_add_(0,maskd[idxi],sumB)\n",
    "      #\\sum_A\n",
    "      F.index_add_(0,maskd[idxj],sumA)\n",
    "      \n",
    "      sum = torch.zeros(w.shape[0],4,4,dtype=dtype, device=device)\n",
    "      # (ss ), (px s), (px px), (py s), (py px), (py py), (pz s), (pz px), (pz py), (pz pz)\n",
    "      #   0,     1         2       3       4         5       6      7         8        9\n",
    "\n",
    "      ind = torch.tensor([[0,1,3,6],\n",
    "                          [1,2,4,7],\n",
    "                          [3,4,5,8],\n",
    "                          [6,7,8,9]],dtype=torch.int64, device=device)\n",
    "      \n",
    "      Pp = -0.5*P[mask]\n",
    "      for i in range(4):\n",
    "        for j in range(4):\n",
    "            #\\sum_{nu \\in A} \\sum_{sigma \\in B} P_{nu, sigma} * (mu nu, lambda, sigma)\n",
    "            sum[...,i,j] = torch.sum(Pp*w[...,ind[i],:][...,:,ind[j]],dim=(1,2))\n",
    "     # print('mask', mask)    #! DIFFERS FROM PYSEQM, PROBABLY packing\n",
    "      F.index_add_(0,mask,sum)\n",
    "\n",
    "      F0 = F.reshape(nmol,molsize,molsize,4,4) \\\n",
    "             .transpose(2,3) \\\n",
    "             .reshape(nmol, 4*molsize, 4*molsize)\n",
    "    #\n",
    "\n",
    "\n",
    "      F0.add_(F0.triu(1).transpose(1,2))     \n",
    "      \n",
    "      rows, cols = torch.tril_indices(F0.shape[1], F0.shape[2])\n",
    "      F0[0][rows, cols] *= -1\n",
    "      F0[0][torch.eye(F0.shape[1]).bool()] *= -1\n",
    "      \n",
    "      # F0 is still symmetric, probably symmetrized above\n",
    "      # here we make it antisymmetric back\n",
    "    #   F0 = 2 * F0 \n",
    "      \n",
    "\n",
    "       #! BE WARNED, THIS iS TAKEN FROM OLD NEXMD, PYSEQM produces non-zero diagonal\n",
    "    #   F0 = F0\n",
    "    #  print('G ANTISYM shape', F0.shape)\n",
    "    #  print('G ANTISYM\\n', F0*2)\n",
    "    \n",
    "    # BEFORE FINAL ASSEMBLY\n",
    "     \n",
    "      F0 = pack.pack(F0, m.nHeavy, m.nHydro)\n",
    "      F0[0].diagonal().fill_(0)\n",
    "      \n",
    "      # print('G ANTISYM\\n', F0*2)\n",
    "      G_sym = torch.unsqueeze(G_sym, 0)\n",
    "      G_full = G_sym +  F0*2 + TMP# summ of G_sym(sym 1c2e + 2c2e) + F0 (antisym 1c2e + 2c2e) \n",
    "      # print('G_full shape', G_full.shape)\n",
    "      # print('G_full\\n', G_full)\n",
    "      # print('G_full\\n', G_full)\n",
    "      return G_full[0] # ! WORKS FOR ONE ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUX routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize_torch(U, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Orthogonalizes the matrix U (d x n) using Gram-Schmidt Orthogonalization.\n",
    "    If the columns of U are linearly dependent with rank(U) = r, the last n-r columns \n",
    "    will be 0.\n",
    "    \n",
    "    Args:\n",
    "        U (numpy.array): A d x n matrix with columns that need to be orthogonalized.\n",
    "        eps (float): Threshold value below which numbers are regarded as 0 (default=1e-15).\n",
    "    \n",
    "    Returns:\n",
    "        (numpy.array): A d x n orthogonal matrix. If the input matrix U's cols were\n",
    "            not linearly independent, then the last n-r cols are zeros.\n",
    "    \n",
    "    Examples:\n",
    "    ```python\n",
    "    >>> import numpy as np\n",
    "    >>> import gram_schmidt as gs\n",
    "    >>> gs.orthogonalize(np.array([[10., 3.], [7., 8.]]))\n",
    "    array([[ 0.81923192, -0.57346234],\n",
    "       [ 0.57346234,  0.81923192]])\n",
    "    >>> gs.orthogonalize(np.array([[10., 3., 4., 8.], [7., 8., 6., 1.]]))\n",
    "    array([[ 0.81923192 -0.57346234  0.          0.        ]\n",
    "       [ 0.57346234  0.81923192  0.          0.        ]])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(U[0])\n",
    "    # numpy can readily reference rows using indices, but referencing full rows is a little\n",
    "    # dirty. So, work with transpose(U)\n",
    "    V = U.T\n",
    "    for i in range(n):\n",
    "        prev_basis = V[0:i]     # orthonormal basis before V[i]\n",
    "        coeff_vec = prev_basis @ V[i].T  # each entry is np.dot(V[j], V[i]) for all j < i\n",
    "        # subtract projections of V[i] onto already determined basis V[0:i]\n",
    "        V[i] -= (coeff_vec @ prev_basis).T\n",
    "        if torch.norm(V[i]) < eps:\n",
    "            V[i][V[i] < eps] = 0.   # set the small entries to 0\n",
    "        else:\n",
    "            V[i] /= torch.norm(V[i])\n",
    "    return V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_V(N_cis, N_rpa, n_V_start):\n",
    "    \n",
    "    # returns vexp1 - guess vector for L-xi routine\n",
    "    logger.qm2(fmt_log(n_V_start, 'n_V_start', 'qm'))\n",
    "    rrwork = torch.zeros(N_rpa * 4, device=device)\n",
    "    i = 0\n",
    "    for ip in range(mol.nocc):\n",
    "        for ih in range(mol.nvirt):\n",
    "            rrwork[i] = mol.e_mo[0][mol.nocc + ih] - mol.e_mo[0][ip]  # !Lancos vectors(i) ???\n",
    "            i += 1                                                                      #  TODO: [0] should be replaced by m batch index\n",
    "                                                                                    \n",
    "    rrwork_sorted, indices = torch.sort(rrwork[:N_cis], descending=False, stable=True) # preserve order of degenerate\n",
    "    logger.qm2(fmt_log(rrwork_sorted, 'rrwork_sorted', 'qm'))\n",
    "\n",
    "\n",
    "    # vexp1 = torch.zeros((N_cis, N_cis), device=device)\n",
    "    vexp1 = torch.zeros((N_cis, N_cis), device=device)\n",
    "    \n",
    "    row_idx = torch.arange(0, int(N_cis), device=device)\n",
    "    col_idx = indices[:N_cis]\n",
    "\n",
    "    vexp1[row_idx, col_idx] = 1.0 \n",
    "    logger.qm2(fmt_log(vexp1, 'V  BEFORE SELECTING PART', 'qm'))\n",
    "    logger.qm2(fmt_log(vexp1.shape, 'V shape', 'qm'))\n",
    "   #! THIS IS NEW, TAKE only part \n",
    "\n",
    "    V = vexp1[:,  :n_V_start]\n",
    "    logger.qm2(fmt_log(V, 'V = vexp1', 'qm'))\n",
    "    logger.qm2(fmt_log(V.shape, 'V shape', 'qm'))\n",
    "\n",
    "    return V\n",
    "\n",
    "# TODO: check whether L_xi should be regenerated during expansion each time or just part of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAVIDSON routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)  # custom logging level; lower than DEBUG\n",
    "                               # printed above QM (QM, DEBUG, INFO, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def davidson(mol, N_exc, keep_n, n_V_max,  max_iter, tol):\n",
    "    \"\"\"\n",
    "    Davidson algorithm for solving eigenvalue problem of large sparse diagonally dominant matrices\n",
    "    Hamiltonian is not generated or stored explicitly, only matrix-vector products are used on-the fly:\n",
    "    guess space V should be orthogonalized at each iteration\n",
    "    M (projection of smaller size) is V.T @ H @ V \n",
    "    #! RPA (TDHF) is not implemented yet, non-Hermitian (non-symmetric), requires also left eigenvectors \n",
    "    note that notation differes between implementations: V.T x A x V is bAb\n",
    "    # TODO: 1) check if convergence of e_vals is needed\n",
    "    # TODO: 2) vectorize and optimize orthogonalization\n",
    "    # TODO: 3) check if some vectors should be dropped \n",
    "    # TODO: 4) eliminate loops \n",
    "    # TODO: 5) check if whole M should be regenerated, or only sub-blocks corresponding to new guess vectors\n",
    "    # TODO: 6) add parameter checker like Krylov dims << N_cis\n",
    "\n",
    "    Args:\n",
    "        mol (PYSEQM object): object to hold all qm data from PYSEQM\n",
    "        N_exc (int)        : number of excited states to calculate\n",
    "        keep_n (int)       : number of e_vals, e_vecs to keep at each iteration\n",
    "        n_V_max (int)      : maximum size of Krylov subspace, \n",
    "                             projected matrix will be no more than M(n_V_max x n_V_max)\n",
    "        max_iter (int)     : maximum number of iterations in Davidson\n",
    "        tol (float)        : treshold for residual\n",
    "        \n",
    "    Returns:\n",
    "        tuple of tensors: eigenvalues (excitation energies in default units, eV) and eigenvectors \n",
    "    \"\"\"    \n",
    "    \n",
    "    n_V_start = N_exc * 2 # dimension of Krylov subspace, analogue of nd1  \n",
    "    N_cis = mol.nocc * mol.nvirt\n",
    "    N_rpa = 2 * N_cis\n",
    "    \n",
    "    term = False  # terminate algorithm\n",
    "    iter = 0\n",
    "    L_xi = torch.zeros((N_rpa, n_V_start), device=device)\n",
    "    L_xi2 = torch.zeros((N_rpa, n_V_start), device=device)\n",
    "    V = gen_V(N_cis, N_rpa, n_V_start) # generate initial guess, V here #! should be renamed\n",
    "    diag = None # create diagonal of M only once\n",
    "    \n",
    "    while iter < max_iter and not term: # Davidson loop\n",
    "        \n",
    "        if iter > 0: # skip first step, as initial V is orthogonal\n",
    "            V = orthogonalize_torch(V)\n",
    "            \n",
    "        print('=================================', flush=True)\n",
    "        print(colored(f' ITERATION : {iter} ', 'red', 'on_white', attrs=['bold']), flush=True)\n",
    "        print('SUBSPACE SIZE V: ', V.shape, flush=True)\n",
    "        print('=================================')\n",
    "       \n",
    "        # ---------- form A x b product --------------------\n",
    "        L_xi = torch.zeros((N_rpa, V.shape[1] ), device=device) #! NOT iter here\n",
    "        logger.qm1(fmt_log(V, 'V BEFORE L_xi after ORTO', 'qm'))\n",
    "        for i in range(V.shape[1]): \n",
    "            # print('=================================', flush=True)\n",
    "            logger.qm3('Lxi iterations=%s', i)\n",
    "            L_xi[:,i] = form_L_xi(V[:,i], mol, N_cis, N_rpa)\n",
    "            # L_xi2[:,i] = torch.clone(L_xi[:,i])\n",
    "            \n",
    "            # L_xi[:,i]  = mult_by_mo_e(N_cis, mol, L_xi[:,i], V[:,i]) \n",
    "            L_xi[:,i] = mult_by_mo_e_vect(N_cis, mol, L_xi[:,i], V[:,i]) \n",
    "            # multiply by MO energies\n",
    "            \n",
    "            \n",
    "            \n",
    "            # raise ValueError('STOP')\n",
    "            logger.qm3(fmt_log(L_xi[:,i], 'L_xi[:,i]', 'qm'))\n",
    "        \n",
    "        L_xi[N_cis:, :] = L_xi[:N_cis] #! TODO: make sure that this A+B, A-B, not just copy for RPA\n",
    "    \n",
    "        right_V = L_xi[N_cis:] # (A)b \n",
    "        \n",
    "        logger.qm1(fmt_log(right_V.shape, 'right_V shape', 'matrix'))\n",
    "        logger.qm1(fmt_log(right_V, 'right_V', 'matrix'))       \n",
    "        # ---------- form b.T x Ab product --------------------\n",
    "        \n",
    "        M =  V.T @ right_V\n",
    "        \n",
    "        # logger.debug(fmt_log(M.shape, 'M shape', 'qm'))\n",
    "        # logger.debug(fmt_log(M, 'M', 'qm'))\n",
    "        if iter == 0:\n",
    "            diag = torch.diag(M) # create diagonal only once\n",
    "            \n",
    "        iter += 1\n",
    "        \n",
    "        logger.qm1(fmt_log(diag, 'diag', 'qm'))\n",
    "    \n",
    "        # ---------- diagonalize projection M --------------------\n",
    "        r_eval, r_evec = torch.linalg.eigh(M) # find eigenvalues and eigenvectors\n",
    "       \n",
    "        r_eval = r_eval.real\n",
    "        r_evec = r_evec.real\n",
    "        r_eval, r_idx = torch.sort(r_eval, descending=False) # sort eigenvalues in ascending order\n",
    "        logger.debug(fmt_log(r_eval, 'RIGHT EVALS', 'evals'))\n",
    "        r_evec = r_evec[:, r_idx] # sort eigenvectors accordingly\n",
    "    \n",
    "        e_val_n = r_eval[:keep_n] # keep only the lowest keep_n eigenvalues; full are still stored as e_val\n",
    "        e_vec_n = r_evec[:, :keep_n]\n",
    "        resids = torch.zeros(V.shape[0], len(e_val_n)) # account for left and right evecs\n",
    "\n",
    "        # ---------- calculate residual vectors --------------------\n",
    "        for j in range(len(e_val_n)): # calc residuals \n",
    "            resids[:,j] = right_V @ e_vec_n[:,j] - e_val_n[j] * (V @ e_vec_n[:,j])\n",
    "            \n",
    "       # logger.debug(fmt_log(resids, 'resids', 'matrix'))     \n",
    "        resids_norms_r = torch.tensor([resids[:,x].norm() for x in range(resids.shape[1])])\n",
    "\n",
    "        # ---------- expand guess space V buy not-converged resids --------------------\n",
    "        # !!! PROBABLY HIGHLY INEFFICIENT !!! \n",
    "        if torch.any(resids_norms_r > tol):\n",
    "            mask_r = resids_norms_r >= tol\n",
    "            large_res_r = resids[:,mask_r] # residuals larger than tol\n",
    "           # logger.debug(fmt_log(large_res_r, 'LARGE RESIDUALS', 'vector'))           \n",
    "            large_res_r.to(device)\n",
    "            cor_e_val_r = e_val_n[mask_r] # corresponding eigenvalues !!! check if matches\n",
    "            \n",
    "            # ------keep adding new resids --------------------\n",
    "            if V.shape[1] <= n_V_max:     \n",
    "\n",
    "                    for j in range(large_res_r.shape[1]):\n",
    "                        if V.shape[1] <= n_V_max:\n",
    "                            s = large_res_r[:,j] # conditioned residuals > tol\n",
    "\n",
    "                            if s.norm() >= tol:\n",
    "                                logger.debug(fmt_log((s.norm().item()), 'NORM of RESIDUAL', 'warn'))\n",
    "                                denom = (diag[j] - cor_e_val_r[j])\n",
    "                                denom.to(device) \n",
    "                                s = s/denom # conditioned residuals\n",
    "                                s.to(device)\n",
    "                                # logger.debug(fmt_log(s.norm(), 'NORM OF NEW RESIDUAAL', 'vector'))\n",
    "                                V = torch.column_stack((V, s/s.norm()))\n",
    "                            else:\n",
    "                                pass\n",
    "            # ------ collapse (restart) if space V is too large; mix eigenvectors with V------------\n",
    "            else:\n",
    "                logger.debug(fmt_log(None, '!!!! MAX subspace reached !!!!', 'warn'))\n",
    "                #logger.debug(fmt_log(V, 'V before collapse', 'qm'))\n",
    "\n",
    "                V =  V @ r_evec[:, :n_V_start]\n",
    "                logger.debug(fmt_log(V.shape, 'V shape after restart', 'qm'))\n",
    "                #logger.debug(fmt_log(V, 'V AFTER collapse', 'qm'))\n",
    "\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            term = True\n",
    "            print('============================', flush=True)\n",
    "            print('all residuals are below tolerance')\n",
    "            print('DAVIDSON ALGORITHM CONVERGED', flush=True)\n",
    "            print('============================', flush=True)\n",
    "\n",
    "            return r_eval, r_evec\n",
    "\n",
    "    # runs after big loop if did not converge\n",
    "    print('============================', flush=True)\n",
    "    print('!!! DAVIDSON ALGORITHM DID NOT CONVERGE !!!', flush=True)\n",
    "    print('============================', flush=True)\n",
    "    \n",
    "    return r_eval, r_evec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 0 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([8, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_256895/1247529461.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  species = torch.tensor([atoms.get_atomic_numbers()], dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'form_L_xi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# mol = run_seqm_1mol('c6h6.xyz')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# eval, _ = davidson(mol = mol, \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#                    N_exc = 8,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39m# logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m mol \u001b[39m=\u001b[39m run_seqm_1mol(\u001b[39m'\u001b[39m\u001b[39mh2o.xyz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[39meval\u001b[39m, _ \u001b[39m=\u001b[39m davidson(mol \u001b[39m=\u001b[39;49m mol, \n\u001b[1;32m     15\u001b[0m                    N_exc \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m                    keep_n \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m                    n_V_max \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, \n\u001b[1;32m     18\u001b[0m                    max_iter \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, \n\u001b[1;32m     19\u001b[0m                    tol \u001b[39m=\u001b[39;49m \u001b[39m1e-6\u001b[39;49m)\n\u001b[1;32m     21\u001b[0m logger\u001b[39m.\u001b[39mdebug(fmt_log(\u001b[39meval\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFINAL eval \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mevals\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[14], line 56\u001b[0m, in \u001b[0;36mdavidson\u001b[0;34m(mol, N_exc, keep_n, n_V_max, max_iter, tol)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(V\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]): \n\u001b[1;32m     54\u001b[0m     \u001b[39m# print('=================================', flush=True)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     logger\u001b[39m.\u001b[39mqm3(\u001b[39m'\u001b[39m\u001b[39mLxi iterations=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, i)\n\u001b[0;32m---> 56\u001b[0m     L_xi[:,i] \u001b[39m=\u001b[39m form_L_xi(V[:,i], mol, N_cis, N_rpa)\n\u001b[1;32m     57\u001b[0m     \u001b[39m# L_xi2[:,i] = torch.clone(L_xi[:,i])\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \n\u001b[1;32m     59\u001b[0m     \u001b[39m# L_xi[:,i]  = mult_by_mo_e(N_cis, mol, L_xi[:,i], V[:,i]) \u001b[39;00m\n\u001b[1;32m     60\u001b[0m     L_xi[:,i] \u001b[39m=\u001b[39m mult_by_mo_e_vect(N_cis, mol, L_xi[:,i], V[:,i]) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'form_L_xi' is not defined"
     ]
    }
   ],
   "source": [
    "# mol = run_seqm_1mol('c6h6.xyz')\n",
    "# eval, _ = davidson(mol = mol, \n",
    "#                    N_exc = 8,\n",
    "#                    keep_n = 4,\n",
    "#                    n_V_max = 50, \n",
    "#                    max_iter = 50, \n",
    "#                    tol = 1e-6)\n",
    "\n",
    "# logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))\n",
    "\n",
    "\n",
    "\n",
    "mol = run_seqm_1mol('h2o.xyz')\n",
    "eval, _ = davidson(mol = mol, \n",
    "                   N_exc = 3,\n",
    "                   keep_n = 2,\n",
    "                   n_V_max = 10, \n",
    "                   max_iter = 3, \n",
    "                   tol = 1e-6)\n",
    "\n",
    "logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 0 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 16])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.69621,  3.59425,  3.67799,  5.22364,  5.24623,  5.35856,  5.39645,  5.42210,  8.16187,  8.18378,  8.20683,  9.60633,  9.64560,  9.68289, 10.07326, 10.08923], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.1071475502570793\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.1745613697839479\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.2440393568512984\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.982610381588808\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 1 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_165479/3498202963.py:34: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  coeff_vec = prev_basis @ V[i].T  # each entry is np.dot(V[j], V[i]) for all j < i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.38429,  3.28952,  3.36376,  4.84494,  5.22684,  5.35821,  5.39267,  5.41620,  6.64028,  7.67619,  8.16193,  8.16411,  8.18378,  8.20718,  8.71632,  9.60640,  9.64560,  9.68289, 10.07326,\n",
      "        10.08923], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.1906089471926826\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.2281865635237594\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.2090488227453984\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.5146511439033261\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 2 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 24])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.24056,  3.02913,  3.10261,  4.54238,  5.08055,  5.19575,  5.35828,  5.39733,  5.42725,  5.52759,  5.67392,  6.46248,  8.16187,  8.18376,  8.20709,  9.60635,  9.64560,  9.68289, 10.07326,\n",
      "        10.08923, 15.99998, 16.11686, 17.87164, 18.91233], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.9655380372217297\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.6265919060401373\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.691561061380143\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.7004616699601334\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 3 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 28])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.19216,  2.84502,  2.89871,  4.30552,  4.69136,  4.76089,  4.77082,  5.25259,  5.35808,  5.39550,  5.42170,  5.99663,  8.16186,  8.18376,  8.20706,  9.60633,  9.64560,  9.68289, 10.07326,\n",
      "        10.08923, 12.79768, 13.12944, 13.70590, 13.96918, 29.82360, 30.34349, 31.56666, 32.05776], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.6083449765814117\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.3145006756292283\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.3726515162420325\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.8212705964082867\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 4 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 32])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.16695,  2.73127,  2.77629,  3.92919,  4.43749,  4.45832,  4.46689,  5.24103,  5.35773,  5.39544,  5.42146,  5.51723,  8.16186,  8.18367,  8.20688,  9.09360,  9.60646,  9.64558,  9.68287,\n",
      "        10.07326, 10.08920, 10.60195, 11.04714, 11.41117, 19.52561, 20.02640, 20.09111, 20.91184, 39.55433, 39.65576, 40.02711, 40.62189], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.523544523592526\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.9938822739496019\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.024822373062541\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m2.20571042827085\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 5 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 36])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.14423,  2.66114,  2.70270,  3.52499,  4.15578,  4.25740,  4.29451,  5.17382,  5.25516,  5.35710,  5.39462,  5.42063,  7.34782,  7.65997,  8.16187,  8.18452,  8.20824,  9.08724,  9.43220,\n",
      "         9.60784,  9.64559,  9.68289, 10.07326, 10.08923, 15.36522, 15.52085, 15.91393, 17.22975, 27.66372, 28.13144, 29.01722, 29.14964, 43.33511, 43.50222, 43.79534, 44.10116],\n",
      "       grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.5453116603076763\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.8455466984438359\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.8601496415927415\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m2.195669983628737\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 6 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 40])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.11848,  2.61311,  2.65313,  3.18795,  3.66890,  4.13022,  4.16670,  4.97528,  5.23697,  5.35344,  5.37980,  5.41238,  5.76226,  6.98343,  7.58563,  7.66427,  8.16187,  8.18410,  8.20754,\n",
      "         9.60664,  9.64558,  9.68288, 10.07326, 10.08922, 13.26677, 13.47966, 13.79197, 14.01217, 21.79596, 22.04282, 22.09729, 22.54155, 35.19652, 35.35132, 35.60619, 36.36718, 45.59239, 45.70030,\n",
      "        46.05794, 46.37768], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.5739843856142289\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.6437748897502167\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.6399143706285428\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.6187604110307692\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 7 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 44])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.09358,  2.58902,  2.63070,  3.04621,  3.23933,  4.05594,  4.09352,  4.88781,  5.16438,  5.22348,  5.35872,  5.40441,  5.46292,  6.69066,  6.75828,  6.90430,  8.16186,  8.18398,  8.20740,\n",
      "         9.60661,  9.64558,  9.68287, 10.07326, 10.08921, 11.84737, 12.09763, 12.48716, 12.53220, 18.21346, 18.59835, 18.74368, 19.70055, 27.92378, 28.44223, 28.67775, 29.09294, 39.41140, 39.65020,\n",
      "        39.79926, 40.12762, 47.01238, 47.19424, 47.61650, 47.76602], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.5293456668383504\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.44776675891288575\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.42037842569488193\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.096291780023915\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 8 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 48])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.07258,  2.57547,  2.61908,  2.95683,  2.97863,  4.00436,  4.05336,  4.81522,  4.96735,  5.20605,  5.35774,  5.40238,  5.44246,  6.17226,  6.46735,  6.50312,  8.16186,  8.18360,  8.20712,\n",
      "         9.60655,  9.64558,  9.68286,  9.73826, 10.07325, 10.08904, 10.28883, 11.37178, 11.44548, 15.16452, 15.58752, 15.65265, 16.37688, 22.29071, 23.09001, 23.12903, 23.41535, 32.77689, 33.15135,\n",
      "        33.30378, 33.77109, 41.95614, 42.18481, 42.34661, 42.51215, 48.02438, 48.23598, 48.68313, 48.68746], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.5293483447808045\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.40376026886459065\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.37093669952497604\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.0606328282889572\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\u001b[1m\u001b[47m\u001b[31m ITERATION : 9 \u001b[0m\n",
      "SUBSPACE SIZE V:  torch.Size([14400, 52])\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "davidson : 93 : DEBUG : \u001b[1m\u001b[32mRIGHT EVALS\u001b[0m\n",
      "\u001b[1m\u001b[32mtensor([ 2.04969,  2.56195,  2.60784,  2.77427,  2.86856,  3.93064,  4.00799,  4.68235,  4.78935,  5.15930,  5.35717,  5.40064,  5.43201,  5.49642,  5.92272,  5.96509,  7.79235,  8.12436,  8.16187,\n",
      "         8.19638,  8.23234,  9.18254,  9.29289,  9.60732,  9.64558,  9.68287, 10.07326, 10.08923, 13.24372, 13.41962, 13.75118, 13.79083, 19.59087, 19.76981, 19.82959, 20.60179, 27.75684, 28.08043,\n",
      "        28.41900, 28.66074, 36.81289, 37.04319, 37.31365, 37.52671, 43.91682, 44.14594, 44.42066, 44.46645, 48.87550, 49.15220, 49.45905, 49.60140], grad_fn=<SortBackward0>)\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.5120440022477968\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.4035070359452903\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m0.37068271158424526\u001b[0m\n",
      "davidson : 124 : DEBUG : \u001b[1m\u001b[31mNORM of RESIDUAL\u001b[0m : \u001b[1m\u001b[31m1.3950938441362728\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "!!! DAVIDSON ALGORITHM DID NOT CONVERGE !!!\n",
      "============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 7.58392 s\n",
      "File: /tmp/ipykernel_165479/3418432135.py\n",
      "Function: davidson at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def davidson(mol, N_exc, keep_n, n_V_max,  max_iter, tol):\n",
      "     2                                               \"\"\"\n",
      "     3                                               Davidson algorithm for solving eigenvalue problem of large sparse diagonally dominant matrices\n",
      "     4                                               Hamiltonian is not generated or stored explicitly, only matrix-vector products are used on-the fly:\n",
      "     5                                               guess space V should be orthogonalized at each iteration\n",
      "     6                                               M (projection of smaller size) is V.T @ H @ V \n",
      "     7                                               #! RPA (TDHF) is not implemented yet, non-Hermitian (non-symmetric), requires also left eigenvectors \n",
      "     8                                               note that notation differes between implementations: V.T x A x V is bAb\n",
      "     9                                               # TODO: 1) check if convergence of e_vals is needed\n",
      "    10                                               # TODO: 2) vectorize and optimize orthogonalization\n",
      "    11                                               # TODO: 3) check if some vectors should be dropped \n",
      "    12                                               # TODO: 4) eliminate loops \n",
      "    13                                               # TODO: 5) check if whole M should be regenerated, or only sub-blocks corresponding to new guess vectors\n",
      "    14                                               # TODO: 6) add parameter checker like Krylov dims << N_cis\n",
      "    15                                           \n",
      "    16                                               Args:\n",
      "    17                                                   mol (PYSEQM object): object to hold all qm data from PYSEQM\n",
      "    18                                                   N_exc (int)        : number of excited states to calculate\n",
      "    19                                                   keep_n (int)       : number of e_vals, e_vecs to keep at each iteration\n",
      "    20                                                   n_V_max (int)      : maximum size of Krylov subspace, \n",
      "    21                                                                        projected matrix will be no more than M(n_V_max x n_V_max)\n",
      "    22                                                   max_iter (int)     : maximum number of iterations in Davidson\n",
      "    23                                                   tol (float)        : treshold for residual\n",
      "    24                                                   \n",
      "    25                                               Returns:\n",
      "    26                                                   tuple of tensors: eigenvalues (excitation energies in default units, eV) and eigenvectors \n",
      "    27                                               \"\"\"    \n",
      "    28                                               \n",
      "    29         1          0.0      0.0      0.0      n_V_start = N_exc * 2 # dimension of Krylov subspace, analogue of nd1  \n",
      "    30         1          0.0      0.0      0.0      N_cis = mol.nocc * mol.nvirt\n",
      "    31         1          0.0      0.0      0.0      N_rpa = 2 * N_cis\n",
      "    32                                               \n",
      "    33         1          0.0      0.0      0.0      term = False  # terminate algorithm\n",
      "    34         1          0.0      0.0      0.0      iter = 0\n",
      "    35         1          0.0      0.0      0.0      L_xi = torch.zeros((N_rpa, n_V_start), device=device)\n",
      "    36         1          0.0      0.0      0.0      L_xi2 = torch.zeros((N_rpa, n_V_start), device=device)\n",
      "    37         1          0.4      0.4      4.9      V = gen_V(N_cis, N_rpa, n_V_start) # generate initial guess, V here #! should be renamed\n",
      "    38         1          0.0      0.0      0.0      diag = None # create diagonal of M only once\n",
      "    39                                               \n",
      "    40        10          0.0      0.0      0.0      while iter < max_iter and not term: # Davidson loop\n",
      "    41                                                   \n",
      "    42         9          0.0      0.0      0.0          if iter > 0: # skip first step, as initial V is orthogonal\n",
      "    43         9          0.2      0.0      3.3              V = orthogonalize_torch(V)\n",
      "    44                                                       \n",
      "    45        10          0.0      0.0      0.1          print('=================================', flush=True)\n",
      "    46        10          0.0      0.0      0.1          print(colored(f' ITERATION : {iter} ', 'red', 'on_white', attrs=['bold']), flush=True)\n",
      "    47        10          0.0      0.0      0.1          print('SUBSPACE SIZE V: ', V.shape, flush=True)\n",
      "    48        10          0.0      0.0      0.0          print('=================================')\n",
      "    49                                                  \n",
      "    50                                                   # ---------- form A x b product --------------------\n",
      "    51        10          0.0      0.0      0.1          L_xi = torch.zeros((N_rpa, V.shape[1] ), device=device) #! NOT iter here\n",
      "    52        10          0.0      0.0      0.1          logger.qm1(fmt_log(V, 'V BEFORE L_xi after ORTO', 'qm'))\n",
      "    53       340          0.0      0.0      0.0          for i in range(V.shape[1]): \n",
      "    54                                                       # print('=================================', flush=True)\n",
      "    55       340          0.0      0.0      0.0              logger.qm3('Lxi iterations=%s', i)\n",
      "    56       340          6.2      0.0     82.4              L_xi[:,i] = form_L_xi(V[:,i], mol, N_cis, N_rpa)\n",
      "    57                                                       # L_xi2[:,i] = torch.clone(L_xi[:,i])\n",
      "    58                                                       \n",
      "    59                                                       # L_xi[:,i]  = mult_by_mo_e(N_cis, mol, L_xi[:,i], V[:,i]) \n",
      "    60       340          0.3      0.0      4.1              L_xi[:,i] = mult_by_mo_e_vect(N_cis, mol, L_xi[:,i], V[:,i]) \n",
      "    61                                                       # multiply by MO energies\n",
      "    62                                                       \n",
      "    63                                                       \n",
      "    64                                                       \n",
      "    65                                                       # raise ValueError('STOP')\n",
      "    66       340          0.2      0.0      2.8              logger.qm3(fmt_log(L_xi[:,i], 'L_xi[:,i]', 'qm'))\n",
      "    67                                                   \n",
      "    68        10          0.0      0.0      0.0          L_xi[N_cis:, :] = L_xi[:N_cis] #! TODO: make sure that this A+B, A-B, not just copy for RPA\n",
      "    69                                               \n",
      "    70        10          0.0      0.0      0.0          right_V = L_xi[N_cis:] # (A)b \n",
      "    71                                                   \n",
      "    72        10          0.0      0.0      0.0          logger.qm1(fmt_log(right_V.shape, 'right_V shape', 'matrix'))\n",
      "    73        10          0.0      0.0      0.1          logger.qm1(fmt_log(right_V, 'right_V', 'matrix'))       \n",
      "    74                                                   # ---------- form b.T x Ab product --------------------\n",
      "    75                                                   \n",
      "    76        10          0.0      0.0      0.2          M =  V.T @ right_V\n",
      "    77                                                   \n",
      "    78                                                   # logger.debug(fmt_log(M.shape, 'M shape', 'qm'))\n",
      "    79                                                   # logger.debug(fmt_log(M, 'M', 'qm'))\n",
      "    80         9          0.0      0.0      0.0          if iter == 0:\n",
      "    81         1          0.0      0.0      0.0              diag = torch.diag(M) # create diagonal only once\n",
      "    82                                                       \n",
      "    83        10          0.0      0.0      0.0          iter += 1\n",
      "    84                                                   \n",
      "    85        10          0.0      0.0      0.1          logger.qm1(fmt_log(diag, 'diag', 'qm'))\n",
      "    86                                               \n",
      "    87                                                   # ---------- diagonalize projection M --------------------\n",
      "    88        10          0.0      0.0      0.3          r_eval, r_evec = torch.linalg.eigh(M) # find eigenvalues and eigenvectors\n",
      "    89                                                  \n",
      "    90        10          0.0      0.0      0.0          r_eval = r_eval.real\n",
      "    91        10          0.0      0.0      0.0          r_evec = r_evec.real\n",
      "    92        10          0.0      0.0      0.0          r_eval, r_idx = torch.sort(r_eval, descending=False) # sort eigenvalues in ascending order\n",
      "    93        10          0.0      0.0      0.2          logger.debug(fmt_log(r_eval, 'RIGHT EVALS', 'evals'))\n",
      "    94        10          0.0      0.0      0.0          r_evec = r_evec[:, r_idx] # sort eigenvectors accordingly\n",
      "    95                                               \n",
      "    96        10          0.0      0.0      0.0          e_val_n = r_eval[:keep_n] # keep only the lowest keep_n eigenvalues; full are still stored as e_val\n",
      "    97        10          0.0      0.0      0.0          e_vec_n = r_evec[:, :keep_n]\n",
      "    98        10          0.0      0.0      0.0          resids = torch.zeros(V.shape[0], len(e_val_n)) # account for left and right evecs\n",
      "    99                                           \n",
      "   100                                                   # ---------- calculate residual vectors --------------------\n",
      "   101        40          0.0      0.0      0.0          for j in range(len(e_val_n)): # calc residuals \n",
      "   102        40          0.0      0.0      0.1              resids[:,j] = right_V @ e_vec_n[:,j] - e_val_n[j] * (V @ e_vec_n[:,j])\n",
      "   103                                                       \n",
      "   104                                                  # logger.debug(fmt_log(resids, 'resids', 'matrix'))     \n",
      "   105        10          0.0      0.0      0.0          resids_norms_r = torch.tensor([resids[:,x].norm() for x in range(resids.shape[1])])\n",
      "   106                                           \n",
      "   107                                                   # ---------- expand guess space V buy not-converged resids --------------------\n",
      "   108                                                   # !!! PROBABLY HIGHLY INEFFICIENT !!! \n",
      "   109        10          0.0      0.0      0.0          if torch.any(resids_norms_r > tol):\n",
      "   110        10          0.0      0.0      0.0              mask_r = resids_norms_r >= tol\n",
      "   111        10          0.0      0.0      0.0              large_res_r = resids[:,mask_r] # residuals larger than tol\n",
      "   112                                                      # logger.debug(fmt_log(large_res_r, 'LARGE RESIDUALS', 'vector'))           \n",
      "   113        10          0.0      0.0      0.0              large_res_r.to(device)\n",
      "   114        10          0.0      0.0      0.0              cor_e_val_r = e_val_n[mask_r] # corresponding eigenvalues !!! check if matches\n",
      "   115                                                       \n",
      "   116                                                       # ------keep adding new resids --------------------\n",
      "   117        10          0.0      0.0      0.0              if V.shape[1] <= n_V_max:     \n",
      "   118                                           \n",
      "   119        40          0.0      0.0      0.0                      for j in range(large_res_r.shape[1]):\n",
      "   120        40          0.0      0.0      0.0                          if V.shape[1] <= n_V_max:\n",
      "   121        40          0.0      0.0      0.0                              s = large_res_r[:,j] # conditioned residuals > tol\n",
      "   122                                           \n",
      "   123        40          0.0      0.0      0.0                              if s.norm() >= tol:\n",
      "   124        40          0.0      0.0      0.3                                  logger.debug(fmt_log((s.norm().item()), 'NORM of RESIDUAL', 'warn'))\n",
      "   125        40          0.0      0.0      0.1                                  denom = (diag[j] - cor_e_val_r[j])\n",
      "   126        40          0.0      0.0      0.0                                  denom.to(device) \n",
      "   127        40          0.0      0.0      0.0                                  s = s/denom # conditioned residuals\n",
      "   128        40          0.0      0.0      0.0                                  s.to(device)\n",
      "   129                                                                           # logger.debug(fmt_log(s.norm(), 'NORM OF NEW RESIDUAAL', 'vector'))\n",
      "   130        40          0.0      0.0      0.3                                  V = torch.column_stack((V, s/s.norm()))\n",
      "   131                                                                       else:\n",
      "   132                                                                           pass\n",
      "   133                                                       # ------ collapse (restart) if space V is too large; mix eigenvectors with V------------\n",
      "   134                                                       else:\n",
      "   135                                                           logger.debug(fmt_log(None, '!!!! MAX subspace reached !!!!', 'warn'))\n",
      "   136                                                           #logger.debug(fmt_log(V, 'V before collapse', 'qm'))\n",
      "   137                                           \n",
      "   138                                                           V =  V @ r_evec[:, :n_V_start]\n",
      "   139                                                           logger.debug(fmt_log(V.shape, 'V shape after restart', 'qm'))\n",
      "   140                                                           #logger.debug(fmt_log(V, 'V AFTER collapse', 'qm'))\n",
      "   141                                           \n",
      "   142                                                           continue\n",
      "   143                                           \n",
      "   144                                                   else:\n",
      "   145                                                       term = True\n",
      "   146                                                       print('============================', flush=True)\n",
      "   147                                                       print('all residuals are below tolerance')\n",
      "   148                                                       print('DAVIDSON ALGORITHM CONVERGED', flush=True)\n",
      "   149                                                       print('============================', flush=True)\n",
      "   150                                           \n",
      "   151                                                       return r_eval, r_evec\n",
      "   152                                           \n",
      "   153                                               # runs after big loop if did not converge\n",
      "   154         1          0.0      0.0      0.0      print('============================', flush=True)\n",
      "   155         1          0.0      0.0      0.0      print('!!! DAVIDSON ALGORITHM DID NOT CONVERGE !!!', flush=True)\n",
      "   156         1          0.0      0.0      0.0      print('============================', flush=True)\n",
      "   157                                               \n",
      "   158         1          0.0      0.0      0.0      return r_eval, r_evec"
     ]
    }
   ],
   "source": [
    "mol = run_seqm_1mol('c60.xyz')\n",
    "%lprun -u 1 -f davidson davidson(mol = mol, N_exc = 8, keep_n = 4, n_V_max = 60, max_iter = 10, tol = 1e-6)\n",
    "# %lprun -u 1 -f davidson davidson(mol = mol, N_exc = 8,keep_n = 4, n_V_max = 60, max_iter = 1, tol = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qm",
   "language": "python",
   "name": "qm"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
