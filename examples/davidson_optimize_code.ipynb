{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/vast/projects/ml4chem/NikitaFedik/GITHUB/PYSEQM_davidson')\n",
    "# %load_ext line_profiler\n",
    "# %load_ext autoreload\n",
    "\n",
    "# %autoreload 2\n",
    "# %reload_ext autoreload\n",
    "\n",
    "# === IMPORTS ===\n",
    "\n",
    "import logging, sys\n",
    "import torch\n",
    "import seqm\n",
    "from ase.io import read as ase_read\n",
    "from seqm.seqm_functions.constants import Constants\n",
    "from seqm.Molecule import Molecule\n",
    "from seqm.ElectronicStructure import Electronic_Structure\n",
    "from termcolor import colored\n",
    "# from termcolor import colored\n",
    "\n",
    "\n",
    "from seqm.seqm_functions.fock import fock\n",
    "from seqm.seqm_functions.pack import unpack\n",
    "import seqm.seqm_functions.pack as pack\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#=== TORCH OPTIONS ===\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "# else:\n",
    "#     device = torch.device('cpu')\n",
    "dtype = torch.float64\n",
    "# torch.set_printoptions(precision=5, linewidth=200, profile=\"full\", sci_mode=False)\n",
    "torch.set_printoptions(precision=5, linewidth=200, sci_mode=False, profile = 'short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/vast/home/fns'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colored logging with custom level QM for deeper routines\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(funcName)s : %(lineno)d : %(levelname)s : %(message)s')\n",
    "\n",
    "QM1 = evel=logging.DEBUG - 3 # informal level of depth; QM1 - almost always, usually outside of loops\n",
    "QM2 = evel=logging.DEBUG - 4 #                          QM2 - sometimes, in the loops\n",
    "QM3 = evel=logging.DEBUG - 5\n",
    "\n",
    "logging.addLevelName(QM1, \"QM1\")\n",
    "def qm1(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM1 ):\n",
    "        self._log(QM1, message, args, **kwargs) \n",
    "        \n",
    "logging.addLevelName(QM2, \"QM2\")\n",
    "def qm2(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM2):\n",
    "        self._log(QM2, message, args, **kwargs) \n",
    " \n",
    "logging.addLevelName(QM3, \"QM3\")\n",
    "def qm3(self, message, *args, **kwargs):\n",
    "    if self.isEnabledFor(QM3 ):\n",
    "        self._log(QM3, message, args, **kwargs) \n",
    "           \n",
    "        \n",
    "logging.Logger.qm1 = qm1   \n",
    "logging.Logger.qm2 = qm2\n",
    "logging.Logger.qm3 = qm3\n",
    "  \n",
    "logger = logging.getLogger()\n",
    "\n",
    "                              \n",
    "colors = {'qm'        : ('cyan',     None, None),\n",
    "          'matrix'    : ('blue',     None, ['bold']),\n",
    "          'vector'    : ('yellow',   None, ['bold']),\n",
    "          'evals'     : ('green',    None, ['bold']),\n",
    "          'warn'     : ('red',    None, ['bold'])\n",
    "          }\n",
    "\n",
    "def fmt_log(data, message, fmt):\n",
    "    \"\"\"\n",
    "    fmt_log : formats log message with color and style using termcolor module\n",
    "\n",
    "    Args:\n",
    "        data (any): data to print\n",
    "        message (str or None): message to print, pass None if no message is needed\n",
    "        fmt (str): style from colors dict\n",
    "\n",
    "    Returns:\n",
    "        str: formatted string with color and style\n",
    "    \"\"\"    \n",
    "\n",
    "    if type(data) is list or type(data) is tuple or type(data) is torch.Tensor:\n",
    "        \n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])}\\n' # add new line to align array\n",
    "    else:\n",
    "        mes = f'{colored(message, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2])} : '\n",
    "        \n",
    "    if data == None:\n",
    "        return mes\n",
    "    else:\n",
    "        return mes + str(colored(data, colors[fmt][0], colors[fmt][1], attrs=colors[fmt][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log\n",
    "\n",
    "07/13/23 - QM part seems to be wortking fine\n",
    "full diagonalization agrees with NEXMD\n",
    "small guess space misses relevant vectors, but large guess includes them\n",
    "\n",
    "\n",
    "PASCAL 1 COULD BE INCORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seqm_1mol(xyz):\n",
    "    \"\"\"\n",
    "    run_seqm_1mol : run PYSEQM for a single molecule\n",
    "\n",
    "    Args:\n",
    "        xyz (str): path to xyz file\n",
    "\n",
    "    Returns:\n",
    "        Molecule object: PYSEQM object with molecule data\n",
    "    \"\"\"    \n",
    "    \n",
    "    atoms = ase_read(xyz)\n",
    "    species = torch.tensor([atoms.get_atomic_numbers()], dtype=torch.long, device=device)\n",
    "    coordinates = torch.tensor([atoms.get_positions()], dtype=dtype, device=device)\n",
    "    \n",
    "    const = Constants().to(device)\n",
    "\n",
    "    elements = [0]+sorted(set(species.reshape(-1).tolist()))\n",
    "\n",
    "    seqm_parameters = {\n",
    "                    'method' : 'PM3',  # AM1, MNDO, PM#\n",
    "                    'scf_eps' : 1.0e-6,  # unit eV, change of electric energy, as nuclear energy doesnt' change during SCF\n",
    "                    'scf_converger' : [2,0.0], # converger used for scf loop\n",
    "                                            # [0, 0.1], [0, alpha] constant mixing, P = alpha*P + (1.0-alpha)*Pnew\n",
    "                                            # [1], adaptive mixing\n",
    "                                            # [2], adaptive mixing, then pulay\n",
    "                    'sp2' : [False, 1.0e-5],  # whether to use sp2 algorithm in scf loop,\n",
    "                                                #[True, eps] or [False], eps for SP2 conve criteria\n",
    "                    'elements' : elements, #[0,1,6,8],\n",
    "                    'learned' : [], # learned parameters name list, e.g ['U_ss']\n",
    "                    #'parameter_file_dir' : '../seqm/params/', # file directory for other required parameters\n",
    "                    'pair_outer_cutoff' : 1.0e10, # consistent with the unit on coordinates\n",
    "                    'eig' : True,\n",
    "                    'excited' : True,\n",
    "                    }\n",
    "\n",
    "    mol = seqm.Molecule.Molecule(const, seqm_parameters, coordinates, species).to(device)\n",
    "\n",
    "    ### Create electronic structure driver:\n",
    "    esdriver = Electronic_Structure(seqm_parameters).to(device)\n",
    "\n",
    "    ### Run esdriver on m:\n",
    "    esdriver(mol)\n",
    "    \n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_seqm(xyz_list):\n",
    "    \"\"\"\n",
    "    run_seqm_1mol : run PYSEQM for a single molecule\n",
    "\n",
    "    Args:\n",
    "        xyz (str): path to xyz file\n",
    "\n",
    "    Returns:\n",
    "        Molecule object: PYSEQM object with molecule data\n",
    "    \"\"\"    \n",
    "    \n",
    "    atoms = [ase_read(x) for x in xyz_list]\n",
    "    species = torch.tensor([atoms[x].get_atomic_numbers() for x in range(len(atoms))], dtype=torch.long, device=device)\n",
    "    coordinates = torch.tensor([atoms[x].get_positions() for x in range(len(atoms))], dtype=dtype, device=device)\n",
    "    \n",
    "    const = Constants().to(device)\n",
    "\n",
    "    elements = [0]+sorted(set(species.reshape(-1).tolist()))\n",
    "\n",
    "    seqm_parameters = {\n",
    "                    'method' : 'PM3',  # AM1, MNDO, PM#\n",
    "                    'scf_eps' : 1.0e-6,  # unit eV, change of electric energy, as nuclear energy doesnt' change during SCF\n",
    "                    'scf_converger' : [2,0.0], # converger used for scf loop\n",
    "                                            # [0, 0.1], [0, alpha] constant mixing, P = alpha*P + (1.0-alpha)*Pnew\n",
    "                                            # [1], adaptive mixing\n",
    "                                            # [2], adaptive mixing, then pulay\n",
    "                    'sp2' : [False, 1.0e-5],  # whether to use sp2 algorithm in scf loop,\n",
    "                                                #[True, eps] or [False], eps for SP2 conve criteria\n",
    "                    'elements' : elements, #[0,1,6,8],\n",
    "                    'learned' : [], # learned parameters name list, e.g ['U_ss']\n",
    "                    #'parameter_file_dir' : '../seqm/params/', # file directory for other required parameters\n",
    "                    'pair_outer_cutoff' : 1.0e10, # consistent with the unit on coordinates\n",
    "                    'eig' : True,\n",
    "                    'excited' : True,\n",
    "                    }\n",
    "\n",
    "    mol = seqm.Molecule.Molecule(const, seqm_parameters, coordinates, species).to(device)\n",
    "\n",
    "    ### Create electronic structure driver:\n",
    "    esdriver = Electronic_Structure(seqm_parameters).to(device)\n",
    "\n",
    "    ### Run esdriver on m:\n",
    "    esdriver(mol)\n",
    "    \n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# molecules = run_seqm(['h2o.xyz', 'h2o.xyz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = run_seqm_1mol('/vast/projects/ml4chem/NikitaFedik/GITHUB/PYSEQM_davidson/examples/h2o.xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = run_seqm_1mol('/vast/projects/ml4chem/NikitaFedik/GITHUB/PYSEQM_davidson/examples/h2o.xyz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAVIDSON routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)  # custom logging level; lower than DEBUG\n",
    "                               # printed above QM (QM, DEBUG, INFO, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL new modules translated from NEXMD\n",
    "# keep as seqm functions \n",
    "\n",
    "from seqm.seqm_functions.excited import ortho as orthogonalize\n",
    "from seqm.seqm_functions.excited.hamiltonian import gen_V\n",
    "from seqm.seqm_functions.excited.hamiltonian import form_cis\n",
    "import seqm.seqm_functions.excited\n",
    "from seqm.seqm_functions.excited.orb_transform import mo2ao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def davidson(device, mol, N_exc, keep_n, n_V_max,  max_iter, tol):\n",
    "    \"\"\"\n",
    "    Davidson algorithm for solving eigenvalue problem of large sparse diagonally dominant matrices\n",
    "    Hamiltonian is not generated or stored explicitly, only matrix-vector products are used on-the fly:\n",
    "    guess space V should be orthogonalized at each iteration\n",
    "    M (projection of smaller size) is V.T @ H @ V \n",
    "    #! RPA (TDHF) is not implemented yet, non-Hermitian (non-symmetric), requires also left eigenvectors \n",
    "    note that notation differes between implementations: V.T x A x V is bAb\n",
    "    # TODO: 1) check if convergence of e_vals is needed\n",
    "    # TODO: 2) vectorize and optimize orthogonalization\n",
    "    # TODO: 3) check if some vectors should be dropped \n",
    "    # TODO: 4) eliminate loops \n",
    "    # TODO: 5) check if whole M should be regenerated, or only sub-blocks corresponding to new guess vectors\n",
    "    # TODO: 6) add parameter checker like Krylov dims << N_cis\n",
    "\n",
    "    Args:\n",
    "        mol (PYSEQM object): object to hold all qm data from PYSEQM\n",
    "        N_exc (int)        : number of excited states to calculate\n",
    "        keep_n (int)       : number of e_vals, e_vecs to keep at each iteration\n",
    "        n_V_max (int)      : maximum size of Krylov subspace, \n",
    "                             projected matrix will be no more than M(n_V_max x n_V_max)\n",
    "        max_iter (int)     : maximum number of iterations in Davidson\n",
    "        tol (float)        : treshold for residual\n",
    "        \n",
    "    Returns:\n",
    "        tuple of tensors: eigenvalues (excitation energies in default units, eV) and eigenvectors \n",
    "    \"\"\"    \n",
    "    \n",
    "    n_V_start = N_exc * 2 # dimension of Krylov subspace, analogue of nd1  \n",
    "    N_cis = mol.nocc * mol.nvirt\n",
    "    N_rpa = 2 * N_cis\n",
    "    \n",
    "    term = False  # terminate algorithm\n",
    "    iter = 0\n",
    "    L_xi = torch.zeros((N_rpa, n_V_start), device=device)\n",
    "    V = gen_V(device, mol, N_cis, N_rpa) # generate initial guess, V here #! should be renamed\n",
    "    diag = None # create diagonal of M only once\n",
    "    \n",
    "    while iter < max_iter and not term: # Davidson loop\n",
    "        \n",
    "        if iter > 0: # skip first step, as initial V is orthogonal\n",
    "            V = orthogonalize(V)\n",
    "            \n",
    "        print('=================================', flush=True)\n",
    "        print(colored(f' ITERATION : {iter} ', 'red', 'on_white', attrs=['bold']), flush=True)\n",
    "        print('SUBSPACE SIZE V: ', V.shape, flush=True)\n",
    "        print('=================================')\n",
    "       \n",
    "        # ---------- form A x b product --------------------\n",
    "        L_xi = torch.zeros((N_rpa, V.shape[1]), device=device) #! NOT iter here\n",
    "\n",
    "        logger.qm1(fmt_log(V, 'V BEFORE L_xi after ORTO', 'qm'))\n",
    "        \n",
    "        L_xi = form_cis(device, V, mol, N_cis, n_V_start )\n",
    "        \n",
    "        # for i in range(V.shape[2]): \n",
    "        #     # print('=================================', flush=True)\n",
    "        #     logger.qm3('Lxi iterations=%s', i)\n",
    "        #    # L_xi[:,i] = form_cis(device, V[:,i], mol, N_cis, N_rpa)\n",
    "        #     L_xi[:,i] = form_cis(device, V[:, : ,i], mol, N_cis, N_rpa)\n",
    "        #     # L_xi2[:,i] = torch.clone(L_xi[:,i])\n",
    "            \n",
    "        #     # L_xi[:,i]  = mult_by_mo_e(N_cis, mol, L_xi[:,i], V[:,i]) \n",
    "        #     L_xi[:,i] = mult_by_mo_e_vect(N_cis, mol, L_xi[:,i], V[:,i]) \n",
    "        #     # multiply by MO energies\n",
    "        # raise ValueError('======= STOP ===========')\n",
    "            \n",
    "            \n",
    "       \n",
    "        # logger.qm3(fmt_log(L_xi, 'L_xi', 'qm'))\n",
    "        print('L_xi.shape', L_xi.shape)\n",
    "        print('L_xi\\n', L_xi)\n",
    "        # raise ValueError('STOP')\n",
    "        L_xi[N_cis:, :] = L_xi[:N_cis] #! TODO: make sure that this A+B, A-B, not just copy for RPA\n",
    "    \n",
    "        right_V = L_xi[N_cis:] # (A)b \n",
    "        \n",
    "        # logger.qm1(fmt_log(right_V.shape, 'right_V shape', 'matrix'))\n",
    "        # logger.qm1(fmt_log(right_V, 'right_V', 'matrix'))       \n",
    "        # ---------- form b.T x Ab product --------------------\n",
    "        \n",
    "        M =  V.T @ right_V\n",
    "        \n",
    "        # logger.debug(fmt_log(M.shape, 'M shape', 'qm'))\n",
    "        # logger.debug(fmt_log(M, 'M', 'qm'))\n",
    "        if iter == 0:\n",
    "            diag = torch.diag(M) # create diagonal only once\n",
    "            \n",
    "        iter += 1\n",
    "        \n",
    "        logger.qm1(fmt_log(diag, 'diag', 'qm'))\n",
    "    \n",
    "        # ---------- diagonalize projection M --------------------\n",
    "        r_eval, r_evec = torch.linalg.eigh(M) # find eigenvalues and eigenvectors\n",
    "       \n",
    "        r_eval = r_eval.real\n",
    "        r_evec = r_evec.real\n",
    "        r_eval, r_idx = torch.sort(r_eval, descending=False) # sort eigenvalues in ascending order\n",
    "        logger.debug(fmt_log(r_eval, 'RIGHT EVALS', 'evals'))\n",
    "        r_evec = r_evec[:, r_idx] # sort eigenvectors accordingly\n",
    "    \n",
    "        e_val_n = r_eval[:keep_n] # keep only the lowest keep_n eigenvalues; full are still stored as e_val\n",
    "        e_vec_n = r_evec[:, :keep_n]\n",
    "        resids = torch.zeros(V.shape[0], len(e_val_n)) # account for left and right evecs\n",
    "\n",
    "        # ---------- calculate residual vectors --------------------\n",
    "        for j in range(len(e_val_n)): # calc residuals \n",
    "            resids[:,j] = right_V @ e_vec_n[:,j] - e_val_n[j] * (V @ e_vec_n[:,j])\n",
    "            \n",
    "       # logger.debug(fmt_log(resids, 'resids', 'matrix'))     \n",
    "        resids_norms_r = torch.tensor([resids[:,x].norm() for x in range(resids.shape[1])])\n",
    "\n",
    "        # ---------- expand guess space V buy not-converged resids --------------------\n",
    "        # !!! PROBABLY HIGHLY INEFFICIENT !!! \n",
    "        if torch.any(resids_norms_r > tol):\n",
    "            mask_r = resids_norms_r >= tol\n",
    "            large_res_r = resids[:,mask_r] # residuals larger than tol\n",
    "           # logger.debug(fmt_log(large_res_r, 'LARGE RESIDUALS', 'vector'))           \n",
    "            large_res_r.to(device)\n",
    "            cor_e_val_r = e_val_n[mask_r] # corresponding eigenvalues !!! check if matches\n",
    "            \n",
    "            # ------keep adding new resids --------------------\n",
    "            if V.shape[1] <= n_V_max:     \n",
    "\n",
    "                    for j in range(large_res_r.shape[1]):\n",
    "                        if V.shape[1] <= n_V_max:\n",
    "                            s = large_res_r[:,j] # conditioned residuals > tol\n",
    "\n",
    "                            if s.norm() >= tol:\n",
    "                                logger.debug(fmt_log((s.norm().item()), 'NORM of RESIDUAL', 'warn'))\n",
    "                                denom = (diag[j] - cor_e_val_r[j])\n",
    "                                denom.to(device) \n",
    "                                s = s/denom # conditioned residuals\n",
    "                                s.to(device)\n",
    "                                # logger.debug(fmt_log(s.norm(), 'NORM OF NEW RESIDUAAL', 'vector'))\n",
    "                                V = torch.column_stack((V, s/s.norm()))\n",
    "                            else:\n",
    "                                pass\n",
    "            # ------ collapse (restart) if space V is too large; mix eigenvectors with V------------\n",
    "            else:\n",
    "                logger.debug(fmt_log(None, '!!!! MAX subspace reached !!!!', 'warn'))\n",
    "                #logger.debug(fmt_log(V, 'V before collapse', 'qm'))\n",
    "\n",
    "                V =  V @ r_evec[:, :n_V_start]\n",
    "                logger.debug(fmt_log(V.shape, 'V shape after restart', 'qm'))\n",
    "                #logger.debug(fmt_log(V, 'V AFTER collapse', 'qm'))\n",
    "\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            term = True\n",
    "            print('============================', flush=True)\n",
    "            print('all residuals are below tolerance')\n",
    "            print('DAVIDSON ALGORITHM CONVERGED', flush=True)\n",
    "            print('============================', flush=True)\n",
    "\n",
    "            return r_eval, r_evec\n",
    "\n",
    "    # runs after big loop if did not converge\n",
    "    print('============================', flush=True)\n",
    "    print('!!! DAVIDSON ALGORITHM DID NOT CONVERGE !!!', flush=True)\n",
    "    print('============================', flush=True)\n",
    "    \n",
    "    return r_eval, r_evec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<module> : 13 : DEBUG : FINAL eval  : <built-in function eval>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == GEN V ==\n",
      "V shape torch.Size([8, 8])\n",
      "V\n",
      " tensor([[0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "=================================\n",
      " ITERATION : 0 \n",
      "SUBSPACE SIZE V:  torch.Size([8, 8])\n",
      "=================================\n",
      "V_mo torch.Size([32, 2])\n",
      "V_mo_tmp torch.Size([6, 6])\n",
      "V_ao torch.Size([32, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (6) at non-singleton dimension 1.  Target sizes: [6, 4].  Tensor sizes: [32, 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63487/562670072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                    \u001b[0mn_V_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                    \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                    tol = -1e-6)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FINAL eval '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'evals'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_63487/2739817304.py\u001b[0m in \u001b[0;36mdavidson\u001b[0;34m(device, mol, N_exc, keep_n, n_V_max, max_iter, tol)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'V BEFORE L_xi after ORTO'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'qm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mL_xi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_cis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_cis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_V_start\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# for i in range(V.shape[2]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vast/projects/ml4chem/NikitaFedik/GITHUB/PYSEQM_davidson/seqm/seqm_functions/excited/hamiltonian.py\u001b[0m in \u001b[0;36mform_cis\u001b[0;34m(device, V, mol, N_cis, CIS)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mV_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mV_ao\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmo2ao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_cis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# mo to ao basis (mo2site)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'V_ao.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_ao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_ao\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vast/projects/ml4chem/NikitaFedik/GITHUB/PYSEQM_davidson/seqm/seqm_functions/excited/orb_transform.py\u001b[0m in \u001b[0;36mmo2ao\u001b[0;34m(device, N_cis, V_mo, mol)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mV_ao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_mo\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnocc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# operations on |X|, |Y| from RPA is ignored for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"V_ao\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_ao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mV_mo_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnocc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_ao\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mV_ao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV_mo_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (6) at non-singleton dimension 1.  Target sizes: [6, 4].  Tensor sizes: [32, 6]"
     ]
    }
   ],
   "source": [
    "# mol = run_seqm_1mol('c6h6.xyz')\n",
    "# eval, _ = davidson(mol = mol, \n",
    "#                    N_exc = 8,\n",
    "#                    keep_n = 4,\n",
    "#                    n_V_max = 50, \n",
    "#                    max_iter = 50, \n",
    "#                    tol = 1e-6)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))\n",
    "from seqm.seqm_functions.excited import ortho as orthogonalize\n",
    "from seqm.seqm_functions.excited.hamiltonian import gen_V\n",
    "from seqm.seqm_functions.excited.hamiltonian import form_cis\n",
    "from seqm.seqm_functions.excited.orb_transform import mo2ao\n",
    "\n",
    "\n",
    "\n",
    "mol = run_seqm_1mol('/vast/projects/ml4chem/NikitaFedik/GITHUB/PYSEQM_davidson/examples/h2o.xyz')\n",
    "eval, _ = davidson(device = 'cpu', \n",
    "                   mol = mol, \n",
    "                   N_exc = 3,\n",
    "                   keep_n = 2,\n",
    "                   n_V_max = 10, \n",
    "                   max_iter = 3, \n",
    "                   tol = -1e-6)\n",
    "\n",
    "logger.debug(fmt_log(eval, 'FINAL eval ', 'evals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    -0.87887,     -0.00000,      0.34020,      0.00000,     -0.33445,     -0.00000],\n",
       "         [    -0.00000,     -0.00000,     -0.00000,      1.00000,     -0.00000,      0.00000],\n",
       "         [    -0.00000,     -0.77088,     -0.00000,     -0.00000,     -0.00000,      0.63698],\n",
       "         [    -0.10818,      0.00000,     -0.82490,     -0.00000,     -0.55482,     -0.00000],\n",
       "         [    -0.32855,     -0.45041,     -0.31921,     -0.00000,      0.53866,     -0.54510],\n",
       "         [    -0.32855,      0.45041,     -0.31921,     -0.00000,      0.53866,      0.54510]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol.C_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    -0.87887,     -0.00000,     -0.00000,     -0.10818,     -0.32855,     -0.32855],\n",
       "         [    -0.00000,     -0.00000,     -0.77088,      0.00000,     -0.45041,      0.45041],\n",
       "         [     0.34020,     -0.00000,     -0.00000,     -0.82490,     -0.31921,     -0.31921],\n",
       "         [     0.00000,      1.00000,     -0.00000,     -0.00000,     -0.00000,     -0.00000],\n",
       "         [    -0.33445,     -0.00000,     -0.00000,     -0.55482,      0.53866,      0.53866],\n",
       "         [    -0.00000,      0.00000,      0.63698,     -0.00000,     -0.54510,      0.54510]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol.C_mo.transpose(1,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qm",
   "language": "python",
   "name": "qm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
